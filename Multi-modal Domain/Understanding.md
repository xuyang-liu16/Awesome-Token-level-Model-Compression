### 2022
- **[1] Multimodal Token Fusion for Vision Transformers**, CVPR 2022.
  
  *Wang, Yikai and Chen, Xinghao and Cao, Lele and Huang, Wenbing and Sun, Fuchun and Wang, Yunhe.*

  [[Paper](https://arxiv.org/abs/2204.08721)] [Code] ![](https://img.shields.io/badge/TokenFusion-blue) ![](https://img.shields.io/badge/Image_to_Image_Translation-green) ![](https://img.shields.io/badge/RGB_Depth_Semantic_Segmentation-green) ![](https://img.shields.io/badge/Point_Cloud_3D_Object_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

### 2023
- **[2] PuMer: Pruning and Merging Tokens for Efficient Vision Language Models**, ACL 2023.

  *Cao, Qingqing and Paranjape, Bhargavi and Hajishirzi, Hannaneh.*

  [[Paper](https://arxiv.org/abs/2305.17530)] [[Code](https://github.com/csarron/PuMer)] ![](https://img.shields.io/badge/PuMer-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

### 2024

- **[3] LLaVA-PruMerge: Adaptive Token Reduction for Efficient Large Multimodal Models**, arXiv 2024.

  *Shang, Yuzhang and Cai, Mu and Xu, Bingxin and Lee, Yong Jae and Yan, Yan.*

  [[Paper](https://arxiv.org/abs/2403.15388)] [[Code](https://github.com/42Shawn/LLaVA-PruMerge)] ![](https://img.shields.io/badge/LLaVA_PruMerge-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[4] Accelerating Transformers with Spectrum-Preserving Token Merging**, NeurIPS 2024.

  *Tran, Hoai-Chau and Nguyen, Duy MH and Nguyen, Duy M and Nguyen, Trung-Tin and Le, Ngan and Xie, Pengtao and Sonntag, Daniel and Zou, James Y and Nguyen, Binh T and Niepert, Mathias.*

  [[Paper](https://arxiv.org/abs/2405.16148)] [[Code](https://github.com/hchautran/PiToMe)] ![](https://img.shields.io/badge/PiToMe-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[5] MADTP: Multimodal Alignment-Guided Dynamic Token Pruning for Accelerating Vision-Language Transformer**, CVPR 2024.

  *Cao, Jianjian and Ye, Peng and Li, Shengze and Yu, Chong and Tang, Yansong and Lu, Jiwen and Chen, Tao.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024/html/Cao_MADTP_Multimodal_Alignment-Guided_Dynamic_Token_Pruning_for_Accelerating_Vision-Language_Transformer_CVPR_2024_paper.html)] [[Code](https://github.com/double125/MADTP)] ![](https://img.shields.io/badge/MADTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[6] DeCo: Decoupling Token Compression from Semantic Abstraction in Multimodal Large Language Models**, arXiv 2024.

  *Yao, Linli and Li, Lei and Ren, Shuhuai and Wang, Lean and Liu, Yuanxin and Sun, Xu and Hou, Lu.*

  [[Paper](https://arxiv.org/abs/2405.20985)] [[Code](https://github.com/yaolinli/DeCo)] ![](https://img.shields.io/badge/DeCo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[7] Token-level Correlation-guided Compression for Efficient Multimodal Document Understanding**, arXiv 2024.

  *Renshan Zhang, Yibo Lyu, Rui Shao, Gongwei Chen, Weili Guan and Liqiang Nie.*

  [[Paper](https://arxiv.org/abs/2407.14439)] [[Code](https://github.com/JiuTian-VL/TokenCorrCompressor)] ![](https://img.shields.io/badge/TokenCorrCompressor-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[8] Turbo: Informativity-Driven Acceleration Plug-In for Vision-Language Large Models**, ECCV 2024.

  *Ju, Chen and Wang, Haicheng and Li, Zeqian and Chen, Xu and Zhai, Zhonghua and Huang, Weilin and Xiao, Shuai.*

  [[Paper](https://arxiv.org/abs/2407.11717)] [Code] ![](https://img.shields.io/badge/Turbo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Image_Captioning-green) ![](https://img.shields.io/badge/Multi_Modal_Retrieval-green) ![](https://img.shields.io/badge/Text2Image-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[9] mPLUG-DocOwl 1.5: Unified Structure Learning for OCR-free Document Understanding**, Findings of EMNLP 2024.

  *Hu, Anwen and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Zhang, Liang and Zhang, Bo and Li, Chen and Zhang, Ji and Jin, Qin and Huang, Fei and others.*

  [[Paper](https://arxiv.org/abs/2403.12895)] [[Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl1.5)] ![](https://img.shields.io/badge/mPLUG_DocOwl1.5-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[10] LLaVA-VID: An Image is Worth 2 Tokens in Large Language Models**, ECCV 2024.

  *Li, Yanwei and Wang, Chengyao and Jia, Jiaya.*

  [[Paper](https://arxiv.org/abs/2311.17043)] [[Code](https://github.com/dvlab-research/LLaVA-VID)] ![](https://img.shields.io/badge/LLaMA_VID-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[11] ZipVL: Efficient Large Vision-Language Models with Dynamic Token Sparsification**, arXiv 2024.

  *He, Yefei and Chen, Feng and Liu, Jing and Shao, Wenqi and Zhou, Hong and Zhang, Kaipeng and Zhuang, Bohan.*

  [[Paper](https://arxiv.org/abs/2410.08584)] [Code] ![](https://img.shields.io/badge/ZipVL-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[12] ATP-LLaVA: Adaptive Token Pruning for Large Vision Language Models**, arXiv 2024.

  *Ye, Xubing and Gan, Yukang and Ge, Yixiao and Zhang, Xiao-Ping and Tang, Yansong.*

  [[Paper](https://arxiv.org/abs/2412.00447)] [[Code](https://yxxxb.github.io/ATP-LLaVA-page/)] ![](https://img.shields.io/badge/ATP_LLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[13] Efficient Multi-modal Large Language Models via Visual Token Grouping**, arXiv 2024.

  *Huang, Minbin and Huang, Runhui and Shi, Han and Chen, Yimeng and Zheng, Chuanyang and Sun, Xiangguo and Jiang, Xin and Li, Zhenguo and Cheng, Hong.*

  [[Paper](https://arxiv.org/abs/2411.17773)] [Code] ![](https://img.shields.io/badge/VisToG-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[14] Fit and Prune: Fast and Training-free Visual Token Pruning for Multi-modal Large Language Models**, arXiv 2024.

  *Ye, Weihao and Wu, Qiong and Lin, Wenhao and Zhou, Yiyi.*

  [[Paper](https://arxiv.org/abs/2409.10197)] [[Code](https://github.com/ywh187/FitPrune)] ![](https://img.shields.io/badge/FitPrune-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[15] Video Token Sparsification for Efficient Multimodal LLMs in Autonomous Driving**, arXiv 2024.

  *Ma, Yunsheng and Abdelraouf, Amr and Gupta, Rohit and Wang, Ziran and Han, Kyungtae.*

  [[Paper](https://arxiv.org/abs/2409.11182)] [Code] ![](https://img.shields.io/badge/VTS-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Saliency_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[16] iLLaVA: An Image is Worth Fewer Than 1/3 Input Tokens in Large Multimodal Models**, arXiv 2024.

  *Hu, Lianyu and Shang, Fanhua and Wan, Liang and Feng, Wei.*

  [[Paper](https://arxiv.org/abs/2412.06263)] [[Code](https://github.com/hulianyuyy/iLLaVA)] ![](https://img.shields.io/badge/iLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[17] mPLUG-DocOwl2: High-resolution Compressing for OCR-free Multi-page Document Understanding**, arXiv 2024.

  *Anwen Hu and Haiyang Xu and Liang Zhang and Jiabo Ye and Ming Yan and Ji Zhang and Qin Jin and Fei Huang and Jingren Zhou.*

  [[Paper](https://arxiv.org/abs/2409.03420)] [[Code](https://github.com/X-PLUG/mPLUG-DocOwl/tree/main/DocOwl2)] ![](https://img.shields.io/badge/mPLUG_DocOwl2-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Document_Understanding-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[18] VLTP: Vision-Language Guided Token Pruning for Task-Oriented Segmentation**, arXiv 2024.

  *Chen, Hanning and Ni, Yang and Huang, Wenjun and Liu, Yezi and Jeong, SungHeon and Wen, Fei and Bastian, Nathaniel and Latapie, Hugo and Imani, Mohsen.*

  [[Paper](https://arxiv.org/abs/2409.08464)] [Code] ![](https://img.shields.io/badge/VLTP-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[19] CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers**, ICML 2024.

  *Shi, Dachuan and Tao, Chaofan and Rao, Anyi and Yang, Zhendong and Yuan, Chun and Wang, Jiaqi.*

  [[Paper](https://arxiv.org/abs/2305.17455)] [Code] ![](https://img.shields.io/badge/CrossGET-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Image_Captioning-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/CrossModal_Based-purple) ![](https://img.shields.io/badge/Token_Ensemble-orange)

- **[20] Less is More: A Simple yet Effective Token Reduction Method for Efficient Multi-modal LLMs**, arXiv 2024.

  *Song, Dingjie and Wang, Wenjun and Chen, Shunian and Wang, Xidong and Guan, Michael and Wang, Benyou.*

  [[Paper](https://arxiv.org/abs/2409.10994)] [[Code](https://github.com/FreedomIntelligence/TRIM/)] ![](https://img.shields.io/badge/LessIsMore-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[21] An Image is Worth 1/2 Tokens After Layer 2: Plug-and-Play Inference Acceleration for Large Vision-Language Models**, ECCV 2024.

  *Chen, Liang and Zhao, Haozhe and Liu, Tianyu and Bai, Shuai and Lin, Junyang and Zhou, Chang and Chang, Baobao.*

  [[Paper](https://arxiv.org/abs/2403.06764)] [[Code](https://github.com/pkunlp-icler/FastV)] ![](https://img.shields.io/badge/FastV-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[22] Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See**, arXiv 2024.

  *Zhang, Zeliang and Pham, Phu and Zhao, Wentian and Wan, Kun and Li, Yu-Jhe and Zhou, Jianing and Miranda, Daniel and Kale, Ajinkya and Xu, Chenliang.*

  [[Paper](https://arxiv.org/abs/2410.06169)] [[Code](https://github.com/ZhangAIPI/YOPO_MLLM_Pruning)] ![](https://img.shields.io/badge/VisualAsText-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Head_Pruning-orange)

- **[23] PAR: Prompt-Aware Token Reduction Method for Efficient Large Multimodal Models**, arXiv 2024.

  *Wu, Tianxiang and Nie, Minxin and Cao, Ziqiang.*

  [[Paper](https://arxiv.org/abs/2410.07278)] [Code] ![](https://img.shields.io/badge/PAR-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[24] Is Less More? Exploring Token Condensation as Training-free Adaptation for CLIP**, arXiv 2024.

  *Wang, Zixin and Gong, Dong and Wang, Sen and Huang, Zi and Luo, Yadan.*

  [[Paper](https://arxiv.org/abs/2410.14729)] [Code] ![](https://img.shields.io/badge/TokenCond-blue) ![](https://img.shields.io/badge/Image_Text_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[25] Accelerating MLLMs by Searching Optimal Vision Token Reduction**, arXiv 2024.

  *Zhao, Shiyu and Wang, Zhenting and Juefei-Xu, Felix and Xia, Xide and Liu, Miao and Wang, Xiaofang and Liang, Mingfu and Zhang, Ning and Metaxas, Dimitris N and Yu, Licheng.*

  [[Paper](https://arxiv.org/abs/2412.00556)] [Code] ![](https://img.shields.io/badge/G_Search-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[26] [CLS] Attention is All You Need for Training-Free Visual Token Pruning: Make VLM Inference Faster**, arXiv 2024.

  *Zhang, Qizhe and Cheng, Aosong and Lu, Ming and Zhuo, Zhiyong and Wang, Minqi and Cao, Jiajun and Guo, Shaobo and She, Qi and Zhang, Shanghang.*

  [[Paper](https://arxiv.org/abs/2412.01818)] [[Code](https://github.com/Theia-4869/FasterVLM)] ![](https://img.shields.io/badge/FasterVLM-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[27] FoPru: Focal Pruning for Efficient Large Vision-Language Models**, arXiv 2024.

  *Lei Jiang and Weizhe Huang and Tongxuan Liu and Yuting Zeng and Jing Li and Lechao Cheng and Xiaohua Xu.*

  [[Paper](https://arxiv.org/abs/2411.14164)] [Code] ![](https://img.shields.io/badge/FoPru-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[28] Inference Optimal VLMs Need Only One Visual Token but Larger Models**, arXiv 2024.

  *Li, Kevin Y and Goyal, Sachin and Semedo, Joao D and Kolter, J Zico.*

  [[Paper](https://arxiv.org/abs/2411.03312)] [[Code](https://github.com/locuslab/llava-token-compression)] ![](https://img.shields.io/badge/QueCC-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[29] VASparse: Towards Efficient Visual Hallucination Mitigation for Large Vision-Language Model via Visual-Aware Sparsification**, arXiv 2024.

  *Xianwei Zhuang and Zhihong Zhu and Yuxin Xie and Liming Liang and Yuexian Zou.*

  [[Paper](https://arxiv.org/abs/2501.06553)] [[Code](https://github.com/mengchuang123/VASparse-github)] ![](https://img.shields.io/badge/VASparse-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Visual_Hallucination-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[30] Rethinking Token Reduction in MLLMs: Towards a Unified Paradigm for Training-Free Acceleration**, arXiv 2024.

  *Han, Yuhang and Liu, Xuyang and Ding, Pengxiang and Wang, Donglin and Chen, Honggang and Yan, Qingsen and Huang, Siteng.*

  [[Paper](https://arxiv.org/abs/2411.17686)] [Code] ![](https://img.shields.io/badge/FiCoCo-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[31] [CLS] Token Tells Everything Needed for Training-free Efficient MLLMs**, arXiv 2024.

  *Wang, Ao and Sun, Fengyuan and Chen, Hui and Lin, Zijia and Han, Jungong and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2412.05819)] [[Code](https://github.com/THU-MIG/VTC-CLS)] ![](https://img.shields.io/badge/VTC_CLS-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[32] FOLDER: Accelerating Multi-modal Large Language Models with Enhanced Performance**, arXiv 2024.

  *Haicheng Wang and Zhemeng Yu and Gabriele Spadaro and Chen Ju and Victor Quétu and Enzo Tartaglione.*

  [[Paper](https://arxiv.org/abs/2501.02430)] [[Code](https://github.com/anakin-skywalker-Joseph/Folder)] ![](https://img.shields.io/badge/FOLDER-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[33] FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression**, arXiv 2024.

  *Zhu, Yuke and Xie, Chi and Liang, Shuang and Zheng, Bo and Guo, Sheng.*

  [[Paper](https://arxiv.org/abs/2411.14228)] [Code] ![](https://img.shields.io/badge/FocusLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[34] TURBO: Token Utilization Refactor and Boost in Transformers for Efficient Vision-Language Models**, arXiv 2024.

  *Liu, Xin and Li, Yue and Jiang, Hao and Xu, Mengmi.*

  [[Paper](https://arxiv.org/abs/2405.23456)] [Code] ![](https://img.shields.io/badge/TURBO-blue) ![](https://img.shields.io/badge/Token_Reuse-green) ![](https://img.shields.io/badge/Token_Redistribution-green) ![](https://img.shields.io/badge/Token_Refactor-brown)

- **[35] FocusLLaVA: A Coarse-to-Fine Approach for Efficient and Effective Visual Token Compression**, arXiv 2024.

  *Zhu, Yuke and Xie, Chi and Liang, Shuang and Zheng, Bo and Guo, Sheng.*

  [[Paper](https://arxiv.org/abs/2411.14228)] [Code] ![](https://img.shields.io/badge/FocusLLaVA-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

### 2025

- **[36] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, ICLR 2025.

  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2409.01156)] [Code] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Text_Video_Retrieval-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[37] ST<sup>3</sup>: Accelerating Multimodal Large Language Model by Spatial-Temporal Visual Token Trimming**, AAAI 2025.

  *Jiedong Zhuang and Lu Lu and Ming Dai and Rui Hu and Jian Chen and Qiang Liu and Haoji Hu.*

  [[Paper](https://arxiv.org/abs/2412.20105)] [Code] ![](https://img.shields.io/badge/ST3-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[38] HiRED: Attention-Guided Token Dropping for Efficient Inference of High-Resolution Vision-Language Models**, AAAI 2025.

  *Arif, Kazi Hasan Ibn and Yoon, JinYi and Nikolopoulos, Dimitrios S and Vandierendonck, Hans and John, Deepu and Ji, Bo.*

  [[Paper](https://arxiv.org/abs/2408.10945)] [[Code](https://github.com/hasanar1f/HiRED)] ![](https://img.shields.io/badge/HiRED-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[39] Boosting Multimodal Large Language Models with Visual Tokens Withdrawal for Rapid Inference**, AAAI 2025.

  *Lin, Zhihang and Lin, Mingbao and Lin, Luxi and Ji, Rongrong.*

  [[Paper](https://arxiv.org/abs/2405.05803)] [[Code](https://github.com/lzhxmu/VTW)] ![](https://img.shields.io/badge/VTW-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/KL_Divergence_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[40] LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One Vision Token**, ICLR 2025.

  *Shaolei Zhang and Qingkai Fang and Zhe Yang and Yang Feng.*

  [[Paper](https://arxiv.org/abs/2501.03895)] [[Code](https://github.com/ictnlp/LLaVA-Mini)] ![](https://img.shields.io/badge/LLaVA_Mini-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[41] Recoverable Compression: A Multimodal Vision Token Recovery Mechanism Guided by Text Information**, AAAI 2025.

  *Chen, Yi and Xu, Jian and Zhang, Xu-Yao and Liu, Wen-Zhuo and Liu, Yang-Yang and Liu, Cheng-Lin.*

  [[Paper](https://arxiv.org/abs/2409.01179)] [[Code](https://github.com/banjiuyufen/RecoverableCompression)] ![](https://img.shields.io/badge/Recoverable_Compression-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[42] What Kind of Visual Tokens Do We Need? Training-free Visual Token Pruning for Multi-modal Large Language Models from the Perspective of Graph**, AAAI 2025.

  *Yutao Jiang and Qiong Wu and Wenhao Lin and Wei Yu and Yiyi Zhou.*

  [[Paper](https://arxiv.org/abs/2501.02268)] [Code] ![](https://img.shields.io/badge/LLaVA_Mini-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[43] FrameFusion: Combining Similarity and Importance for Video Token Reduction on Large Visual Language Models**, arXiv 2025.

  *Fu, Tianyu and Liu, Tengxuan and Han, Qinghao and Dai, Guohao and Yan, Shengen and Yang, Huazhong and Ning, Xuefei and Wang, Yu.*

  [[Paper](https://arxiv.org/abs/2501.01986)] [[Code](https://thu-nics.github.io/FrameFusion_Project_Page/)] ![](https://img.shields.io/badge/FrameFusion-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[44] Compression with Global Guidance: Towards Training-free High-Resolution MLLMs Acceleration**, arXiv 2025.

  *Xuyang Liu and Ziming Wang and Yuhang Han and Yingyao Wang and Jiale Yuan and Jun Song and Bo Zheng and Linfeng Zhang and Siteng Huang and Honggang Chen.*

  [[Paper](https://arxiv.org/abs/2501.05179)] [[Code](https://github.com/xuyang-liu16/GlobalCom2)] ![](https://img.shields.io/badge/GlobalCom2-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[45] AdaFV: Accelerating VLMs with Self-Adaptive Cross-Modality Attention Mixture**, arXiv 2025.

  *Jiayi Han and Liang Du and Yiwen Wu and Xiangguo Zhou and Hongwei Du and Weibo Zheng.*

  [[Paper](https://arxiv.org/abs/2501.09532)] [Code] ![](https://img.shields.io/badge/AdaFV-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[46] Dynamic Token Reduction during Generation for Vision Language Models**, arXiv 2025.

  *Xiaoyu Liang and Chaofeng Guan and Jiaying Lu and Huiyao Chen and Huan Wang and Haoji Hu.*

  [[Paper](https://arxiv.org/abs/2501.14204)] [Code] ![](https://img.shields.io/badge/DyRate-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[47] Stop Looking for “Important Tokens” in Multimodal Language Models: Duplication Matters More**, arXiv 2025.

  *Zichen Wen and Yifeng Gao and Shaobo Wang and Junyuan Zhang and Qintong Zhang and Weijia Li and Conghui He and Linfeng Zhang*

  [[Paper](https://arxiv.org/abs/2502.11494)] [[Code](https://github.com/ZichenWen1/DART)] ![](https://img.shields.io/badge/DART-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[48] AdaRETAKE: Adaptive Redundancy Reduction to Perceive Longer for Video-language Understanding**, arXiv 2025.

  *Xiao Wang and Qingyi Si and Jianlong Wu and Shiyu Zhu and Li Cao and Liqiang Nie.*

  [[Paper](https://arxiv.org/abs/2503.12559)] [[Code](https://github.com/SCZwangxiao/video-FlexReduc)] ![](https://img.shields.io/badge/AdaRETAKE-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[49] Video Compression Commander: Plug-and-Play Inference Acceleration for Video Large Language Models**, arXiv 2025.

  *Liu, Xuyang and Wang, Yiyu and Ma, Junpeng and Zhang, Linfeng.*

  [[Paper](https://arxiv.org/abs/2505.14454)] [[Code](https://github.com/xuyang-liu16/VidCom2)] ![](https://img.shields.io/badge/VidCom2-blue) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[50] VScan: Rethinking Visual Token Reduction for Efficient Large Vision-Language Models**, arXiv 2025.

  *Zhang, Ce and Ma, Kaixin and Fang, Tianqing and Yu, Wenhao and Zhang, Hongming and Zhang, Zhisong and Xie, Yaqi and Sycara, Katia and Mi, Haitao and Yu, Dong.*

  [[Paper](https://arxiv.org/abs/2505.22654)] [Code] ![](https://img.shields.io/badge/VScan-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[51] Balanced Token Pruning: Accelerating Vision Language Models Beyond Local Optimization**, arXiv 2025.

  *Li, Kaiyuan and Chen, Xiaoyue and Gao, Chen and Li, Yong and Chen, Xinlei.*

  [[Paper](https://arxiv.org/abs/2505.22038)] [Code] ![](https://img.shields.io/badge/BTP-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[52] EffiVLM-Bench: A Comprehensive Benchmark for Evaluating Training-Free Acceleration in Large Visual-Languge Models**, ACL 2025.

  *Zekun Wang, MingHua Ma, Zexin Wang, Rongchuan Mu, liping shan, Ming Liu, Bing Qin.*

  [[Paper](https://arxiv.org/abs/2506.00479)] [[Code](https://github.com/EffiVLM-Bench/EffiVLM-Bench)] ![](https://img.shields.io/badge/EffiVLM_Bench-blue) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/VideoQA-green) ![](https://img.shields.io/badge/Training_Free-brown)

  
