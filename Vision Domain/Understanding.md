<div align=center>

# Token Compression for Vision Domain

:loudspeaker: Collections of Awesome Token Compression for Vision Understanding Domain Resources.

</div>


## ðŸ“š <span id="head1"> *Contents* </span>

- Token Compression for Vision Domain
  - [Image Recognition](#Image-Recognition)
  - [Video Recognition](#Video-Recognition)
  - [Dense Prediction](#Dense-Prediction)


### Image Recognition
### 2021
- **[1] IA-REDÂ²: Interpretability-aware Redundancy Reduction for Vision Transformers**, NeurIPS 2021.
  
  *Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude.*

  [[Paper](https://arxiv.org/abs/2106.12620)] [Code] ![](https://img.shields.io/badge/IA_RED2-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Pan2021:IA-RED2,
    title={IA-RED $\^{} 2$: Interpretability-aware redundancy reduction for vision transformers},
    author={Pan, Bowen and Panda, Rameswar and Jiang, Yifan and Wang, Zhangyang and Feris, Rogerio and Oliva, Aude},
    booktitle=NIPS,
    volume={34},
    pages={24898--24911},
    year={2021}
  }
  ```
  </details>  

- **[2] DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification**, NeurIPS 2021.
  
  *Rao, Yongming and Zhao, Wenliang and Liu, Benlin and Lu, Jiwen and Zhou, Jie and Hsieh, Cho-Jui.*

  [[Paper](https://arxiv.org/abs/2106.02034)] [[Code](https://github.com/RaoYongming/DynamicViT)] ![](https://img.shields.io/badge/DynamicViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Object_Detection-green) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Rao2021:DynamicViT,
    title={{DynamicViT}: Efficient Vision Transformers with Dynamic Token Sparsification},
    author={Yongming Rao and Wenliang Zhao and Benlin Liu and Jiwen Lu and Jie Zhou and Cho{-}Jui Hsieh},
    booktitle=NIPS,
    volume={34},
    pages={13937--13949},
    year={2021}
  }
  ```
  </details>  

- **[3]  Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition**, NeurIPS 2021.

  *Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao.*

  [[Paper](https://arxiv.org/abs/2105.15075)] [[Code](https://github.com/blackfeather-wang/Dynamic-Vision-Transformer)] ![](https://img.shields.io/badge/DVT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)
  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{wang2021dvt,
    title={Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition},
    author={Wang, Yulin and Huang, Rui and Song, Shiji and Huang, Zeyi and Huang, Gao},
    booktitle=NIPS,
    volume={34},
    pages={11960--11973},
    year={2021}
  }
  ```
  </details>  


### 2022

- **[1] Adaptive Token Sampling for Efficient Vision Transformers**, ECCV 2022.
  
  *Fayyaz, Mohsen and Koohpayegani, Soroush Abbasi and Jafari, Farnoush Rezaei and Sengupta, Sunando and Joze, Hamid Reza Vaezi and Sommerlade, Eric and Pirsiavash, Hamed and Gall, JÃ¼rgen.*

  [[Paper](https://arxiv.org/abs/2111.15667)] [[Code](https://adaptivetokensampling.github.io/)] ![](https://img.shields.io/badge/ATS-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[2] SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning**, ECCV 2022.
  
  *Kong, Zhenglun and Dong, Peiyan and Ma, Xiaolong and Meng, Xin and Niu, Wei and Sun, Mengshu and Shen, Xuan and Yuan, Geng and Ren, Bin and Tang, Hao and others.*

  [[Paper](https://arxiv.org/abs/2112.13890)] [[Code](https://github.com/PeiyanFlying/SPViT)] ![](https://img.shields.io/badge/SPViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Kong2022:SPViT,
    title={Spvit: Enabling faster vision transformers via latency-aware soft token pruning},
    author={Kong, Zhenglun and Dong, Peiyan and Ma, Xiaolong and Meng, Xin and Niu, Wei and Sun, Mengshu and Shen, Xuan and Yuan, Geng and Ren, Bin and Tang, Hao and others},
    booktitle={European conference on computer vision},
    pages={620--640},
    year={2022},
    organization={Springer}
  }
  ```
  </details>  
  
- **[3] SaiT: Sparse Vision Transformers through Adaptive Token Pruning**, arXiv 2022.
  
  *Li, Ling and Thorsley, David and Hassoun, Joseph.*

  [[Paper](https://arxiv.org/abs/2210.05832)] [Code] ![](https://img.shields.io/badge/SaiT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
   @article{Li2022SaiT,
    title={Sait: Sparse vision transformers through adaptive token pruning},
    author={Li, Ling and Thorsley, David and Hassoun, Joseph},
    journal={arXiv preprint arXiv:2210.05832},
    year={2022}
  }
  ```
  </details>  

- **[4] Not all patches are what you need: Expediting vision transformers via token reorganizations**, ICLR 2022.
  
  *Liang, Youwei and Ge, Chongjian and Tong, Zhan and Song, Yibing and Wang, Jue and Xie, Pengtao.*

  [[Paper](https://arxiv.org/abs/2202.07800)] [[Code](https://github.com/youweiliang/evit)] ![](https://img.shields.io/badge/EViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[5] Patch Slimming for Efficient Vision Transformers**, CVPR 2022.
  
  *Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng.*

  [[Paper](https://arxiv.org/abs/2106.02852)] [Code] ![](https://img.shields.io/badge/Patch_Slimming-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Tang2022:PatchSlim,
    title={Patch slimming for efficient vision transformers},
    author={Tang, Yehui and Han, Kai and Wang, Yunhe and Xu, Chang and Guo, Jianyuan and Xu, Chao and Tao, Dacheng},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={12165--12174},
    year={2022}
  }
  ```
  </details>  

- **[6] Evo-ViT: Slow-Fast Token Evolution for Dynamic Vision Transformer**, AAAI 2022.
  
  *Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing.*

  [[Paper](https://arxiv.org/abs/2108.01390)] [[Code](https://github.com/YifanXu74/Evo-ViT)] ![](https://img.shields.io/badge/EvoViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Xu2022:Evo-ViT,
  title={Evo-vit: Slow-fast token evolution for dynamic vision transformer},
  author={Xu, Yifan and Zhang, Zhijie and Zhang, Mengdan and Sheng, Kekai and Li, Ke and Dong, Weiming and Zhang, Liqing and Xu, Changsheng and Sun, Xing},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={36},
  number={3},
  pages={2964--2972},
  year={2022}
  }
  ```
  </details>  

- **[7] A-ViT: Adaptive Tokens for Efficient Vision Transformer**, CVPR 2022.
  
  *Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo.*

  [[Paper](https://arxiv.org/abs/2112.07658)] [[Code](https://github.com/hongxuYin/A-ViT)] ![](https://img.shields.io/badge/A_ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Yin2022:A-ViT,
    title={A-vit: Adaptive tokens for efficient vision transformer},
    author={Yin, Hongxu and Vahdat, Arash and Alvarez, Jose M and Mallya, Arun and Kautz, Jan and Molchanov, Pavlo},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={10809--10818},
    year={2022}
  }
  ```
  </details>  

- **[8] Learning to Merge Tokens in Vision Transformers**, arXiv 2022.
  
  *Renggli, Cedric and Pinto, AndrÃ© Susano and Houlsby, Neil and Mustafa, Basil and Puigcerver, Joan and Riquelme, Carlos.*

  [[Paper](https://arxiv.org/abs/2202.12015)] [Code] ![](https://img.shields.io/badge/Patch_Merger-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-purple)

- **[9] Self-slimmed Vision Transformer**, ECCV 2022.
  
  *Zong, Zhuofan and Li, Kunchang and Song, Guanglu and Wang, Yali and Qiao, Yu and Leng, Biao and Liu, Yu.*

  [[Paper](https://arxiv.org/abs/2111.12624)] [[Code](https://github.com/Sense-X/SiT)] ![](https://img.shields.io/badge/SelfSlimmedViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-purple)

  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{Zong2022:SiT,
    title={Self-slimmed vision transformer},
    author={Zong, Zhuofan and Li, Kunchang and Song, Guanglu and Wang, Yali and Qiao, Yu and Leng, Biao and Liu, Yu},
    booktitle={European Conference on Computer Vision},
    pages={432--448},
    year={2022},
    organization={Springer}
  }
  ```
  </details>  

  
### 2023

- **[1] Token Pooling in Vision Transformers for Image Classification**, WACV 2023.
  
  *Marin, Dmitrii and Chang, Jen-Hao Rick and Ranjan, Anurag and Prabhu, Anish and Rastegari, Mohammad and Tuzel, Oncel.*

  [[Paper](https://arxiv.org/abs/2110.03860)] [Code] ![](https://img.shields.io/badge/TokenPooling-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>
  
  ```text
  @inproceedings{Marin2023:TokenPooling,
    title={Token pooling in vision transformers for image classification},
    author={Marin, Dmitrii and Chang, Jen-Hao Rick and Ranjan, Anurag and Prabhu, Anish and Rastegari, Mohammad and Tuzel, Oncel},
    booktitle={Proceedings of the IEEE/CVF winter conference on applications of computer vision},
    pages={12--21},
    year={2023}
  }
  ```
  </details>  
  
- **[2] Beyond attentive tokens: Incorporating token importance and diversity for efficient vision transformers**, CVPR 2023.
  
  *Long, Sifan and Zhao, Zhen and Pi, Jimin and Wang, Shengsheng and Wang, Jingdong.*

  [[Paper](https://arxiv.org/abs/2211.11315)] [Code] ![](https://img.shields.io/badge/BAT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
  
  ```text
  @inproceedings{Long2023:BAT,
    title={Beyond attentive tokens: Incorporating token importance and diversity for efficient vision transformers},
    author={Long, Sifan and Zhao, Zhen and Pi, Jimin and Wang, Shengsheng and Wang, Jingdong},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={10334--10343},
    year={2023}
  }
  ```
  </details> 
  
- **[3] Which Tokens to Use? Investigating Token Reduction in Vision Transformers**, ICCVw 2023.
  
  *Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B.*

  [[Paper](https://arxiv.org/abs/2308.04657)] [[Code](https://github.com/JoakimHaurum/TokenReduction)] ![](https://img.shields.io/badge/TokenReduction-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Training_Free-brown) 

- **[4] Efficient Vision Transformer via Token Merger**, TIP 2023.
  
  *Feng, Zhanzhou and Zhang, Shiliang.*

  [[Paper](https://ieeexplore.ieee.org/abstract/document/10183862)] [Code] ![](https://img.shields.io/badge/TokenMerger-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
    
  ```
  @article{Feng2023TokenMerger,
    title={Efficient vision transformer via token merger},
    author={Feng, Zhanzhou and Zhang, Shiliang},
    journal=TIP,
    year={2023},
    publisher={IEEE}
  }
  ```
  </details>  

- **[5] Token Merging: Your ViT But Faster**, ICLR 2023.
  
  *Bolya, Daniel and Fu, Cheng-Yang and Dai, Xiaoliang and Zhang, Peizhao and Feichtenhofer, Christoph and Hoffman, Judy.*

  [[Paper](https://arxiv.org/abs/2210.09461)] [[Code]([https://github.com/facebookresearch/ToMe](https://github.com/facebookresearch/ToMe))] ![](https://img.shields.io/badge/ToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
      
  ```
  @inproceedings{Bolya2023:ToMe,
    title={Token Merging: Your {ViT} But Faster},
    author={Daniel Bolya and Cheng{-}Yang Fu and Xiaoliang Dai and Peizhao Zhang and Christoph Feichtenhofer and Judy Hoffman},
    booktitle=ICLR,
    year= {2023}
  }
  ```
  </details>  


### 2024

- **[1] Efficient Transformer Adaptation with Soft Token Merging**, CVPRw 2024.
  
  *Yuan, Xin and Fei, Hongliang and Baek, Jinoo.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2024W/ELVM/papers/Yuan_Efficient_Transformer_Adaptation_with_Soft_Token_Merging_CVPRW_2024_paper.pdf)] [Code] ![](https://img.shields.io/badge/SoftToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Machine_Translation-green) ![](https://img.shields.io/badge/Visual_Question_Answering-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
    
  ```
  @inproceedings{Yuan2024:SoftToMe,
    title={Efficient transformer adaptation with soft token merging},
    author={Yuan, Xin and Fei, Hongliang and Baek, Jinoo},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={3658--3668},
    year={2024}
  }
  ```
  </details>  

- **[2] Agglomerative Token Clustering**, ECCV 2024.
  
  *Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B.*

  [[Paper](https://arxiv.org/abs/2409.11923)] [[Code](https://github.com/JoakimHaurum/ATC)] ![](https://img.shields.io/badge/ATC-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Image_Generation-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
    
  ```
  @inproceedings{haurum2024agglomerative,
    title={Agglomerative Token Clustering},
    author={Haurum, Joakim Bruslund and Escalera, Sergio and Taylor, Graham W and Moeslund, Thomas B},
    booktitle={European Conference on Computer Vision},
    pages={200--218},
    year={2024},
    organization={Springer}
  }
  ```
  </details>  

- **[3] Token Pruning using a Lightweight Background Aware Vision Transformer**, NeurIPSw 2024.
  
  *Sah, Sudhakar and Kumar, Ravish and Rohmetra, Honnesh and Saboori, Ehsan.*

  [[Paper](https://arxiv.org/abs/2410.09324)] [Code] ![](https://img.shields.io/badge/BAViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] GTP-ViT: Efficient Vision Transformers via Graph-Based Token Propagation**, WACV 2024.
  
  *Xu, Xuwei and Wang, Sen and Chen, Yudong and Zheng, Yanping and Wei, Zhewei and Liu, Jiajun.*

  [[Paper](https://arxiv.org/abs/2311.03035)] [[Code](https://github.com/Ackesnal/GTP-ViT)] ![](https://img.shields.io/badge/GTP_ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

  <details> <summary>BibTex</summary>
    
  ```
  @inproceedings{Xu2024:GTP-ViT,
    title={GTP-ViT: efficient Vision transformers via graph-based token propagation},
    author={Xu, Xuwei and Wang, Sen and Chen, Yudong and Zheng, Yanping and Wei, Zhewei and Liu, Jiajun},
    booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
    pages={86--95},
    year={2024}
  }
  ```
  </details>  


- **[5] PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference**, arXiv 2024.
  
  *Li, Ye and Tang, Chen and Meng, Yuan and Fan, Jiajun and Chai, Zenghao and Ma, Xinzhu and Wang, Zhi and Zhu, Wenwu.*

  [[Paper](https://arxiv.org/abs/2407.05010)] [Code] ![](https://img.shields.io/badge/PRANCE-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange) ![](https://img.shields.io/badge/Token_Merging-orange) ![](https://img.shields.io/badge/Channel_Pruning-orange)

- **[6] TPC-ViT: Token Propagation Controller for Efficient Vision Transformer**, WACV 2024.
  
  *Zhu, Wentao.*

  [[Paper](https://arxiv.org/abs/2401.01470)] [Code] ![](https://img.shields.io/badge/TPC_ViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Caching-orange)

- **[7] Efficient Visual Transformer by Learnable Token Merging**, arXiv 2024.
  
  *Wang, Yancheng and Yang, Yingzhen.*

  [[Paper](https://arxiv.org/abs/2407.15219)] [[Code](https://github.com/Statistical-Deep-Learning/LTM)] ![](https://img.shields.io/badge/LTM-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Dense_Prediction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[8] HeatViT: Hardware-Efficient Adaptive Token Pruning for Vision Transformers**, HPCA 2023.
  
  *Dong, Peiyan and Sun, Mengshu and Lu, Alec and Xie, Yanyue and Liu, Kenneth and Kong, Zhenglun and Meng, Xin and Li, Zhengang and Lin, Xue and Fang, Zhenman and others.*

  [[Paper](https://ieeexplore.ieee.org/abstract/document/10071047)] [Code] ![](https://img.shields.io/badge/HeatViT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[9] Learning to Merge Tokens via Decoupled Embedding for Efficient Vision Transformers**, arXiv 2024.
  
  *Lee, Dong Hoon and Hong, Seunghoon.*

  [[Paper](https://arxiv.org/abs/2412.10569)] [Code] ![](https://img.shields.io/badge/DEToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[10] Energy Minimizing-based Token Merging for Accelerating Transformers**, ICLRw 2024.
  
  *Tran, Hoai-Chau and Nguyen, Duy Minh Ho and Nguyen, Manh-Duy and Le, Ngan Hoang and Nguyen, Binh T.*

  [[Paper](https://openreview.net/forum?id=R7dCHc2Rp0)] [Code] ![](https://img.shields.io/badge/EMToMe-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Energy_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[11] Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers**, CVPR 2024.
  
  *Wang, Hongjie and Dedhia, Bhishma and Jha, Niraj K.*

  [[Paper](https://arxiv.org/abs/2305.17328)] [Code] ![](https://img.shields.io/badge/Zero_TPrune-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

   <details> <summary>BibTex</summary>
  
  ```
  @inproceedings{wang2024zero,
    title={Zero-TPrune: Zero-shot token pruning through leveraging of the attention graph in pre-trained transformers},
    author={Wang, Hongjie and Dedhia, Bhishma and Jha, Niraj K},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={16070--16079},
    year={2024}
  }
  ```
  </details> 

- **[12] PaPr: Training-Free One-Step Patch Pruning with Lightweight ConvNets for Faster Inference**, ECCV 2024.
  
  *Tanvir Mahmud, Burhaneddin Yaman, Chun-Hao Liu, Diana Marculescu.*

  [[Paper](https://arxiv.org/abs/2403.16020)] [[Code](https://github.com/tanvir-utexas/PaPr)] ![](https://img.shields.io/badge/PaPr-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Model_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[13] Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer**, arXiv 2024.
  
  *Shuai Peng, Di Fu, Baole Wei, Yong Cao, Liangcai Gao, Zhi Tang.*

  [[Paper](https://arxiv.org/abs/2408.17062)] [Code] ![](https://img.shields.io/badge/Vote&Mix-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Attention_Based-purple) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[14] Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation**, NeurIPS 2024.
  
  *Zhao, Wangbo and Tang, Jiasheng and Han, Yizeng and Song, Yibing and Wang, Kai and Huang, Gao and Wang, Fan and You, Yang.*

  [[Paper](https://arxiv.org/abs/2403.11808)] [[Code](https://github.com/NUS-HPC-AI-Lab/Dynamic-Tuning)] ![](https://img.shields.io/badge/DyT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

  <details> <summary>BibTex</summary>
    
  ```
  @inproceedings{Zhao2024:DyT,
    title={Dynamic Tuning Towards Parameter and Inference Efficiency for ViT Adaptation},
    author={Zhao, Wangbo and Tang, Jiasheng and Han, Yizeng and Song, Yibing and Wang, Kai and Huang, Gao and Wang, Fan and You, Yang},
    booktitle=NIPS,
    year={2024}
  }
  ```
  </details>  

  

### Video Recognition:

- **[1] TokenLearner: Adaptive Space-Time Tokenization for Videos**, NeurIPS 2021.
  
  *Michael Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, Anelia Angelova.*

  [[Paper](https://proceedings.neurips.cc/paper/2021/hash/6a30e32e56fce5cf381895dfe6ca7b6f-Abstract.html)] [[Code](https://github.com/google-research/scenic/tree/main/scenic/projects/token_learner)] ![](https://img.shields.io/badge/TokenLearner-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] Efficient Video Transformers with Spatial-Temporal Token Selection**, ECCV 2022.
  
  *Junke Wang and Xitong Yang and Hengduo Li and Li Liu and Zuxuan Wu and Yu-Gang Jiang.*

  [[Paper](https://arxiv.org/abs/2111.11591)] [[Code](https://github.com/wdrink/STTS)] ![](https://img.shields.io/badge/STTS-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[3] Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation**, ICCV 2023.
  
  *Shuangrui Ding, Peisen Zhao, Xiaopeng Zhang, Rui Qian, Hongkai Xiong, Qi Tian.*

  [[Paper](https://arxiv.org/abs/2308.04549)] [Code] ![](https://img.shields.io/badge/PSTA-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Semantic_Based-purple) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[4] vid-TLDR: Training Free Token merging for Light-weight Video Transformer**, CVPR 2024.
  
  *Joonmyung Choi and Sanghyeok Lee and Jaewon Chu and Minhyuk Choi and Hyunwoo J. Kim.*

  [[Paper](https://arxiv.org/abs/2403.13347)] [[Code](https://github.com/mlvlab/vid-TLDR)] ![](https://img.shields.io/badge/vid_TLDR-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Salience_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[5] Efficient Video Transformers via Spatial-temporal Token Merging for Action Recognition**, ACM MM 2024.
  
  *Zhanzhou Feng, Jiaming Xu, Lei Ma, Shiliang Zhang.*

  [[Paper](https://dl.acm.org/doi/abs/10.1145/3633781)] [Code] ![](https://img.shields.io/badge/STToMe-blue) ![](https://img.shields.io/badge/Action_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Spatio_Temporal-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[6] Don't Look Twice: Faster Video Transformers with Run-Length Tokenization**, NeurIPS 2024.

  *Choudhury, Rohan and Zhu, Guanglei and Liu, Sihan and Niinuma, Koichiro and Kitani, Kris M and Jeni, Laszlo Attila.*

  [[Paper](https://dl.acm.org/doi/abs/10.1145/3633781)] [[Code](https://github.com/rccchoudhury/rlt)] ![](https://img.shields.io/badge/RLT-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Temporal_Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)
  
- **[7] TempMe: Video Temporal Token Merging for Efficient Text-Video Retrieval**, arXiv 2024.
  
  *Shen, Leqi and Hao, Tianxiang and Zhao, Sicheng and Zhang, Yifeng and Liu, Pengzhang and Bao, Yongjun and Ding, Guiguang.*

  [[Paper](https://arxiv.org/abs/2409.01156)] [Code] ![](https://img.shields.io/badge/TempMe-blue) ![](https://img.shields.io/badge/Video_Recognition-green) ![](https://img.shields.io/badge/Text_Video_Retrieval-green) ![](https://img.shields.io/badge/Training_Free-brown) ![](https://img.shields.io/badge/Similarity_Based-purple) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[8] Efficient Video Action Detection with Token Dropout and Context Refinement**, ICCV 2023.
  
  *Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin.*

  [[Paper](https://arxiv.org/abs/2304.08451)] [[Code](https://github.com/MCG-NJU/EVAD)] ![](https://img.shields.io/badge/EVAD-blue) ![](https://img.shields.io/badge/Video_Action_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)


### Dense Prediction:

### 2022

- **[1] Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer**, CVPR 2022.
  
  *Zeng, Wang and Jin, Sheng and Liu, Wentao and Qian, Chen and Luo, Ping and Ouyang, Wanli and Wang, Xiaogang.*

  [[Paper](https://arxiv.org/abs/2204.08680)] [[Code](https://github.com/zengwang430521/TCFormer)] ![](https://img.shields.io/badge/TCFormer-blue) ![](https://img.shields.io/badge/Pose_Estimation-green) ![](https://img.shields.io/badge/Mesh_Reconstruction-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[2] GroupViT: Semantic Segmentation Emerges From Text Supervision**, CVPR 2022.
  
  *Xu, Jiarui and De Mello, Shalini and Liu, Sifei and Byeon, Wonmin and Breuel, Thomas and Kautz, Jan and Wang, Xiaolong.*

  [[Paper](https://arxiv.org/abs/2202.11094)] [[Code](https://github.com/NVlabs/GroupViT)] ![](https://img.shields.io/badge/GroupViT-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[3] Less is more: Pay less attention in vision transformers**, AAAI 2022.
  
  *Zizheng Pan and Bohan Zhuang and Haoyu He and Jing Liu and Jianfei Cai.*

  [[Paper](https://arxiv.org/abs/2105.14217)] [[Code](https://github.com/ziplab/LIT)] ![](https://img.shields.io/badge/LIT-blue) ![](https://img.shields.io/badge/Image_Recognition-green) ![](https://img.shields.io/badge/Object_Detection-green) ![](https://img.shields.io/badge/Instance_Segmentation-green) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

### 2023

- **[1] Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation**, ICCV 2023.
  
  *Tang, Quan and Zhang, Bowen and Liu, Jiajun and Liu, Fagui and Liu, Yifan.*

  [[Paper](https://openaccess.thecvf.com/content/ICCV2023/html/Tang_Dynamic_Token_Pruning_in_Plain_Vision_Transformers_for_Semantic_Segmentation_ICCV_2023_paper.html)] [Code] ![](https://img.shields.io/badge/DTP-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[2] Content-Aware Token Sharing for Efficient Semantic Segmentation With Vision Transformers**, CVPR 2023.
  
  *Lu, Chenyang and de Geus, Daan and Dubbelman, Gijs.*

  [[Paper](https://openaccess.thecvf.com/content/CVPR2023/html/Lu_Content-Aware_Token_Sharing_for_Efficient_Semantic_Segmentation_With_Vision_Transformers_CVPR_2023_paper.html)] [Code] ![](https://img.shields.io/badge/CATS-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Sharing-orange)

- **[3] Efficient Video Action Detection with Token Dropout and Context Refinement**, ICCV 2023.
  
  *Chen, Lei and Tong, Zhan and Song, Yibing and Wu, Gangshan and Wang, Limin.*

  [[Paper](https://arxiv.org/abs/2304.08451)] [[Code](https://github.com/MCG-NJU/EVAD)] ![](https://img.shields.io/badge/EVAD-blue) ![](https://img.shields.io/badge/Video_Action_Detection-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)
  
### 2024

- **[1] Revisiting token pruning for object detection and instance segmentation**, WACV 2024.

  *Liu, Yifei and Gehrig, Mathias and Messikommer, Nico and Cannici, Marco and Scaramuzza, Davide.*
  
  [[Paper](https://arxiv.org/abs/2306.07050)] [[Code](https://github.com/uzh-rpg/svit)] ![](https://img.shields.io/badge/SViT-blue) ![](https://img.shields.io/badge/Object_Detection-green) ![](https://img.shields.io/badge/Instance_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[2] Dynamic Token-Pass Transformers for Semantic Segmentation**, WACV 2024.

  *Liu, Yuang and Zhou, Qiang and Wang, Jin and Wang, Zhibin and Wang, Fan and Wang, Jun and Zhang, Wei.*
  
  [[Paper](https://arxiv.org/abs/2308.01944)] [[Code](https://github.com/FLHonker/DoViT-code)] ![](https://img.shields.io/badge/DoViT-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

- **[3] DTMFormer: Dynamic Token Merging for Boosting Transformer-Based Medical Image Segmentation**, AAAI 2024.

  *Wang, Zhehao and Lin, Xian and Wu, Nannan and Yu, Li and Cheng, Kwang-Ting and Yan, Zengqiang.*
  
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/28394)] [[Code](https://github.com/iam-nacl/DTMFormer)] ![](https://img.shields.io/badge/DTMFormer-blue) ![](https://img.shields.io/badge/Medical_Image_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[4] Segformer++: Efficient Token-Merging Strategies for High-Resolution Semantic Segmentation**, MIPR 2024.
  
  *Daniel Kienzle and Marco Kantonis and Robin SchÃ¶n and Rainer Lienhart.*
  
  [[Paper](https://arxiv.org/abs/2405.14467)] [Code] ![](https://img.shields.io/badge/Segformer++-blue) ![](https://img.shields.io/badge/Semantic_Segmentation-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Merging-orange)

- **[5] ToSA: Token Selective Attention for Efficient Vision Transformers**, arXiv 2024.
  
  *Singh, Manish Kumar et al.*
  
  [[Paper](https://arxiv.org/abs/2406.08816)] [Code] ![](https://img.shields.io/badge/ToSA-blue) ![](https://img.shields.io/badge/Vision_Recognition-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)

### 2025

- **[1] Less is More: Token Context-aware Learning for Object Tracking**, AAAI 2025.
  
  *Chenlong Xu and Bineng Zhong and Qihua Liang and Yaozong Zheng and Guorong Li and Shuxiang Song.*
  
  [[Paper](https://arxiv.org/abs/2501.00758)] [[Code](https://github.com/XuChenLong/LMTrack)] ![](https://img.shields.io/badge/LMTrack-blue) ![](https://img.shields.io/badge/Object_Tracking-green) ![](https://img.shields.io/badge/Training_Based-brown) ![](https://img.shields.io/badge/Token_Pruning-orange)



  



