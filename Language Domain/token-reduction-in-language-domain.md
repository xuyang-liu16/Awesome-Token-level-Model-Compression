# Token Reduction Techniques in Language Domain

Maintainers: [Junjie Chen](https://www.junjie-chen.info), [Xuyang Liu](https://xuyang-liu16.github.io/)

If your work is missing, please let us know by creating an issue or submitting a pull request.

Color code: ![Abbreviation](https://img.shields.io/badge/Abbreviation-blue) ![Application](https://img.shields.io/badge/Application-green) ![Reduction_Criteria](https://img.shields.io/badge/Reduction_Criteria-purple) ![Reduction_Mechanism](https://img.shields.io/badge/Reduction_Mechanism-orange) ![W./W.o._Training](https://img.shields.io/badge/W./W.o._Training-brown) ![Keywords](https://img.shields.io/badge/Keywords-yellow) 

## 1999

- **[1] What is linguistic redundancy**, University of Chicago 1999  
  *Wit, EC and Gillette, Marie*  
  [[Paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=182d3980d8f41616da690d607f6f980dbf582352)] ![Theory](https://img.shields.io/badge/Theory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wit1999linguistic,
    title={What is linguistic redundancy},
    author={Wit, EC and Gillette, Marie},
    journal={University of Chicago},
    year={1999},
    publisher={Citeseer}
  }
  ```
  </details>  

## 2016

- **[1] Phased lstm: Accelerating recurrent network training for long or event-based sequences**, NeurIPS 2016  
  *Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii*  
  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf)] ![Phased_lstm](https://img.shields.io/badge/Phased_lstm-blue) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{neil2016phased,
    title={Phased lstm: Accelerating recurrent network training for long or event-based sequences},
    author={Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
    journal={Advances in neural information processing systems},
    volume={29},
    year={2016}
  }
  ```
  </details>  

- **[2] Hierarchical question answering for long documents**, arXiv 2016  
  *Choi, Eunsol and Hewlett, Daniel and Lacoste, Alexandre and Polosukhin, Illia and Uszkoreit, Jakob and Berant, Jonathan*  
  [[Paper](https://arxiv.org/pdf/1611.01839)] ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{choi2016hierarchical,
    title={Hierarchical question answering for long documents},
    author={Choi, Eunsol and Hewlett, Daniel and Lacoste, Alexandre and Polosukhin, Illia and Uszkoreit, Jakob and Berant, Jonathan},
    journal={arXiv preprint arXiv:1611.01839},
    year={2016}
  }
  ```
  </details>  

## 2017

- **[1] Reasonet: Learning to stop reading in machine comprehension**, KDD 2017  
  *Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu*  
  [[Paper](https://arxiv.org/pdf/1609.05284)] ![Reasonet](https://img.shields.io/badge/Reasonet-blue) ![Marchine_Reading_Comprehension](https://img.shields.io/badge/Marchine_Reading_Comprehension-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{shen2017reasonet,
    title={Reasonet: Learning to stop reading in machine comprehension},
    author={Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
    booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
    pages={1047--1055},
    year={2017}
  }
  ```
  </details>  

- **[2] Learning to skim text**, ACL 2017  
  *Yu, Adams Wei and Lee, Hongrae and Le, Quoc V*  
  [[Paper](https://arxiv.org/pdf/1704.06877)] ![LSTM--Jump](https://img.shields.io/badge/LSTM--Jump-blue) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yu2017learning,
    title={Learning to Skim Text},
    author={Yu, Adams Wei and Lee, Hongrae and Le, Quoc},
    booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1880--1890},
    year={2017}
  }
  ```
  </details>  

- **[3] Learning when to skim and when to read**, RepL4NLP 2017  
  *Johansen, Alexander Rosenberg and Socher, Richard*  
  [[Paper](https://arxiv.org/pdf/1712.05483)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{johansen2017learning,
    title={Learning when to skim and when to read},
    author={Johansen, Alexander and Socher, Richard},
    booktitle={Proceedings of the 2nd Workshop on Representation Learning for NLP},
    pages={257--264},
    year={2017}
  }
  ```
  </details>  

- **[4] Training a subsampling mechanism in expectation**, ICLR 2017  
  *Raffel, Colin and Lawson, Dieterich*  
  [[Paper](https://arxiv.org/pdf/1702.06914)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{raffel2017training,
    title={Training a subsampling mechanism in expectation},
    author={Raffel, Colin and Lawson, Dieterich},
    journal={arXiv preprint arXiv:1702.06914},
    year={2017}
  }
  ```
  </details>  

- **[5] Variable computation in recurrent neural networks**, ICLR 2017  
  *Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas*  
  [[Paper](https://openreview.net/pdf?id=S1LVSrcge)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jernite2022variable,
    title={Variable Computation in Recurrent Neural Networks},
    author={Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    booktitle={International Conference on Learning Representations},
    year={2022}
  }
  ```
  </details>  

## 2018

- **[1] Neural speed reading via skim-rnn**, ICLR 2018  
  *Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh*  
  [[Paper](https://openreview.net/pdf?id=Sy-dQG-Rb)] ![Skim--RNN_](https://img.shields.io/badge/Skim--RNN_-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{seo2018neural,
    title={Neural Speed Reading via Skim-RNN},
    author={Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

- **[2] Skip rnn: Learning to skip state updates in recurrent neural networks**, ICLR 2018  
  *Campos, Víctor and Jou, Brendan and Giró-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu*  
  [[Paper](https://openreview.net/pdf?id=HkwVAXyCW)] [[Code](https://github.com/imatge-upc/skiprnn-2017-telecombcn)] ![Skip_RNN_](https://img.shields.io/badge/Skip_RNN_-blue) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{campos2018skip,
    title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},
    author={Campos, V{\'\i}ctor and Jou, Brendan and Gir{\'o}-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

- **[3] Accelerating neural transformer via an average attention network**, ACL 2018  
  *Zhang, Biao and Xiong, Deyi and Su, Jinsong*  
  [[Paper](https://aclanthology.org/P18-1166.pdf)] [[Code](https://github.com/bzhangGo/transformer-aan)] ![transformer--aan](https://img.shields.io/badge/transformer--aan-blue) ![Translation](https://img.shields.io/badge/Translation-green) ![Average_Attention](https://img.shields.io/badge/Average_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhang2018accelerating,
    title={Accelerating Neural Transformer via an Average Attention Network},
    author={Zhang, Biao and Xiong, Deyi and Su, Jinsong},
    booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1789--1798},
    year={2018}
  }
  ```
  </details>  

- **[4] Generating wikipedia by summarizing long sequences**, ICLR 2018  
  *Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam*  
  [[Paper](https://openreview.net/pdf?id=Hyg0vbWC-)] [[Code](https://github.com/lucidrains/memory-compressed-attention)] ![Summarization](https://img.shields.io/badge/Summarization-green) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{liu2018generating,
    title={Generating Wikipedia by Summarizing Long Sequences},
    author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

## 2019

- **[1] Transformer-XL: Attentive language models beyond a fixed-length context**, ACL 2019  
  *Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan*  
  [[Paper](https://aclanthology.org/P19-1285.pdf)] [[Code](https://github.com/kimiyoung/transformer-xl)] ![Transformer--XL](https://img.shields.io/badge/Transformer--XL-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Cache](https://img.shields.io/badge/Cache-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Recurrent_Mechanism](https://img.shields.io/badge/Recurrent_Mechanism-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{dai2019transformer,
    title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
    author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
    booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    pages={2978--2988},
    year={2019}
  }
  ```
  </details>  

- **[2] Neural speed reading with structural-jump-lstm**, ICLR 2019  
  *Christian Hansen and Casper Hansen and Stephen Alstrup and Jakob Grue Simonsen and Christina Lioma*  
  [[Paper](https://openreview.net/pdf?id=B1xf9jAqFQ)] [[Code](https://github.com/Varyn/Neural-Speed-Reading-with-Structural-Jump-LSTM)] ![Structural--Jump--LSTM](https://img.shields.io/badge/Structural--Jump--LSTM-blue) ![Agent](https://img.shields.io/badge/Agent-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  hansen2018neural,
  title={Neural Speed Reading with Structural-Jump-{LSTM}},
  author={Christian Hansen and Casper Hansen and Stephen Alstrup and Jakob Grue Simonsen and Christina Lioma},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=B1xf9jAqFQ},
  }
  ```
  </details>  

- **[3] How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings**, EMNLP-IJCNLP 2019  
  *Ethayarajh, Kawin*  
  [[Paper](https://aclanthology.org/D19-1006/)] ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{ethayarajh2019contextual,
    title={How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
    author={Ethayarajh, Kawin},
    booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages={55--65},
    year={2019}
  }
  ```
  </details>  

## 2020

- **[1] Compressive transformers for long-range sequence modelling**, ICLR 2020  
  *Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P*  
  [[Paper](https://openreview.net/pdf?id=SylKikSYDH)] [[Code](https://github.com/lucidrains/compressive-transformer-pytorch)] ![Compressive_Transformer](https://img.shields.io/badge/Compressive_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Cache](https://img.shields.io/badge/Cache-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Most--used](https://img.shields.io/badge/Most--used-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{raecompressive,
    title={Compressive Transformers for Long-Range Sequence Modelling},
    author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
    booktitle={International Conference on Learning Representations}
  }
  ```
  </details>  

- **[2] PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination**, ICML 2020  
  *Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish*  
  [[Paper](https://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf)] [[Code](https://github.com/IBM/PoWER-BERT)] ![PoWER--BERT](https://img.shields.io/badge/PoWER--BERT-blue) ![QA](https://img.shields.io/badge/QA-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Embedding](https://img.shields.io/badge/Embedding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Static](https://img.shields.io/badge/Static-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{goyal2020power,
    title={Power-bert: Accelerating bert inference via progressive word-vector elimination},
    author={Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
    booktitle={International Conference on Machine Learning},
    pages={3690--3699},
    year={2020},
    organization={PMLR}
  }
  ```
  </details>  

- **[3] Funnel-transformer: Filtering out sequential redundancy for efficient language processing**, NeurIPS 2020  
  *Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc*  
  [[Paper](https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf)] [[Code](https://github.com/laiguokun/Funnel-Transformer)] ![Funnel--transformer](https://img.shields.io/badge/Funnel--transformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{dai2020funnel,
    title={Funnel-transformer: Filtering out sequential redundancy for efficient language processing},
    author={Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={4271--4282},
    year={2020}
  }
  ```
  </details>  

- **[4] Multi-scale Transformer Language Models**, arXiv 2020  
  *Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y-Lan*  
  [[Paper](https://arxiv.org/pdf/2005.00581)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{subramanian2020multi,
    title={Multi-scale Transformer Language Models},
    author={Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y-Lan},
    journal={arXiv preprint arXiv:2005.00581},
    year={2020}
  }
  ```
  </details>  

- **[5] Reformer: The efficient transformer**, ICLR 2020  
  *Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya*  
  [[Paper](https://openreview.net/pdf?id=rkgNKkHtvB)] [[Code](https://github.com/google/trax/tree/master/trax/models/reformer)] ![Reformer](https://img.shields.io/badge/Reformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Translation](https://img.shields.io/badge/Translation-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Locality--Sensitive_Hashing](https://img.shields.io/badge/Locality--Sensitive_Hashing-orange) ![Cache](https://img.shields.io/badge/Cache-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  Kitaev2020Reformer:,
  title={Reformer: The Efficient Transformer},
  author=[@authors]{Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=rkgNKkHtvB}
  }
  ```
  </details>  

- **[6] Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words**, ACL 2020  
  *Klafka, Josef and Ettinger, Allyson*  
  [[Paper](https://aclanthology.org/2020.acl-main.434.pdf)] [[Code](https://github.com/jklafka/context-probes)] ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{klafka2020spying,
    title={Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words},
    author={Klafka, Josef and Ettinger, Allyson},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    pages={4801--4811},
    year={2020}
  }
  ```
  </details>  

- **[7] DeFormer: Decomposing pre-trained transformers for faster question answering**, ACL 2020  
  *Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan*  
  [[Paper](https://aclanthology.org/2020.acl-main.411.pdf)] [[Code](https://github.com/StonyBrookNLP/deformer)] ![DeFormer](https://img.shields.io/badge/DeFormer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sentence--Sentence](https://img.shields.io/badge/Sentence--Sentence-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder](https://img.shields.io/badge/Transformer_Encoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{cao2020deformer,
    title={DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering},
    author={Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    pages={4487--4497},
    year={2020}
  }
  ```
  </details>  

- **[8] Attention is not only a weight: Analyzing transformers with vector norms**, EMNLP 2020  
  *Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro*  
  [[Paper](https://aclanthology.org/2020.emnlp-main.574.pdf)] [[Code](https://github.com/gorokoba560/norm-analysis-of-transformer)] ![Norm--based_Analysis](https://img.shields.io/badge/Norm--based_Analysis-yellow) ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kobayashi2020attention,
    title={Attention is Not Only a Weight: Analyzing Transformers with Vector Norms},
    author={Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    pages={7057--7075},
    year={2020}
  }
  ```
  </details>  

## 2021

- **[1] TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference**, NAACL 2021  
  *Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong*  
  [[Paper](https://aclanthology.org/2021.naacl-main.463.pdf)] [[Code](https://github.com/thunlp/TR-BERT)] ![TR--BERT](https://img.shields.io/badge/TR--BERT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Random](https://img.shields.io/badge/Random-purple) ![Prediction--guided](https://img.shields.io/badge/Prediction--guided-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{ye2021tr,
    title={TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference},
    author={Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong},
    booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages={5798--5809},
    year={2021}
  }
  ```
  </details>  

- **[2] Poolingformer: Long document modeling with pooling attention**, ICML  2021  
  *Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv, Jiancheng and Duan, Nan and Chen, Weizhu*  
  [[Paper](http://proceedings.mlr.press/v139/zhang21h/zhang21h.pdf)] ![Poolingformer](https://img.shields.io/badge/Poolingformer-blue) ![Long_Sequence_Summarization](https://img.shields.io/badge/Long_Sequence_Summarization-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhang2021poolingformer,
    title={Poolingformer: Long document modeling with pooling attention},
    author={Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv, Jiancheng and Duan, Nan and Chen, Weizhu},
    booktitle={International Conference on Machine Learning},
    pages={12437--12446},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

- **[3] Length-adaptive transformer: Train once with length drop, use anytime with search**, ACL 2021  
  *Kim, Gyuwan and Cho, Kyunghyun*  
  [[Paper](https://aclanthology.org/2021.acl-long.508.pdf)] [[Code](https://github.com/clovaai/length-adaptive-transformer)] ![Length--Adaptive_Transformer](https://img.shields.io/badge/Length--Adaptive_Transformer-blue) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Token--leval_Classification](https://img.shields.io/badge/Token--leval_Classification-green) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Distillation](https://img.shields.io/badge/Distillation-yellow) ![LayerDrop](https://img.shields.io/badge/LayerDrop-yellow) ![LengthDrop](https://img.shields.io/badge/LengthDrop-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kim2021length,
    title={Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search},
    author={Kim, Gyuwan and Cho, Kyunghyun},
    booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    pages={6501--6511},
    year={2021}
  }
  ```
  </details>  

- **[4] Efficient content-based sparse attention with routing transformers**, TACL 2021  
  *Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David*  
  [[Paper](https://aclanthology.org/2021.tacl-1.4.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/routing_transformer)] ![Routing_Transformer](https://img.shields.io/badge/Routing_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Image_Generation](https://img.shields.io/badge/Image_Generation-green) ![Clustering](https://img.shields.io/badge/Clustering-orange) ![Routing_Attention](https://img.shields.io/badge/Routing_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{roy2021efficient,
    title={Efficient content-based sparse attention with routing transformers},
    author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
    journal={Transactions of the Association for Computational Linguistics},
    volume={9},
    pages={53--68},
    year={2021},
    publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
  }
  ```
  </details>  

- **[5] Not all memories are created equal: Learning to forget by expiring**, ICML 2021  
  *Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela*  
  [[Paper](http://proceedings.mlr.press/v139/sukhbaatar21a/sukhbaatar21a.pdf)] [[Code](https://github.com/facebookresearch/transformer-sequential)] ![Expire--Span](https://img.shields.io/badge/Expire--Span-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Expire](https://img.shields.io/badge/Expire-orange) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Expire](https://img.shields.io/badge/Expire-yellow) ![Memory](https://img.shields.io/badge/Memory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{sukhbaatar2021not,
    title={Not all memories are created equal: Learning to forget by expiring},
    author={Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
    booktitle={International Conference on Machine Learning},
    pages={9902--9912},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

- **[6] On the transformer growth for progressive bert training**, NAACL 2021  
  *Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei*  
  [[Paper](https://aclanthology.org/2021.naacl-main.406.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/grow_bert)] ![CompoundGrow](https://img.shields.io/badge/CompoundGrow-blue) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![BERT_Pretraining](https://img.shields.io/badge/BERT_Pretraining-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{gu2021transformer,
    title={On the Transformer Growth for Progressive BERT Training},
    author={Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei},
    booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages={5174--5180},
    year={2021}
  }
  ```
  </details>  

- **[7] Memory-efficient Transformers via Top-$ k $ Attention**, SUSTAINLP 2021  
  *Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2021.sustainlp-1.5.pdf)] [[Code](https://github.com/ag1988/top_k_attention)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Query_chunking](https://img.shields.io/badge/Query_chunking-orange) ![Top--k_attention](https://img.shields.io/badge/Top--k_attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{gupta2021memory,
    title={Memory-efficient Transformers via Top-k Attention},
    author={Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan},
    booktitle={Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
    pages={39--52},
    year={2021}
  }
  ```
  </details>  

- **[8] Magic pyramid: Accelerating inference with early exiting and token pruning**, NeurIPS-ENLSP 2021  
  *He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul*  
  [[Paper](https://neurips2021-nlp.github.io/papers/21/CameraReady/magic_pyramid.pdf)] ![Magic_Pyramid](https://img.shields.io/badge/Magic_Pyramid-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity_Detection](https://img.shields.io/badge/Similarity_Detection-green) ![Topic_Identification_](https://img.shields.io/badge/Topic_Identification_-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Early_Existing](https://img.shields.io/badge/Early_Existing-yellow) ![Token_Pruning](https://img.shields.io/badge/Token_Pruning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{he2021magic,
    title={Magic pyramid: Accelerating inference with early exiting and token pruning},
    author={He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul},
    journal={arXiv preprint arXiv:2111.00230},
    year={2021}
  }
  ```
  </details>  

- **[9] El-attention: Memory efficient lossless attention for generation**, ICML 2021  
  *Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and Gong, Yeyun and Duan, Nan and Zhang, Ruofei*  
  [[Paper](https://proceedings.mlr.press/v139/yan21a/yan21a.pdf)] [[Code](https://github.com/microsoft/fastseq)] ![EL--Attention](https://img.shields.io/badge/EL--Attention-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Encoder--Decoder_Attention](https://img.shields.io/badge/Encoder--Decoder_Attention-yellow) ![Self--Attention](https://img.shields.io/badge/Self--Attention-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yan2021attention,
    title={El-attention: Memory efficient lossless attention for generation},
    author={Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and Gong, Yeyun and Duan, Nan and Zhang, Ruofei},
    booktitle={International Conference on Machine Learning},
    pages={11648--11658},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

## 2022

- **[1] Learned Token Pruning for Transformers**, KDD 2022  
  *Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt*  
  [[Paper](https://dl.acm.org/doi/pdf/10.1145/3534678.3539260)] [[Code](https://github.com/kssteven418/LTP)] ![LTP](https://img.shields.io/badge/LTP-blue) ![Sentence_Similarity](https://img.shields.io/badge/Sentence_Similarity-green) ![Classification](https://img.shields.io/badge/Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Threshold](https://img.shields.io/badge/Learnable_Threshold-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kim2022learned,
    title={Learned token pruning for transformers},
    author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
    booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages={784--794},
    year={2022}
  }
  ```
  </details>  

- **[2] Block-skim: Efficient question answering for transformer**, AAAI  2022  
  *Guan, Yue and Li, Zhengyi and Lin, Zhouhan and Zhu, Yuhao and Leng, Jingwen and Guo, Minyi*  
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/21316/21065)] [[Code](https://github.com/ChandlerGuan/blockskim)] ![Block--Skim](https://img.shields.io/badge/Block--Skim-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Conv](https://img.shields.io/badge/Conv-orange) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guan2022block,
    title={Block-skim: Efficient question answering for transformer},
    author={Guan, Yue and Li, Zhengyi and Lin, Zhouhan and Zhu, Yuhao and Leng, Jingwen and Guo, Minyi},
    booktitle={Proceedings of the AAAI conference on artificial intelligence},
    volume={36},
    number={10},
    pages={10710--10719},
    year={2022}
  }
  ```
  </details>  

- **[3] Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models**, ACL 2022  
  *Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor*  
  [[Paper](https://aclanthology.org/2022.findings-emnlp.412.pdf)] [[Code](https://github.com/BYU-PCCL/prompt-compression-contrastive-coding)] ![Toxicity_reduction](https://img.shields.io/badge/Toxicity_reduction-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Prompt_Compression](https://img.shields.io/badge/Prompt_Compression-yellow) ![Exploration](https://img.shields.io/badge/Exploration-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{wingate2022prompt,
    title={Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models},
    author={Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
    pages={5621--5634},
    year={2022}
  }
  ```
  </details>  

- **[4] Memorizing transformers**, ICLR 2022  
  *Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy*  
  [[Paper](https://openreview.net/pdf?id=TrjbxzRcnf-)] [[Code](https://github.com/lucidrains/memorizing-transformers-pytorch)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Caching](https://img.shields.io/badge/Caching-orange) ![kNN_Attention](https://img.shields.io/badge/kNN_Attention-orange) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  wu2022memorizing,
  title={Memorizing Transformers},
  author={Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=TrjbxzRcnf-}
  }
  ```
  </details>  

- **[5] Transkimmer: Transformer learns to layer-wise skim**, ACL 2022  
  *Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi*  
  [[Paper](https://aclanthology.org/2022.acl-long.502.pdf)] [[Code](https://github.com/chandlerguan/transkimmer)] ![Transkimmer](https://img.shields.io/badge/Transkimmer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guan2022transkimmer,
    title={Transkimmer: Transformer Learns to Layer-wise Skim},
    author={Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={7275--7286},
    year={2022}
  }
  ```
  </details>  

- **[6] Adapler: Speeding up inference by adaptive length reduction**, ACL 2022  
  *Modarressi, Ali and Mohebbi, Hosein and Pilehvar, Mohammad Taher*  
  [[Paper](https://aclanthology.org/2022.acl-long.1.pdf)] [[Code](https://github.com/amodaresi/AdapLeR)] ![AdapLeR](https://img.shields.io/badge/AdapLeR-blue) ![Topic_Classification](https://img.shields.io/badge/Topic_Classification-green) ![Sentiment_Classification](https://img.shields.io/badge/Sentiment_Classification-green) ![Knowledge_Extraction](https://img.shields.io/badge/Knowledge_Extraction-green) ![QA](https://img.shields.io/badge/QA-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{modarressi2022adapler,
    title={AdapLeR: Speeding up Inference by Adaptive Length Reduction},
    author={Modarressi, Ali and Mohebbi, Hosein and Pilehvar, Mohammad Taher},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1--15},
    year={2022}
  }
  ```
  </details>  

- **[7] Fine-and coarse-granularity hybrid self-attention for efficient bert**, ACL 2022  
  *Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong*  
  [[Paper](https://aclanthology.org/2022.acl-long.330.pdf)] [[Code](https://github.com/pierre-zhao/FCA-BERT)] ![FCA--BERT](https://img.shields.io/badge/FCA--BERT-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity](https://img.shields.io/badge/Similarity-green) ![NLI](https://img.shields.io/badge/NLI-green) ![Sentiment](https://img.shields.io/badge/Sentiment-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhao2022fine,
    title={Fine-and Coarse-Granularity Hybrid Self-Attention for Efficient BERT},
    author={Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={4811--4820},
    year={2022}
  }
  ```
  </details>  

## 2023

- **[1] Efficient long-text understanding with short-text models**, TACL 2023  
  *Ivgi, Maor and Shaham, Uri and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2023.tacl-1.17.pdf)] [[Code](https://github.com/Mivg/SLED)] ![SLED](https://img.shields.io/badge/SLED-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{ivgi2023efficient,
    title={Efficient long-text understanding with short-text models},
    author={Ivgi, Maor and Shaham, Uri and Berant, Jonathan},
    journal={Transactions of the Association for Computational Linguistics},
    volume={11},
    pages={284--299},
    year={2023},
    publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
  }
  ```
  </details>  

- **[2] Extensible prompts for language models on Zero-shot Language Style Customization**, NeurIPS 2023  
  *Tao Ge and Jing Hu and Li Dong and Shaoguang Mao and Yan Xia and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=lRxpVfDMzz)] ![Language_Style_Customization](https://img.shields.io/badge/Language_Style_Customization-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2023extensible,
  title={Extensible Prompts for Language Models on Zero-shot Language Style Customization},
  author={Tao Ge and Jing Hu and Li Dong and Shaoguang Mao and Yan Xia and Xun Wang and Si-Qing Chen and Furu Wei},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=lRxpVfDMzz}
  }
  ```
  </details>  

- **[3] Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification**, EMNLP 2023  
  *Yun, Jungmin and Kim, Mihyeon and Kim, Youngbin*  
  [[Paper](https://aclanthology.org/2023.findings-emnlp.909.pdf)] ![Document_Classification](https://img.shields.io/badge/Document_Classification-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Fuzzy--based_Token_Pruning](https://img.shields.io/badge/Fuzzy--based_Token_Pruning-orange) ![Token_Combining](https://img.shields.io/badge/Token_Combining-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yun2023focus,
    title={Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification},
    author={Yun, Jungmin and Kim, Mihyeon and Kim, Youngbin},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
    pages={13617--13628},
    year={2023}
  }
  ```
  </details>  

- **[4] Compressing context to enhance inference efficiency of large language models**, EMNLP 2023  
  *Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.391.pdf)] [[Code](https://github.com/liyucheng09/Selective_Context)] ![Selective_Context](https://img.shields.io/badge/Selective_Context-blue) ![QA](https://img.shields.io/badge/QA-green) ![Summarization](https://img.shields.io/badge/Summarization-green) ![Conversation](https://img.shields.io/badge/Conversation-green) ![Self--Information](https://img.shields.io/badge/Self--Information-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Generation](https://img.shields.io/badge/LLM_Generation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{li2023compressing,
    title={Compressing Context to Enhance Inference Efficiency of Large Language Models},
    author={Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={6342--6353},
    year={2023}
  }
  ```
  </details>  

- **[5] Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning**, ACL 2023  
  *Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng*  
  [[Paper](https://aclanthology.org/2023.acl-long.172.pdf)] [[Code](https://github.com/fanyin3639/Rethinking-instruction-effectiveness)] ![STDC](https://img.shields.io/badge/STDC-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Syntax--guided](https://img.shields.io/badge/Syntax--guided-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yin2023did,
    title={Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning},
    author={Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng},
    booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={3063--3079},
    year={2023}
  }
  ```
  </details>  

- **[6] Llmlingua: Compressing prompts for accelerated inference of large language models**, EMNLP 2023  
  *Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.825.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LLMLingua_](https://img.shields.io/badge/LLMLingua_-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jiang2023llmlingua,
    title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models},
    author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={13358--13376},
    year={2023}
  }
  ```
  </details>  

- **[7] Learning to compress prompts with gist tokens**, NeurIPS 2023  
  *Mu, Jesse and Li, Xiang and Goodman, Noah*  
  [[Paper](https://openreview.net/pdf?id=2DtxPCL3T5)] [[Code](https://github.com/jayelm/gisting)] ![gisting](https://img.shields.io/badge/gisting-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  mu2023learning,
  title={Learning to Compress Prompts with Gist Tokens},
  author={Jesse Mu and Xiang Lisa Li and Noah Goodman},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=2DtxPCL3T5}
  }
  ```
  </details>  

- **[8] Adapting language models to compress contexts**, EMNLP 2023  
  *Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.232.pdf)] [[Code](https://github.com/princeton-nlp/AutoCompressors)] ![AutoCompressors](https://img.shields.io/badge/AutoCompressors-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{chevalier2023adapting,
    title={Adapting Language Models to Compress Contexts},
    author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={3829--3846},
    year={2023}
  }
  ```
  </details>  

- **[9] Inference with reference: Lossless acceleration of large language models**, arXiv 2023  
  *Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu*  
  [[Paper](https://arxiv.org/pdf/2304.04487)] ![LLMA](https://img.shields.io/badge/LLMA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Copy](https://img.shields.io/badge/Copy-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @misc{yang2023inference,
      title={Inference with Reference: Lossless Acceleration of Large Language Models},
      author={Nan Yang and Tao Ge and Liang Wang and Binxing Jiao and Daxin Jiang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2023},
      eprint={2304.04487},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }
  ```
  </details>  

- **[10] Efficient prompting via dynamic in-context learning**, arXiv 2023  
  *Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya*  
  [[Paper](https://arxiv.org/pdf/2305.11170)] ![DynaICL](https://img.shields.io/badge/DynaICL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhou2023efficient,
    title={Efficient prompting via dynamic in-context learning},
    author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya},
    journal={arXiv preprint arXiv:2305.11170},
    year={2023}
  }
  ```
  </details>  

- **[11] Nugget: Neural agglomerative embeddings of text**, ICML  2023  
  *Qin, Guanghui and Van Durme, Benjamin*  
  [[Paper](https://arxiv.org/pdf/2310.01732)] [[Code](https://github.com/hiaoxui/nugget)] ![Nugget](https://img.shields.io/badge/Nugget-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Nugget_Generator](https://img.shields.io/badge/Nugget_Generator-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{qin2023nugget,
    title={NUGGET: neural agglomerative embeddings of text},
    author={Qin, Guanghui and Van Durme, Benjamin},
    booktitle={Proceedings of the 40th International Conference on Machine Learning},
    pages={28337--28350},
    year={2023}
  }
  ```
  </details>  

- **[12] Unlimiformer: Long-range transformers with unlimited length input**, NeurIPS 2023  
  *Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley*  
  [[Paper](https://openreview.net/pdf?id=lJWUJWLCJo)] [[Code](https://github.com/abertsch72/unlimiformer)] ![Unlimiformer](https://img.shields.io/badge/Unlimiformer-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Top--k_retrieval](https://img.shields.io/badge/Top--k_retrieval-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  bertsch2023unlimiformer,
  title={Unlimiformer: Long-Range Transformers with Unlimited Length Input},
  author={Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=lJWUJWLCJo}
  }
  ```
  </details>  

- **[13] Walking down the memory maze: Beyond context limit through interactive reading**, arXiv 2023  
  *Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli*  
  [[Paper](https://arxiv.org/pdf/2310.05029)] ![MemWalker](https://img.shields.io/badge/MemWalker-blue) ![Similarity](https://img.shields.io/badge/Similarity-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Divide_and_Donquer](https://img.shields.io/badge/Divide_and_Donquer-orange) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{chen2023walking,
    title={Walking down the memory maze: Beyond context limit through interactive reading},
    author={Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli},
    journal={arXiv preprint arXiv:2310.05029},
    year={2023}
  }
  ```
  </details>  

- **[14] Sparse token transformer with attention back tracking**, ICLR 2023  
  *Heejun Lee and Minki Kang and Youngwan Lee and Sung Ju Hwang*  
  [[Paper](https://openreview.net/pdf?id=VV0hSE8AxCw)] [[Code](https://github.com/gmlwns2000/sttabt)] ![STTABT](https://img.shields.io/badge/STTABT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Attention_Approximation_Network](https://img.shields.io/badge/Attention_Approximation_Network-purple) ![Attention_Back--tracking](https://img.shields.io/badge/Attention_Back--tracking-orange) ![Concrete_Masking](https://img.shields.io/badge/Concrete_Masking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  lee2023sparse,
  title={Sparse Token Transformer with Attention Back Tracking},
  author={Heejun Lee and Minki Kang and Youngwan Lee and Sung Ju Hwang},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=VV0hSE8AxCw}
  }
  ```
  </details>  

- **[15] Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time**, NeurIPS 2023  
  *Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava*  
  [[Paper](https://openreview.net/pdf?id=JZfg6wGi6g)] ![Scissorhands](https://img.shields.io/badge/Scissorhands-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  liu2023scissorhands,
  title={Scissorhands: Exploiting the Persistence of Importance Hypothesis for {LLM} {KV} Cache Compression at Test Time},
  author={Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=JZfg6wGi6g}
  }
  ```
  </details>  

- **[16] H2o: Heavy-hitter oracle for efficient generative inference of large language models**, NeurIPS 2023  
  *Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen*  
  [[Paper](https://openreview.net/pdf?id=RkRrPp7GKO)] [[Code](https://github.com/FMInference/H2O)] ![H2O](https://img.shields.io/badge/H2O-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  zhang2023ho,
  title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=RkRrPp7GKO}
  }
  ```
  </details>  

- **[17] Landmark attention: Random-access infinite context length for transformers**, NeurIPS 2023  
  *Amirkeivan Mohtashami and Martin Jaggi*  
  [[Paper](https://openreview.net/pdf?id=7eHn64wOVy)] [[Code](https://github.com/epfml/landmark-attention/)] ![Landmark_attention](https://img.shields.io/badge/Landmark_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  mohtashami2023randomaccess,
  title={Random-Access Infinite Context Length for Transformers},
  author={Amirkeivan Mohtashami and Martin Jaggi},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=7eHn64wOVy}
  }
  ```
  </details>  

- **[18] Optimizing retrieval-augmented reader models via token elimination**, EMNLP 2023  
  *Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.93.pdf)] [[Code](https://github.com/IntelLabs/token_elimination)] ![Token_Elimination](https://img.shields.io/badge/Token_Elimination-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{berchansky2023optimizing,
    title={Optimizing Retrieval-augmented Reader Models via Token Elimination},
    author={Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={1506--1524},
    year={2023}
  }
  ```
  </details>  

- **[19] Efficient transformers with dynamic token pooling**, ACL 2023  
  *Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria*  
  [[Paper](https://aclanthology.org/2023.acl-long.353.pdf)] [[Code](https://github.com/PiotrNawrot/dynamic-pooling)] ![Dynamic_Token_Pooling](https://img.shields.io/badge/Dynamic_Token_Pooling-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{nawrot2023efficient,
    title={Efficient Transformers with Dynamic Token Pooling},
    author={Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria},
    booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={6403--6417},
    year={2023}
  }
  ```
  </details>  

- **[20] Dynamic context pruning for efficient and interpretable autoregressive transformers**, NeurIPS 2023  
  *Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann*  
  [[Paper](https://openreview.net/pdf?id=uvdJgFFzby)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  anagnostidis2023dynamic,
  title={Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
  author={Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=uvdJgFFzby}
  }
  ```
  </details>  

## 2024

- **[1] Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression**, ACL 2024  
  *Jiang, Huiqiang  and Wu, Qianhui  and Luo, Xufang  and Li, Dongsheng  and Lin, Chin-Yew  and Yang, Yuqing  and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2024.acl-long.91.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LongLLMLingua](https://img.shields.io/badge/LongLLMLingua-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jiang-etal-2024-longllmlingua,
      title = "{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in Long Context Scenarios via Prompt Compression",
      author = "Jiang, Huiqiang  and
        Wu, Qianhui  and
        Luo, Xufang  and
        Li, Dongsheng  and
        Lin, Chin-Yew  and
        Yang, Yuqing  and
        Qiu, Lili",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.91",
      doi = "10.18653/v1/2024.acl-long.91",
      pages = "1658--1677"
  }
  ```
  </details>  

- **[2] In-context autoencoder for context compression in a large language model**, ICLR 2024  
  *Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=uREj4ZuGJE)] [[Code](https://github.com/getao/icae)] ![ICAE](https://img.shields.io/badge/ICAE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2024incontext,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=uREj4ZuGJE}
  }
  ```
  </details>  

- **[3] Efficient streaming language models with attention sinks**, ICLR 2024  
  *Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis*  
  [[Paper](https://openreview.net/pdf?id=NG7sS51zVF)] [[Code](https://github.com/mit-han-lab/streaming-llm)] ![StreamingLLM](https://img.shields.io/badge/StreamingLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  xiao2024efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=NG7sS51zVF}
  }
  ```
  </details>  

- **[4] Model tells you what to discard: Adaptive kv cache compression for llms**, ICLR 2024  
  *Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao*  
  [[Paper](https://openreview.net/pdf?id=uNrFpDPMyo)] [[Code](https://github.com/machilusZ/FastGen)] ![FastGen](https://img.shields.io/badge/FastGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Policy](https://img.shields.io/badge/Policy-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2024model,
  title={Model Tells You What to Discard: Adaptive {KV} Cache Compression for {LLM}s},
  author={Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=uNrFpDPMyo}
  }
  ```
  </details>  

- **[5] Sparq attention: Bandwidth-efficient llm inference**, ICLR-ME-FoMo 2024  
  *Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr*  
  [[Paper](https://openreview.net/pdf?id=Ue8EHzaFI4)] [[Code](https://github.com/graphcore-research/llm-inference-research/tree/2024-01-paper)] ![SparQ_attention](https://img.shields.io/badge/SparQ_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Sparsity](https://img.shields.io/badge/Sparsity-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ribar2024sparq,
  title={SparQ Attention: Bandwidth-Efficient {LLM} Inference},
  author={Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2024},
  url={https://openreview.net/forum?id=Ue8EHzaFI4}
  }
  ```
  </details>  

- **[6] FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing**, arXiv 2024  
  *Li, Zekai and Zheng, Jintu and Liu, Ji and Liu, Han and Zhu, Haowei and Li, Zeping and Yang, Fuwei and Huang, Haiduo and Peng, Jinzhang and Li, Dong and others*  
  [[Paper](https://arxiv.org/pdf/2412.11494)] ![FTP](https://img.shields.io/badge/FTP-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024ftp,
    title={FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing},
    author={Li, Zekai and Zheng, Jintu and Liu, Ji and Liu, Han and Zhu, Haowei and Li, Zeping and Yang, Fuwei and Huang, Haiduo and Peng, Jinzhang and Li, Dong and others},
    journal={arXiv preprint arXiv:2412.11494},
    year={2024}
  }
  ```
  </details>  

- **[7] Discrete prompt compression with reinforcement learning**, IEEE Access 2024  
  *Jung, Hoyoun and Kim, Kyung-Joong*  
  [[Paper](https://arxiv.org/pdf/2308.08758)] [[Code](https://github.com/nenomigami/PromptCompressor)] ![PCRL](https://img.shields.io/badge/PCRL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{jung2024discrete,
    title={Discrete prompt compression with reinforcement learning},
    author={Jung, Hoyoun and Kim, Kyung-Joong},
    journal={IEEE Access},
    year={2024},
    publisher={IEEE}
  }
  ```
  </details>  

- **[8] Hierarchical context merging: Better long context understanding for pre-trained LLMs**, ICLR 2024  
  *Woomin Song and Seunghyuk Oh and Sangwoo Mo and Jaehyung Kim and Sukmin Yun and Jung-Woo Ha and Jinwoo Shin*  
  [[Paper](https://openreview.net/pdf?id=ulaUJFd96G)] [[Code](https://github.com/alinlab/HOMER)] ![HOMER](https://img.shields.io/badge/HOMER-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![QA](https://img.shields.io/badge/QA-green) ![Passkey_Retrieval](https://img.shields.io/badge/Passkey_Retrieval-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  song2024hierarchical,
  title={Hierarchical Context Merging: Better Long Context Understanding for Pre-trained {LLM}s},
  author={Woomin Song and Seunghyuk Oh and Sangwoo Mo and Jaehyung Kim and Sukmin Yun and Jung-Woo Ha and Jinwoo Shin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=ulaUJFd96G}
  }
  ```
  </details>  

- **[9] LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference**, arXiv 2024  
  *Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar*  
  [[Paper](https://arxiv.org/pdf/2407.14057)] [[Code](https://github.com/Adam-Mazur/Lazy-Llama)] ![LazyLLM](https://img.shields.io/badge/LazyLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Restore](https://img.shields.io/badge/Restore-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{fu2024lazyllm,
    title={Lazyllm: Dynamic token pruning for efficient long context llm inference},
    author={Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
    journal={arXiv preprint arXiv:2407.14057},
    year={2024}
  }
  ```
  </details>  

- **[10] You only cache once: Decoder-decoder architectures for language models**, NeurIPS 2024  
  *Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=25Ioxw576r)] [[Code](https://github.com/microsoft/unilm/tree/master/YOCO)] ![YOCO](https://img.shields.io/badge/YOCO-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Early_Exit](https://img.shields.io/badge/Early_Exit-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Decoder--Decoder](https://img.shields.io/badge/Decoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  sun2024you,
  title={You Only Cache Once: Decoder-Decoder Architectures for Language Models},
  author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=25Ioxw576r}
  }
  ```
  </details>  

- **[11] Reducing Transformer Key-Value Cache Size with Cross-Layer Attention**, NeurIPS 2024  
  *William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan-Kelley*  
  [[Paper](https://openreview.net/pdf?id=M2UzLRoqic)] [[Code](https://github.com/JerryYin777/Cross-Layer-Attention)] ![CLA](https://img.shields.io/badge/CLA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  brandon2024reducing,
  title={Reducing Transformer Key-Value Cache Size with Cross-Layer Attention},
  author={William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan-Kelley},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=M2UzLRoqic}
  }
  ```
  </details>  

- **[12] Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression**, arXiv 2024  
  *Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene*  
  [[Paper](https://arxiv.org/pdf/2407.12077)] [[Code](https://github.com/SmerkyG/GoldFinch-paper?tab=readme-ov-file)] ![GoldFinch](https://img.shields.io/badge/GoldFinch-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Early_Exit](https://img.shields.io/badge/Early_Exit-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{goldstein2024goldfinch,
    title={Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-cache compression},
    author={Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene},
    journal={arXiv preprint arXiv:2407.12077},
    year={2024}
  }
  ```
  </details>  

- **[13] Long-context language modeling with parallel context encoding**, ACL 2024  
  *Yen, Howard  and Gao, Tianyu  and Chen, Danqi*  
  [[Paper](https://aclanthology.org/2024.acl-long.142.pdf)] [[Code](https://github.com/princeton-nlp/CEPE)] ![CEPE](https://img.shields.io/badge/CEPE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Encoder--Decoder](https://img.shields.io/badge/Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yen-etal-2024-long,
      title = "Long-Context Language Modeling with Parallel Context Encoding",
      author = "Yen, Howard  and
        Gao, Tianyu  and
        Chen, Danqi",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.142/",
      doi = "10.18653/v1/2024.acl-long.142",
      pages = "2588--2610"
  }
  ```
  </details>  

- **[14] Transformers are multi-state rnns**, EMNLP 2024  
  *Oren, Matanel  and Hassid, Michael  and Yarden, Nir  and Adi, Yossi  and Schwartz, Roy*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1043.pdf)] [[Code](https://github.com/schwartz-lab-NLP/TOVA)] ![TOVA](https://img.shields.io/badge/TOVA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Multi--State_RNN](https://img.shields.io/badge/Multi--State_RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{oren-etal-2024-transformers,
      title = "Transformers are Multi-State {RNN}s",
      author = "Oren, Matanel  and
        Hassid, Michael  and
        Yarden, Nir  and
        Adi, Yossi  and
        Schwartz, Roy",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1043/",
      doi = "10.18653/v1/2024.emnlp-main.1043",
      pages = "18724--18741"
  }
  ```
  </details>  

- **[15] PyramidInfer: Pyramid KV Cache Compression for High-throughput LLM Inference**, ACL 2024  
  *Yang, Dongjie  and Han, Xiaodong  and Gao, Yan  and Hu, Yao  and Zhang, Shilin  and Zhao, Hai*  
  [[Paper](https://aclanthology.org/2024.findings-acl.195.pdf)] [[Code](https://github.com/mutonix/pyramidinfer)] ![PyramidInfer](https://img.shields.io/badge/PyramidInfer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yang-etal-2024-pyramidinfer,
      title = "{P}yramid{I}nfer: Pyramid {KV} Cache Compression for High-throughput {LLM} Inference",
      author = "Yang, Dongjie  and
        Han, Xiaodong  and
        Gao, Yan  and
        Hu, Yao  and
        Zhang, Shilin  and
        Zhao, Hai",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-acl.195/",
      doi = "10.18653/v1/2024.findings-acl.195",
      pages = "3258--3270"
  }
  ```
  </details>  

- **[16] Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling**, arXiv 2024  
  *Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others*  
  [[Paper](https://arxiv.org/pdf/2406.02069)] [[Code](https://github.com/Zefan-Cai/KVCache-Factory)] ![PyramidKV](https://img.shields.io/badge/PyramidKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{cai2024pyramidkv,
    title={Pyramidkv: Dynamic kv cache compression based on pyramidal information funneling},
    author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
    journal={arXiv preprint arXiv:2406.02069},
    year={2024}
  }
  ```
  </details>  

- **[17] Keyformer: Kv cache reduction through key tokens selection for efficient generative inference**, MLSys 2024  
  *Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham*  
  [[Paper](https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf)] [[Code](https://github.com/d-matrix-ai/keyformer-llm)] ![Keyformer](https://img.shields.io/badge/Keyformer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{MLSYS2024_48fecef4,
   author = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
   booktitle = {Proceedings of Machine Learning and Systems},
   editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
   pages = {114--127},
   title = {Keyformer: KV Cache reduction through key tokens selection for Efficient Generative Inference},
   url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
   volume = {6},
   year = {2024}
  }
  ```
  </details>  

- **[18] Finch: Prompt-guided key-value cache compression**, TACL 2024  
  *Corallo, Giulio and Papotti, Paolo*  
  [[Paper](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00716/2480391/tacl_a_00716.pdf)] [[Code](https://github.com/giulio98/context-compression/)] ![FINCH](https://img.shields.io/badge/FINCH-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{10.1162/tacl_a_00716,
      author = {Corallo, Giulio and Papotti, Paolo},
      title = {FINCH: Prompt-guided Key-Value Cache Compression for Large Language Models},
      journal = {Transactions of the Association for Computational Linguistics},
      volume = {12},
      pages = {1517-1532},
      year = {2024},
      month = {11},
      issn = {2307-387X},
      doi = {10.1162/tacl_a_00716},
      url = {https://doi.org/10.1162/tacl\_a\_00716},
      eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00716/2480391/tacl\_a\_00716.pdf},
  }
  
  
  ```
  </details>  

- **[19] A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder**, arXiv 2024  
  *Jo, Hyun-rae and Shin, Dongkun*  
  [[Paper](https://arxiv.org/pdf/2407.20485#page=1.45)] [[Code](https://github.com/Dirac-Notation/A2SF)] ![A2sf](https://img.shields.io/badge/A2sf-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{jo2024a2sf,
    title={A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder},
    author={Jo, Hyun-rae and Shin, Dongkun},
    journal={arXiv preprint arXiv:2407.20485},
    year={2024}
  }
  ```
  </details>  

- **[20] Razorattention: Efficient kv cache compression through retrieval heads**, arXiv 2024  
  *Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi*  
  [[Paper](https://arxiv.org/pdf/2407.15891)] ![RazorAttention](https://img.shields.io/badge/RazorAttention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Compensation_Token](https://img.shields.io/badge/Compensation_Token-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{tang2024razorattention,
    title={Razorattention: Efficient kv cache compression through retrieval heads},
    author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
    journal={arXiv preprint arXiv:2407.15891},
    year={2024}
  }
  ```
  </details>  

- **[21] SirLLM: Streaming infinite retentive LLM**, ACL 2024  
  *Yao, Yao  and Li, Zuchao  and  Zhao, Hai*  
  [[Paper](https://aclanthology.org/2024.acl-long.143.pdf)] [[Code](https://github.com/Zoeyyao27/SirLLM)] ![SirLLM](https://img.shields.io/badge/SirLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Token_Entropy](https://img.shields.io/badge/Token_Entropy-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Token_Entropy](https://img.shields.io/badge/Token_Entropy-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yao-etal-2024-sirllm,
      title = "{S}ir{LLM}: Streaming Infinite Retentive {LLM}",
      author = "Yao, Yao  and
        Li, Zuchao  and
        Zhao, Hai",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.143/",
      doi = "10.18653/v1/2024.acl-long.143",
      pages = "2611--2624"
  }
  ```
  </details>  

- **[22] A Simple and Effective $ L_2 $ Norm-Based Strategy for KV Cache Compression**, EMNLP 2024  
  *Devoto, Alessio  and  Zhao, Yu  and   Scardapane, Simone  and Minervini, Pasquale*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1027.pdf)] [[Code](https://aclanthology.org/attachments/2024.emnlp-main.1027.software.zip)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![L2_Norm](https://img.shields.io/badge/L2_Norm-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![L2--norm--based](https://img.shields.io/badge/L2--norm--based-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{devoto-etal-2024-simple,
      title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Cache Compression",
      author = "Devoto, Alessio  and
        Zhao, Yu  and
        Scardapane, Simone  and
        Minervini, Pasquale",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1027/",
      doi = "10.18653/v1/2024.emnlp-main.1027",
      pages = "18476--18499"
  }
  ```
  </details>  

- **[23] Dynamic memory compression: Retrofitting llms for accelerated inference**, arXiv 2024  
  *Nawrot, Piotr and La'ncucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M*  
  [[Paper](https://arxiv.org/pdf/2403.09636)] ![DMC](https://img.shields.io/badge/DMC-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{nawrot2024dynamic,
    title={Dynamic memory compression: Retrofitting llms for accelerated inference},
    author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
    journal={arXiv preprint arXiv:2403.09636},
    year={2024}
  }
  ```
  </details>  

- **[24] Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks**, arXiv 2024  
  *Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia*  
  [[Paper](https://arxiv.org/pdf/2407.08454#page=1.71)] ![KVMerger](https://img.shields.io/badge/KVMerger-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Similarity_Map](https://img.shields.io/badge/Similarity_Map-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Gaussian_Kernel_weighted_merging](https://img.shields.io/badge/Gaussian_Kernel_weighted_merging-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wang2024model,
    title={Model tells you where to merge: Adaptive kv cache merging for llms on long-context tasks},
    author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
    journal={arXiv preprint arXiv:2407.08454},
    year={2024}
  }
  ```
  </details>  

- **[25] Anchor-based large language models**, ACL 2024  
  *Pang, Jianhui  and Ye, Fanghua  and Wong, Derek  and  He, Xin  and  Chen, Wanshun  and  Wang, Longyue*  
  [[Paper](https://aclanthology.org/2024.findings-acl.295.pdf)] [[Code](https://github.com/pangjh3/AnLLM)] ![AnLLM](https://img.shields.io/badge/AnLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Anchor_Token](https://img.shields.io/badge/Anchor_Token-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{pang-etal-2024-anchor,
      title = "Anchor-based Large Language Models",
      author = "Pang, Jianhui  and
        Ye, Fanghua  and
        Wong, Derek  and
        He, Xin  and
        Chen, Wanshun  and
        Wang, Longyue",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-acl.295/",
      doi = "10.18653/v1/2024.findings-acl.295",
      pages = "4958--4976"
  }
  ```
  </details>  

- **[26] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Cache Consumption**, COLM 2024  
  *Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao*  
  [[Paper](https://openreview.net/pdf?id=8tKjqqMM5z)] [[Code](https://github.com/zcli-charlie/Awesome-KVCache)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Survey](https://img.shields.io/badge/Survey-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  luohe2024keep,
  title={Keep the Cost Down: A Review on Methods to Optimize {LLM}{\textquoteright}s {KV}-Cache Consumption},
  author={Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao},
  booktitle={First Conference on Language Modeling},
  year={2024},
  url={https://openreview.net/forum?id=8tKjqqMM5z}
  }
  ```
  </details>  

- **[27] SCBench: A KV Cache-Centric Analysis of Long-Context Methods**, arXiv 2024  
  *Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others*  
  [[Paper](https://arxiv.org/pdf/2412.10319)] [[Code](https://aka.ms/SCBench)] ![SCBench](https://img.shields.io/badge/SCBench-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Benchmark](https://img.shields.io/badge/Benchmark-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024scbench,
    title={Scbench: A kv cache-centric analysis of long-context methods},
    author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
    journal={arXiv preprint arXiv:2412.10319},
    year={2024}
  }
  ```
  </details>  

- **[28] On the efficacy of eviction policy for key-value constrained generative language model inference**, arXiv 2024  
  *Ren, Siyu and Zhu, Kenny Q*  
  [[Paper](https://arxiv.org/pdf/2402.06262)] [[Code](https://github.com/DRSY/EasyKV)] ![EasyKV](https://img.shields.io/badge/EasyKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Eviction_Scope](https://img.shields.io/badge/Eviction_Scope-yellow) ![Importance_Score](https://img.shields.io/badge/Importance_Score-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{ren2024efficacy,
    title={On the efficacy of eviction policy for key-value constrained generative language model inference},
    author={Ren, Siyu and Zhu, Kenny Q},
    journal={arXiv preprint arXiv:2402.06262},
    year={2024}
  }
  ```
  </details>  

- **[29] Snapkv: Llm knows what you are looking for before generation**, NeurIPS 2024  
  *Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen*  
  [[Paper](https://openreview.net/pdf?id=poE54GOq2l)] [[Code](https://github.com/FasterDecoding/SnapKV)] ![SnapKV](https://img.shields.io/badge/SnapKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  li2024snapkv,
  title={Snap{KV}: {LLM} Knows What You are Looking for Before Generation},
  author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=poE54GOq2l}
  }
  ```
  </details>  

- **[30] SubGen: Token Generation in Sublinear Time and Memory**, arXiv 2024  
  *Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin*  
  [[Paper](https://arxiv.org/pdf/2402.06082)] ![SubGen](https://img.shields.io/badge/SubGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Similarity](https://img.shields.io/badge/Similarity-purple) ![Clustering](https://img.shields.io/badge/Clustering-orange) ![Droping](https://img.shields.io/badge/Droping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zandieh2024subgen,
    title={SubGen: Token Generation in Sublinear Time and Memory},
    author={Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin},
    journal={arXiv preprint arXiv:2402.06082},
    year={2024}
  }
  ```
  </details>  

- **[31] Not All Heads Matter: A Head-Level KV Cache Compression Method with Integrated Retrieval and Reasoning**, arXiv 2024  
  *Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen*  
  [[Paper](https://arxiv.org/pdf/2410.19258)] [[Code](https://github.com/FYYFU/HeadKV)] ![HeadKV](https://img.shields.io/badge/HeadKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Retrieval_and_Reasoning_Importance](https://img.shields.io/badge/Retrieval_and_Reasoning_Importance-yellow) ![_Head--Level](https://img.shields.io/badge/_Head--Level-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{fu2024not,
    title={Not all heads matter: A head-level KV cache compression method with integrated retrieval and reasoning},
    author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},
    journal={arXiv preprint arXiv:2410.19258},
    year={2024}
  }
  ```
  </details>  

- **[32] CORM: Cache Optimization with Recent Message for Large Language Model Inference**, arXiv 2024  
  *Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi*  
  [[Paper](https://arxiv.org/pdf/2404.15949v2)] ![CORM](https://img.shields.io/badge/CORM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Query_Vector_Similarity](https://img.shields.io/badge/Query_Vector_Similarity-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @misc{dai2024corm,
      title={CORM: Cache Optimization with Recent Message for Large Language Model Inference},
      author={Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi},
      year={2024},
      eprint={2404.15949},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }
  ```
  </details>  

- **[33] Attention Score is not All You Need for Token Importance Indicator in KV Cache Reduction: Value Also Matters**, EMNLP 2024  
  *Guo, Zhiyu  and Kamigaito, Hidetaka  and  Watanabe, Taro*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1178.pdf)] [[Code](https://github.com/guozhiyu/vatp)] ![VATP](https://img.shields.io/badge/VATP-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![L1_Norm](https://img.shields.io/badge/L1_Norm-purple) ![Dropping](https://img.shields.io/badge/Dropping-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Value--aware_Token_Pruning](https://img.shields.io/badge/Value--aware_Token_Pruning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guo-etal-2024-attention,
      title = "Attention Score is not All You Need for Token Importance Indicator in {KV} Cache Reduction: Value Also Matters",
      author = "Guo, Zhiyu  and
        Kamigaito, Hidetaka  and
        Watanabe, Taro",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1178/",
      doi = "10.18653/v1/2024.emnlp-main.1178",
      pages = "21158--21166"
  }
  ```
  </details>  

- **[34] MiniCache: KV Cache Compression in Depth Dimension for Large Language Models**, NeurIPS 2024  
  *Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang*  
  [[Paper](https://openreview.net/pdf?id=sgVOjDqUMT)] [[Code](https://minicache.vmv.re/)] ![MiniCache](https://img.shields.io/badge/MiniCache-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![L2_Norm](https://img.shields.io/badge/L2_Norm-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Aaveraging](https://img.shields.io/badge/Aaveraging-orange) ![Retention](https://img.shields.io/badge/Retention-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  liu2024minicache,
  title={MiniCache: {KV} Cache Compression in Depth Dimension for Large Language Models},
  author={Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=sgVOjDqUMT}
  }
  ```
  </details>  

- **[35] Cam: Cache merging for memory-efficient llms inference**, ICML 2024  
  *Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji*  
  [[Paper](https://openreview.net/pdf?id=LCTmppB165)] [[Code](https://github.com/zyxxmu/cam)] ![CaM](https://img.shields.io/badge/CaM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Cache_Compression](https://img.shields.io/badge/KV_Cache_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  zhang2024cam,
  title={CaM: Cache Merging for Memory-efficient {LLM}s Inference},
  author={Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=LCTmppB165}
  }
  ```
  </details>  

- **[36] D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models**, 2024  
   


- **[37] Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration**, 2024  
   


- **[38] Sglang: Efficient execution of structured language model programs**, 2024  
   


- **[39] Prompt cache: Modular attention reuse for low-latency inference**, 2024  
   


- **[40] Cascade inference: Memory bandwidth efficient shared prefix batch decoding**, 2024  
   


- **[41] Hydragen: High-Throughput LLM Inference with Shared Prefixes**, 2024  
   


- **[42] RAGCache: Efficient Knowledge Caching for Retrieval-Augmented Generation**, 2024  
   


- **[43] CacheBlend: Fast Large Language Model Serving with Cached Knowledge Fusion**, 2024  
   


- **[44] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference**, 2024  
   


- **[45] Ada-kv: Optimizing kv cache eviction by adaptive budget allocation for efficient llm inference**, 2024  
   


- **[46] Massive activations in large language models**, 2024  
   


- **[47] Unifying KV Cache Compression for Large Language Models with LeanKV**, 2024  
   


- **[48] More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Cache Compression**, 2024  
   


- **[49] LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Cache Management**, 2024  
   


- **[50] KV-Compress: Paged KV-Cache Compression with Variable Compression Rates per Attention Head**, 2024  
   


- **[51] ZigZagkv: Dynamic KV Cache Compression for Long-context Modeling based on Layer Uncertainty**, 2024  
   


- **[52] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference**, 2024  
   


- **[53] KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Cache Sharing**, 2024  
   


- **[54] LoCoCo: Dropping In Convolutions for Long Context Compression**, 2024  
   


- **[55] SimLayerKV: A Simple Framework for Layer-Level KV Cache Reduction**, 2024  
   


- **[56] EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache Compression Based on Global-Local Importance**, 2024  
   


- **[57] ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable Compression**, 2024  
   


- **[58] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads**, 2024  
   


- **[59] No Token Left Behind: Reliable KV Cache Compression via Importance-Aware Mixed Precision Quantization**, 2024  
   


- **[60] LSH-E Tells You What To Discard: An Adaptive Locality-Sensitive Strategy for KV Cache Compression**, 2024  
   


- **[61] Enhancing and Accelerating Large Language Models via Instruction-Aware Contextual Compression**, 2024  
   


- **[62] LoMA: Lossless Compressed Memory Attention**, 2024  
   


- **[63] KV Cache Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches**, 2024  
   


- **[64] NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time**, 2024  
   


- **[65] TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Cache Selection**, 2024  
   


- **[66] ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction**, 2024  
   


- **[67] Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache**, 2024  
   


- **[68] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Cache Management**, 2024  
   


- **[69] XKV: Personalized KV Cache Memory Reduction for Long-Context LLM Inference**, 2024  
   


- **[70] In-context KV-Cache Eviction for LLMs via Attention-Gate**, 2024  
   


- **[71] Recycled Attention: Efficient inference for long-context language models**, 2024  
   


- **[72] MagicPIG: LSH Sampling for Efficient LLM Generation**, 2024  
   


- **[73] Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Cache**, 2024  
   


- **[74] When Attention Sink Emerges in Language Models: An Empirical View**, 2024  
   


- **[75] Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference**, 2024  
   


- **[76] SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**, 2024  
   


- **[77] A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference**, 2024  
   


- **[78] LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts**, 2024  
   


- **[79] Efficient Sparse Attention needs Adaptive Token Release**, 2024  
   


- **[80] Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling**, 2024  
   


