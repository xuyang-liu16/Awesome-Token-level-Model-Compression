# Token Compression in Language Domain

Maintainers: [Junjie Chen](https://www.junjie-chen.info), [Xuyang Liu](https://xuyang-liu16.github.io/)

If your work is missing, please let us know by creating an issue or submitting a pull request.

Color code: ![Abbreviation](https://img.shields.io/badge/Abbreviation-blue) ![Application](https://img.shields.io/badge/Application-green) ![Compression_Criteria](https://img.shields.io/badge/Compression_Criteria-purple) ![Compression_Mechanism](https://img.shields.io/badge/Compression_Mechanism-orange) ![W./W.o._Training](https://img.shields.io/badge/W./W.o._Training-brown) ![Keywords](https://img.shields.io/badge/Keywords-yellow) 

## 1999

- **[1] What is linguistic redundancy**, University of Chicago 1999  
  *Wit, EC and Gillette, Marie*  
  [[Paper](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=182d3980d8f41616da690d607f6f980dbf582352)] ![Theory](https://img.shields.io/badge/Theory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wit1999linguistic,
    title={What is linguistic redundancy},
    author={Wit, EC and Gillette, Marie},
    journal={University of Chicago},
    year={1999},
    publisher={Citeseer}
  }
  ```
  </details>  

## 2016

- **[1] Phased lstm: Accelerating recurrent network training for long or event-based sequences**, NeurIPS 2016  
  *Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii*  
  [[Paper](https://proceedings.neurips.cc/paper_files/paper/2016/file/5bce843dd76db8c939d5323dd3e54ec9-Paper.pdf)] ![Phased_lstm](https://img.shields.io/badge/Phased_lstm-blue) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{neil2016phased,
    title={Phased lstm: Accelerating recurrent network training for long or event-based sequences},
    author={Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
    journal={Advances in neural information processing systems},
    volume={29},
    year={2016}
  }
  ```
  </details>  

- **[2] Hierarchical question answering for long documents**, arXiv 2016  
  *Choi, Eunsol and Hewlett, Daniel and Lacoste, Alexandre and Polosukhin, Illia and Uszkoreit, Jakob and Berant, Jonathan*  
  [[Paper](https://arxiv.org/pdf/1611.01839)] ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{choi2016hierarchical,
    title={Hierarchical question answering for long documents},
    author={Choi, Eunsol and Hewlett, Daniel and Lacoste, Alexandre and Polosukhin, Illia and Uszkoreit, Jakob and Berant, Jonathan},
    journal={arXiv preprint arXiv:1611.01839},
    year={2016}
  }
  ```
  </details>  

## 2017

- **[1] Reasonet: Learning to stop reading in machine comprehension**, KDD 2017  
  *Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu*  
  [[Paper](https://arxiv.org/pdf/1609.05284)] ![Reasonet](https://img.shields.io/badge/Reasonet-blue) ![Marchine_Reading_Comprehension](https://img.shields.io/badge/Marchine_Reading_Comprehension-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{shen2017reasonet,
    title={Reasonet: Learning to stop reading in machine comprehension},
    author={Shen, Yelong and Huang, Po-Sen and Gao, Jianfeng and Chen, Weizhu},
    booktitle={Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining},
    pages={1047--1055},
    year={2017}
  }
  ```
  </details>  

- **[2] Learning to skim text**, ACL 2017  
  *Yu, Adams Wei and Lee, Hongrae and Le, Quoc V*  
  [[Paper](https://arxiv.org/pdf/1704.06877)] ![LSTM--Jump](https://img.shields.io/badge/LSTM--Jump-blue) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yu2017learning,
    title={Learning to Skim Text},
    author={Yu, Adams Wei and Lee, Hongrae and Le, Quoc},
    booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1880--1890},
    year={2017}
  }
  ```
  </details>  

- **[3] Learning when to skim and when to read**, RepL4NLP 2017  
  *Johansen, Alexander Rosenberg and Socher, Richard*  
  [[Paper](https://arxiv.org/pdf/1712.05483)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{johansen2017learning,
    title={Learning when to skim and when to read},
    author={Johansen, Alexander and Socher, Richard},
    booktitle={Proceedings of the 2nd Workshop on Representation Learning for NLP},
    pages={257--264},
    year={2017}
  }
  ```
  </details>  

- **[4] Training a subsampling mechanism in expectation**, ICLR 2017  
  *Raffel, Colin and Lawson, Dieterich*  
  [[Paper](https://arxiv.org/pdf/1702.06914)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{raffel2017training,
    title={Training a subsampling mechanism in expectation},
    author={Raffel, Colin and Lawson, Dieterich},
    journal={arXiv preprint arXiv:1702.06914},
    year={2017}
  }
  ```
  </details>  

- **[5] Variable computation in recurrent neural networks**, ICLR 2017  
  *Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas*  
  [[Paper](https://openreview.net/pdf?id=S1LVSrcge)] ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jernite2022variable,
    title={Variable Computation in Recurrent Neural Networks},
    author={Jernite, Yacine and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
    booktitle={International Conference on Learning Representations},
    year={2022}
  }
  ```
  </details>  

## 2018

- **[1] Neural speed reading via skim-rnn**, ICLR 2018  
  *Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh*  
  [[Paper](https://openreview.net/pdf?id=Sy-dQG-Rb)] ![Skim--RNN_](https://img.shields.io/badge/Skim--RNN_-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{seo2018neural,
    title={Neural Speed Reading via Skim-RNN},
    author={Seo, Minjoon and Min, Sewon and Farhadi, Ali and Hajishirzi, Hannaneh},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

- **[2] Skip rnn: Learning to skip state updates in recurrent neural networks**, ICLR 2018  
  *Campos, Víctor and Jou, Brendan and Giró-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu*  
  [[Paper](https://openreview.net/pdf?id=HkwVAXyCW)] [[Code](https://github.com/imatge-upc/skiprnn-2017-telecombcn)] ![Skip_RNN_](https://img.shields.io/badge/Skip_RNN_-blue) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{campos2018skip,
    title={Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks},
    author={Campos, V{\'\i}ctor and Jou, Brendan and Gir{\'o}-i-Nieto, Xavier and Torres, Jordi and Chang, Shih-Fu},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

- **[3] Accelerating neural transformer via an average attention network**, ACL 2018  
  *Zhang, Biao and Xiong, Deyi and Su, Jinsong*  
  [[Paper](https://aclanthology.org/P18-1166.pdf)] [[Code](https://github.com/bzhangGo/transformer-aan)] ![transformer--aan](https://img.shields.io/badge/transformer--aan-blue) ![Translation](https://img.shields.io/badge/Translation-green) ![Average_Attention](https://img.shields.io/badge/Average_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhang2018accelerating,
    title={Accelerating Neural Transformer via an Average Attention Network},
    author={Zhang, Biao and Xiong, Deyi and Su, Jinsong},
    booktitle={Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1789--1798},
    year={2018}
  }
  ```
  </details>  

- **[4] Generating wikipedia by summarizing long sequences**, ICLR 2018  
  *Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam*  
  [[Paper](https://openreview.net/pdf?id=Hyg0vbWC-)] [[Code](https://github.com/lucidrains/memory-compressed-attention)] ![Summarization](https://img.shields.io/badge/Summarization-green) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{liu2018generating,
    title={Generating Wikipedia by Summarizing Long Sequences},
    author={Liu, Peter J and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
    booktitle={International Conference on Learning Representations},
    year={2018}
  }
  ```
  </details>  

## 2019

- **[1] Transformer-XL: Attentive language models beyond a fixed-length context**, ACL 2019  
  *Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan*  
  [[Paper](https://aclanthology.org/P19-1285.pdf)] [[Code](https://github.com/kimiyoung/transformer-xl)] ![Transformer--XL](https://img.shields.io/badge/Transformer--XL-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Recurrent_Mechanism](https://img.shields.io/badge/Recurrent_Mechanism-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{dai2019transformer,
    title={Transformer-XL: Attentive Language Models beyond a Fixed-Length Context},
    author={Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime G and Le, Quoc and Salakhutdinov, Ruslan},
    booktitle={Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
    pages={2978--2988},
    year={2019}
  }
  ```
  </details>  

- **[2] Neural speed reading with structural-jump-lstm**, ICLR 2019  
  *Christian Hansen and Casper Hansen and Stephen Alstrup and Jakob Grue Simonsen and Christina Lioma*  
  [[Paper](https://openreview.net/pdf?id=B1xf9jAqFQ)] [[Code](https://github.com/Varyn/Neural-Speed-Reading-with-Structural-Jump-LSTM)] ![Structural--Jump--LSTM](https://img.shields.io/badge/Structural--Jump--LSTM-blue) ![Agent](https://img.shields.io/badge/Agent-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![RNN](https://img.shields.io/badge/RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  hansen2018neural,
  title={Neural Speed Reading with Structural-Jump-{LSTM}},
  author={Christian Hansen and Casper Hansen and Stephen Alstrup and Jakob Grue Simonsen and Christina Lioma},
  booktitle={International Conference on Learning Representations},
  year={2019},
  url={https://openreview.net/forum?id=B1xf9jAqFQ},
  }
  ```
  </details>  

- **[3] How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings**, EMNLP-IJCNLP 2019  
  *Ethayarajh, Kawin*  
  [[Paper](https://aclanthology.org/D19-1006/)] ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{ethayarajh2019contextual,
    title={How Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings},
    author={Ethayarajh, Kawin},
    booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
    pages={55--65},
    year={2019}
  }
  ```
  </details>  

## 2020

- **[1] Compressive transformers for long-range sequence modelling**, ICLR 2020  
  *Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Lillicrap, Timothy P*  
  [[Paper](https://openreview.net/pdf?id=SylKikSYDH)] [[Code](https://github.com/lucidrains/compressive-transformer-pytorch)] ![Compressive_Transformer](https://img.shields.io/badge/Compressive_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Most--used](https://img.shields.io/badge/Most--used-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{raecompressive,
    title={Compressive Transformers for Long-Range Sequence Modelling},
    author={Rae, Jack W and Potapenko, Anna and Jayakumar, Siddhant M and Hillier, Chloe and Lillicrap, Timothy P},
    booktitle={International Conference on Learning Representations}
  }
  ```
  </details>  

- **[2] PoWER-BERT: Accelerating BERT Inference via Progressive Word-vector Elimination**, ICML 2020  
  *Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish*  
  [[Paper](https://proceedings.mlr.press/v119/goyal20a/goyal20a.pdf)] [[Code](https://github.com/IBM/PoWER-BERT)] ![PoWER--BERT](https://img.shields.io/badge/PoWER--BERT-blue) ![QA](https://img.shields.io/badge/QA-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Embedding](https://img.shields.io/badge/Embedding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Static](https://img.shields.io/badge/Static-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{goyal2020power,
    title={Power-bert: Accelerating bert inference via progressive word-vector elimination},
    author={Goyal, Saurabh and Choudhury, Anamitra Roy and Raje, Saurabh and Chakaravarthy, Venkatesan and Sabharwal, Yogish and Verma, Ashish},
    booktitle={International Conference on Machine Learning},
    pages={3690--3699},
    year={2020},
    organization={PMLR}
  }
  ```
  </details>  

- **[3] Funnel-transformer: Filtering out sequential redundancy for efficient language processing**, NeurIPS 2020  
  *Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc*  
  [[Paper](https://proceedings.neurips.cc/paper/2020/file/2cd2915e69546904e4e5d4a2ac9e1652-Paper.pdf)] [[Code](https://github.com/laiguokun/Funnel-Transformer)] ![Funnel--transformer](https://img.shields.io/badge/Funnel--transformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{dai2020funnel,
    title={Funnel-transformer: Filtering out sequential redundancy for efficient language processing},
    author={Dai, Zihang and Lai, Guokun and Yang, Yiming and Le, Quoc},
    journal={Advances in neural information processing systems},
    volume={33},
    pages={4271--4282},
    year={2020}
  }
  ```
  </details>  

- **[4] Multi-scale Transformer Language Models**, arXiv 2020  
  *Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y-Lan*  
  [[Paper](https://arxiv.org/pdf/2005.00581)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Memory](https://img.shields.io/badge/Memory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{subramanian2020multi,
    title={Multi-scale Transformer Language Models},
    author={Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y-Lan},
    journal={arXiv preprint arXiv:2005.00581},
    year={2020}
  }
  ```
  </details>  

- **[5] Reformer: The efficient transformer**, ICLR 2020  
  *Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya*  
  [[Paper](https://openreview.net/pdf?id=rkgNKkHtvB)] [[Code](https://github.com/google/trax/tree/master/trax/models/reformer)] ![Reformer](https://img.shields.io/badge/Reformer-blue) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Translation](https://img.shields.io/badge/Translation-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Locality--Sensitive_Hashing](https://img.shields.io/badge/Locality--Sensitive_Hashing-orange) ![Caching](https://img.shields.io/badge/Caching-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  Kitaev2020Reformer:,
  title={Reformer: The Efficient Transformer},
  author=[@authors]{Nikita Kitaev and Lukasz Kaiser and Anselm Levskaya},
  booktitle={International Conference on Learning Representations},
  year={2020},
  url={https://openreview.net/forum?id=rkgNKkHtvB}
  }
  ```
  </details>  

- **[6] Spying on your neighbors: Fine-grained probing of contextual embeddings for information about surrounding words**, ACL 2020  
  *Klafka, Josef and Ettinger, Allyson*  
  [[Paper](https://aclanthology.org/2020.acl-main.434.pdf)] [[Code](https://github.com/jklafka/context-probes)] ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{klafka2020spying,
    title={Spying on Your Neighbors: Fine-grained Probing of Contextual Embeddings for Information about Surrounding Words},
    author={Klafka, Josef and Ettinger, Allyson},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    pages={4801--4811},
    year={2020}
  }
  ```
  </details>  

- **[7] DeFormer: Decomposing pre-trained transformers for faster question answering**, ACL 2020  
  *Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan*  
  [[Paper](https://aclanthology.org/2020.acl-main.411.pdf)] [[Code](https://github.com/StonyBrookNLP/deformer)] ![DeFormer](https://img.shields.io/badge/DeFormer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sentence--Sentence](https://img.shields.io/badge/Sentence--Sentence-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder](https://img.shields.io/badge/Transformer_Encoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{cao2020deformer,
    title={DeFormer: Decomposing Pre-trained Transformers for Faster Question Answering},
    author={Cao, Qingqing and Trivedi, Harsh and Balasubramanian, Aruna and Balasubramanian, Niranjan},
    booktitle={Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
    pages={4487--4497},
    year={2020}
  }
  ```
  </details>  

- **[8] Attention is not only a weight: Analyzing transformers with vector norms**, EMNLP 2020  
  *Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro*  
  [[Paper](https://aclanthology.org/2020.emnlp-main.574.pdf)] [[Code](https://github.com/gorokoba560/norm-analysis-of-transformer)] ![Norm--based_Analysis](https://img.shields.io/badge/Norm--based_Analysis-yellow) ![Explanation](https://img.shields.io/badge/Explanation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kobayashi2020attention,
    title={Attention is Not Only a Weight: Analyzing Transformers with Vector Norms},
    author={Kobayashi, Goro and Kuribayashi, Tatsuki and Yokoi, Sho and Inui, Kentaro},
    booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
    pages={7057--7075},
    year={2020}
  }
  ```
  </details>  

## 2021

- **[1] TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference**, NAACL 2021  
  *Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong*  
  [[Paper](https://aclanthology.org/2021.naacl-main.463.pdf)] [[Code](https://github.com/thunlp/TR-BERT)] ![TR--BERT](https://img.shields.io/badge/TR--BERT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Random](https://img.shields.io/badge/Random-purple) ![Prediction--guided](https://img.shields.io/badge/Prediction--guided-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{ye2021tr,
    title={TR-BERT: Dynamic Token Reduction for Accelerating BERT Inference},
    author={Ye, Deming and Lin, Yankai and Huang, Yufei and Sun, Maosong},
    booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages={5798--5809},
    year={2021}
  }
  ```
  </details>  

- **[2] Poolingformer: Long document modeling with pooling attention**, ICML  2021  
  *Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv, Jiancheng and Duan, Nan and Chen, Weizhu*  
  [[Paper](http://proceedings.mlr.press/v139/zhang21h/zhang21h.pdf)] ![Poolingformer](https://img.shields.io/badge/Poolingformer-blue) ![Long_Sequence_Summarization](https://img.shields.io/badge/Long_Sequence_Summarization-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhang2021poolingformer,
    title={Poolingformer: Long document modeling with pooling attention},
    author={Zhang, Hang and Gong, Yeyun and Shen, Yelong and Li, Weisheng and Lv, Jiancheng and Duan, Nan and Chen, Weizhu},
    booktitle={International Conference on Machine Learning},
    pages={12437--12446},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

- **[3] Length-adaptive transformer: Train once with length drop, use anytime with search**, ACL 2021  
  *Kim, Gyuwan and Cho, Kyunghyun*  
  [[Paper](https://aclanthology.org/2021.acl-long.508.pdf)] [[Code](https://github.com/clovaai/length-adaptive-transformer)] ![Length--Adaptive_Transformer](https://img.shields.io/badge/Length--Adaptive_Transformer-blue) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Token--leval_Classification](https://img.shields.io/badge/Token--leval_Classification-green) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Distillation](https://img.shields.io/badge/Distillation-yellow) ![LayerDrop](https://img.shields.io/badge/LayerDrop-yellow) ![LengthDrop](https://img.shields.io/badge/LengthDrop-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kim2021length,
    title={Length-Adaptive Transformer: Train Once with Length Drop, Use Anytime with Search},
    author={Kim, Gyuwan and Cho, Kyunghyun},
    booktitle={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
    pages={6501--6511},
    year={2021}
  }
  ```
  </details>  

- **[4] Efficient content-based sparse attention with routing transformers**, TACL 2021  
  *Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David*  
  [[Paper](https://aclanthology.org/2021.tacl-1.4.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/routing_transformer)] ![Routing_Transformer](https://img.shields.io/badge/Routing_Transformer-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Image_Generation](https://img.shields.io/badge/Image_Generation-green) ![Clustering](https://img.shields.io/badge/Clustering-orange) ![Routing_Attention](https://img.shields.io/badge/Routing_Attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{roy2021efficient,
    title={Efficient content-based sparse attention with routing transformers},
    author={Roy, Aurko and Saffar, Mohammad and Vaswani, Ashish and Grangier, David},
    journal={Transactions of the Association for Computational Linguistics},
    volume={9},
    pages={53--68},
    year={2021},
    publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
  }
  ```
  </details>  

- **[5] Not all memories are created equal: Learning to forget by expiring**, ICML 2021  
  *Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela*  
  [[Paper](http://proceedings.mlr.press/v139/sukhbaatar21a/sukhbaatar21a.pdf)] [[Code](https://github.com/facebookresearch/transformer-sequential)] ![Expire--Span](https://img.shields.io/badge/Expire--Span-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Expire](https://img.shields.io/badge/Expire-orange) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Expire](https://img.shields.io/badge/Expire-yellow) ![Memory](https://img.shields.io/badge/Memory-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{sukhbaatar2021not,
    title={Not all memories are created equal: Learning to forget by expiring},
    author={Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
    booktitle={International Conference on Machine Learning},
    pages={9902--9912},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

- **[6] On the transformer growth for progressive bert training**, NAACL 2021  
  *Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei*  
  [[Paper](https://aclanthology.org/2021.naacl-main.406.pdf)] [[Code](https://github.com/google-research/google-research/tree/master/grow_bert)] ![CompoundGrow](https://img.shields.io/badge/CompoundGrow-blue) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![BERT_Pretraining](https://img.shields.io/badge/BERT_Pretraining-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{gu2021transformer,
    title={On the Transformer Growth for Progressive BERT Training},
    author={Gu, Xiaotao and Liu, Liyuan and Yu, Hongkun and Li, Jing and Chen, Chen and Han, Jiawei},
    booktitle={Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
    pages={5174--5180},
    year={2021}
  }
  ```
  </details>  

- **[7] Memory-efficient Transformers via Top-$ k $ Attention**, SUSTAINLP 2021  
  *Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2021.sustainlp-1.5.pdf)] [[Code](https://github.com/ag1988/top_k_attention)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Query_chunking](https://img.shields.io/badge/Query_chunking-orange) ![Top--k_attention](https://img.shields.io/badge/Top--k_attention-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{gupta2021memory,
    title={Memory-efficient Transformers via Top-k Attention},
    author={Gupta, Ankit and Dar, Guy and Goodman, Shaya and Ciprut, David and Berant, Jonathan},
    booktitle={Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing},
    pages={39--52},
    year={2021}
  }
  ```
  </details>  

- **[8] Magic pyramid: Accelerating inference with early exiting and token pruning**, NeurIPS-ENLSP 2021  
  *He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul*  
  [[Paper](https://neurips2021-nlp.github.io/papers/21/CameraReady/magic_pyramid.pdf)] ![Magic_Pyramid](https://img.shields.io/badge/Magic_Pyramid-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity--based_Detection](https://img.shields.io/badge/Similarity--based_Detection-green) ![Topic_Identification_](https://img.shields.io/badge/Topic_Identification_-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Early_Existing](https://img.shields.io/badge/Early_Existing-yellow) ![Token_Pruning](https://img.shields.io/badge/Token_Pruning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{he2021magic,
    title={Magic pyramid: Accelerating inference with early exiting and token pruning},
    author={He, Xuanli and Keivanloo, Iman and Xu, Yi and He, Xiang and Zeng, Belinda and Rajagopalan, Santosh and Chilimbi, Trishul},
    journal={arXiv preprint arXiv:2111.00230},
    year={2021}
  }
  ```
  </details>  

- **[9] El-attention: Memory efficient lossless attention for generation**, ICML 2021  
  *Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and Gong, Yeyun and Duan, Nan and Zhang, Ruofei*  
  [[Paper](https://proceedings.mlr.press/v139/yan21a/yan21a.pdf)] [[Code](https://github.com/microsoft/fastseq)] ![EL--Attention](https://img.shields.io/badge/EL--Attention-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Encoder--Decoder_Attention](https://img.shields.io/badge/Encoder--Decoder_Attention-yellow) ![Self--Attention](https://img.shields.io/badge/Self--Attention-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yan2021attention,
    title={El-attention: Memory efficient lossless attention for generation},
    author={Yan, Yu and Chen, Jiusheng and Qi, Weizhen and Bhendawade, Nikhil and Gong, Yeyun and Duan, Nan and Zhang, Ruofei},
    booktitle={International Conference on Machine Learning},
    pages={11648--11658},
    year={2021},
    organization={PMLR}
  }
  ```
  </details>  

## 2022

- **[1] Learned Token Pruning for Transformers**, KDD 2022  
  *Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt*  
  [[Paper](https://dl.acm.org/doi/pdf/10.1145/3534678.3539260)] [[Code](https://github.com/kssteven418/LTP)] ![LTP](https://img.shields.io/badge/LTP-blue) ![Sentence_Similarity--based](https://img.shields.io/badge/Sentence_Similarity--based-green) ![Classification](https://img.shields.io/badge/Classification-green) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Threshold](https://img.shields.io/badge/Learnable_Threshold-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{kim2022learned,
    title={Learned token pruning for transformers},
    author={Kim, Sehoon and Shen, Sheng and Thorsley, David and Gholami, Amir and Kwon, Woosuk and Hassoun, Joseph and Keutzer, Kurt},
    booktitle={Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages={784--794},
    year={2022}
  }
  ```
  </details>  

- **[2] Block-skim: Efficient question answering for transformer**, AAAI  2022  
  *Guan, Yue and Li, Zhengyi and Lin, Zhouhan and Zhu, Yuhao and Leng, Jingwen and Guo, Minyi*  
  [[Paper](https://ojs.aaai.org/index.php/AAAI/article/view/21316/21065)] [[Code](https://github.com/ChandlerGuan/blockskim)] ![Block--Skim](https://img.shields.io/badge/Block--Skim-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Conv](https://img.shields.io/badge/Conv-orange) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guan2022block,
    title={Block-skim: Efficient question answering for transformer},
    author={Guan, Yue and Li, Zhengyi and Lin, Zhouhan and Zhu, Yuhao and Leng, Jingwen and Guo, Minyi},
    booktitle={Proceedings of the AAAI conference on artificial intelligence},
    volume={36},
    number={10},
    pages={10710--10719},
    year={2022}
  }
  ```
  </details>  

- **[3] Prompt compression and contrastive conditioning for controllability and toxicity reduction in language models**, ACL 2022  
  *Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor*  
  [[Paper](https://aclanthology.org/2022.findings-emnlp.412.pdf)] [[Code](https://github.com/BYU-PCCL/prompt-compression-contrastive-coding)] ![Toxicity_reduction](https://img.shields.io/badge/Toxicity_reduction-green) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Prompt_Compression](https://img.shields.io/badge/Prompt_Compression-yellow) ![Exploration](https://img.shields.io/badge/Exploration-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{wingate2022prompt,
    title={Prompt Compression and Contrastive Conditioning for Controllability and Toxicity Reduction in Language Models},
    author={Wingate, David and Shoeybi, Mohammad and Sorensen, Taylor},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2022},
    pages={5621--5634},
    year={2022}
  }
  ```
  </details>  

- **[4] Memorizing transformers**, ICLR 2022  
  *Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy*  
  [[Paper](https://openreview.net/pdf?id=TrjbxzRcnf-)] [[Code](https://github.com/lucidrains/memorizing-transformers-pytorch)] ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Caching](https://img.shields.io/badge/Caching-orange) ![kNN_Attention](https://img.shields.io/badge/kNN_Attention-orange) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  wu2022memorizing,
  title={Memorizing Transformers},
  author={Yuhuai Wu and Markus Norman Rabe and DeLesley Hutchins and Christian Szegedy},
  booktitle={International Conference on Learning Representations},
  year={2022},
  url={https://openreview.net/forum?id=TrjbxzRcnf-}
  }
  ```
  </details>  

- **[5] Transkimmer: Transformer learns to layer-wise skim**, ACL 2022  
  *Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi*  
  [[Paper](https://aclanthology.org/2022.acl-long.502.pdf)] [[Code](https://github.com/chandlerguan/transkimmer)] ![Transkimmer](https://img.shields.io/badge/Transkimmer-blue) ![QA](https://img.shields.io/badge/QA-green) ![Sequence--leval_Classification](https://img.shields.io/badge/Sequence--leval_Classification-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Skimming](https://img.shields.io/badge/Skimming-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guan2022transkimmer,
    title={Transkimmer: Transformer Learns to Layer-wise Skim},
    author={Guan, Yue and Li, Zhengyi and Leng, Jingwen and Lin, Zhouhan and Guo, Minyi},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={7275--7286},
    year={2022}
  }
  ```
  </details>  

- **[6] Adapler: Speeding up inference by adaptive length reduction**, ACL 2022  
  *Modarressi, Ali and Mohebbi, Hosein and Pilehvar, Mohammad Taher*  
  [[Paper](https://aclanthology.org/2022.acl-long.1.pdf)] [[Code](https://github.com/amodaresi/AdapLeR)] ![AdapLeR](https://img.shields.io/badge/AdapLeR-blue) ![Topic_Classification](https://img.shields.io/badge/Topic_Classification-green) ![Sentiment_Classification](https://img.shields.io/badge/Sentiment_Classification-green) ![Knowledge_Extraction](https://img.shields.io/badge/Knowledge_Extraction-green) ![QA](https://img.shields.io/badge/QA-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{modarressi2022adapler,
    title={AdapLeR: Speeding up Inference by Adaptive Length Reduction},
    author={Modarressi, Ali and Mohebbi, Hosein and Pilehvar, Mohammad Taher},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={1--15},
    year={2022}
  }
  ```
  </details>  

- **[7] Fine-and coarse-granularity hybrid self-attention for efficient bert**, ACL 2022  
  *Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong*  
  [[Paper](https://aclanthology.org/2022.acl-long.330.pdf)] [[Code](https://github.com/pierre-zhao/FCA-BERT)] ![FCA--BERT](https://img.shields.io/badge/FCA--BERT-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Similarity--based](https://img.shields.io/badge/Similarity--based-green) ![NLI](https://img.shields.io/badge/NLI-green) ![Sentiment](https://img.shields.io/badge/Sentiment-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhao2022fine,
    title={Fine-and Coarse-Granularity Hybrid Self-Attention for Efficient BERT},
    author={Zhao, Jing and Wang, Yifan and Bao, Junwei and Wu, Youzheng and He, Xiaodong},
    booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={4811--4820},
    year={2022}
  }
  ```
  </details>  

## 2023

- **[1] Efficient long-text understanding with short-text models**, TACL 2023  
  *Ivgi, Maor and Shaham, Uri and Berant, Jonathan*  
  [[Paper](https://aclanthology.org/2023.tacl-1.17.pdf)] [[Code](https://github.com/Mivg/SLED)] ![SLED](https://img.shields.io/badge/SLED-blue) ![Summarization](https://img.shields.io/badge/Summarization-green) ![QA](https://img.shields.io/badge/QA-green) ![Language_Understanding](https://img.shields.io/badge/Language_Understanding-green) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{ivgi2023efficient,
    title={Efficient long-text understanding with short-text models},
    author={Ivgi, Maor and Shaham, Uri and Berant, Jonathan},
    journal={Transactions of the Association for Computational Linguistics},
    volume={11},
    pages={284--299},
    year={2023},
    publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
  }
  ```
  </details>  

- **[2] Extensible prompts for language models on Zero-shot Language Style Customization**, NeurIPS 2023  
  *Tao Ge and Jing Hu and Li Dong and Shaoguang Mao and Yan Xia and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=lRxpVfDMzz)] ![Language_Style_Customization](https://img.shields.io/badge/Language_Style_Customization-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2023extensible,
  title={Extensible Prompts for Language Models on Zero-shot Language Style Customization},
  author={Tao Ge and Jing Hu and Li Dong and Shaoguang Mao and Yan Xia and Xun Wang and Si-Qing Chen and Furu Wei},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=lRxpVfDMzz}
  }
  ```
  </details>  

- **[3] Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification**, EMNLP 2023  
  *Yun, Jungmin and Kim, Mihyeon and Kim, Youngbin*  
  [[Paper](https://aclanthology.org/2023.findings-emnlp.909.pdf)] ![Document_Classification](https://img.shields.io/badge/Document_Classification-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Fuzzy--based_Token_Pruning](https://img.shields.io/badge/Fuzzy--based_Token_Pruning-orange) ![Token_Combining](https://img.shields.io/badge/Token_Combining-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yun2023focus,
    title={Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification},
    author={Yun, Jungmin and Kim, Mihyeon and Kim, Youngbin},
    booktitle={Findings of the Association for Computational Linguistics: EMNLP 2023},
    pages={13617--13628},
    year={2023}
  }
  ```
  </details>  

- **[4] Compressing context to enhance inference efficiency of large language models**, EMNLP 2023  
  *Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.391.pdf)] [[Code](https://github.com/liyucheng09/Selective_Context)] ![Selective_Context](https://img.shields.io/badge/Selective_Context-blue) ![QA](https://img.shields.io/badge/QA-green) ![Summarization](https://img.shields.io/badge/Summarization-green) ![Conversation](https://img.shields.io/badge/Conversation-green) ![Self--Information](https://img.shields.io/badge/Self--Information-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Generation](https://img.shields.io/badge/LLM_Generation-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{li2023compressing,
    title={Compressing Context to Enhance Inference Efficiency of Large Language Models},
    author={Li, Yucheng and Dong, Bo and Guerin, Frank and Lin, Chenghua},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={6342--6353},
    year={2023}
  }
  ```
  </details>  

- **[5] Did you read the instructions? rethinking the effectiveness of task definitions in instruction learning**, ACL 2023  
  *Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng*  
  [[Paper](https://aclanthology.org/2023.acl-long.172.pdf)] [[Code](https://github.com/fanyin3639/Rethinking-instruction-effectiveness)] ![STDC](https://img.shields.io/badge/STDC-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Syntax--guided](https://img.shields.io/badge/Syntax--guided-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yin2023did,
    title={Did You Read the Instructions? Rethinking the Effectiveness of Task Definitions in Instruction Learning},
    author={Yin, Fan and Vig, Jesse and Laban, Philippe and Joty, Shafiq and Xiong, Caiming and Wu, Chien-Sheng},
    booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={3063--3079},
    year={2023}
  }
  ```
  </details>  

- **[6] Llmlingua: Compressing prompts for accelerated inference of large language models**, EMNLP 2023  
  *Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.825.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LLMLingua_](https://img.shields.io/badge/LLMLingua_-blue) ![Classification](https://img.shields.io/badge/Classification-green) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jiang2023llmlingua,
    title={LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models},
    author={Jiang, Huiqiang and Wu, Qianhui and Lin, Chin-Yew and Yang, Yuqing and Qiu, Lili},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={13358--13376},
    year={2023}
  }
  ```
  </details>  

- **[7] Learning to compress prompts with gist tokens**, NeurIPS 2023  
  *Mu, Jesse and Li, Xiang and Goodman, Noah*  
  [[Paper](https://openreview.net/pdf?id=2DtxPCL3T5)] [[Code](https://github.com/jayelm/gisting)] ![gisting](https://img.shields.io/badge/gisting-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  mu2023learning,
  title={Learning to Compress Prompts with Gist Tokens},
  author={Jesse Mu and Xiang Lisa Li and Noah Goodman},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=2DtxPCL3T5}
  }
  ```
  </details>  

- **[8] Adapting language models to compress contexts**, EMNLP 2023  
  *Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.232.pdf)] [[Code](https://github.com/princeton-nlp/AutoCompressors)] ![AutoCompressors](https://img.shields.io/badge/AutoCompressors-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{chevalier2023adapting,
    title={Adapting Language Models to Compress Contexts},
    author={Chevalier, Alexis and Wettig, Alexander and Ajith, Anirudh and Chen, Danqi},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={3829--3846},
    year={2023}
  }
  ```
  </details>  

- **[9] Inference with reference: Lossless acceleration of large language models**, arXiv 2023  
  *Yang, Nan and Ge, Tao and Wang, Liang and Jiao, Binxing and Jiang, Daxin and Yang, Linjun and Majumder, Rangan and Wei, Furu*  
  [[Paper](https://arxiv.org/pdf/2304.04487)] ![LLMA](https://img.shields.io/badge/LLMA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Copy](https://img.shields.io/badge/Copy-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @misc{yang2023inference,
      title={Inference with Reference: Lossless Acceleration of Large Language Models},
      author={Nan Yang and Tao Ge and Liang Wang and Binxing Jiao and Daxin Jiang and Linjun Yang and Rangan Majumder and Furu Wei},
      year={2023},
      eprint={2304.04487},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }
  ```
  </details>  

- **[10] Efficient prompting via dynamic in-context learning**, arXiv 2023  
  *Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya*  
  [[Paper](https://arxiv.org/pdf/2305.11170)] ![DynaICL](https://img.shields.io/badge/DynaICL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhou2023efficient,
    title={Efficient prompting via dynamic in-context learning},
    author={Zhou, Wangchunshu and Jiang, Yuchen Eleanor and Cotterell, Ryan and Sachan, Mrinmaya},
    journal={arXiv preprint arXiv:2305.11170},
    year={2023}
  }
  ```
  </details>  

- **[11] Nugget: Neural agglomerative embeddings of text**, ICML  2023  
  *Qin, Guanghui and Van Durme, Benjamin*  
  [[Paper](https://arxiv.org/pdf/2310.01732)] [[Code](https://github.com/hiaoxui/nugget)] ![Nugget](https://img.shields.io/badge/Nugget-blue) ![Similarity--based](https://img.shields.io/badge/Similarity--based-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Nugget_Generator](https://img.shields.io/badge/Nugget_Generator-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{qin2023nugget,
    title={NUGGET: neural agglomerative embeddings of text},
    author={Qin, Guanghui and Van Durme, Benjamin},
    booktitle={Proceedings of the 40th International Conference on Machine Learning},
    pages={28337--28350},
    year={2023}
  }
  ```
  </details>  

- **[12] Unlimiformer: Long-range transformers with unlimited length input**, NeurIPS 2023  
  *Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley*  
  [[Paper](https://openreview.net/pdf?id=lJWUJWLCJo)] [[Code](https://github.com/abertsch72/unlimiformer)] ![Unlimiformer](https://img.shields.io/badge/Unlimiformer-blue) ![Similarity--based](https://img.shields.io/badge/Similarity--based-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Top--k_retrieval](https://img.shields.io/badge/Top--k_retrieval-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Transformer_Encoder--Decoder](https://img.shields.io/badge/Transformer_Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  bertsch2023unlimiformer,
  title={Unlimiformer: Long-Range Transformers with Unlimited Length Input},
  author={Amanda Bertsch and Uri Alon and Graham Neubig and Matthew R. Gormley},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=lJWUJWLCJo}
  }
  ```
  </details>  

- **[13] Walking down the memory maze: Beyond context limit through interactive reading**, arXiv 2023  
  *Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli*  
  [[Paper](https://arxiv.org/pdf/2310.05029)] ![MemWalker](https://img.shields.io/badge/MemWalker-blue) ![Similarity--based](https://img.shields.io/badge/Similarity--based-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Divide_and_Donquer](https://img.shields.io/badge/Divide_and_Donquer-orange) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{chen2023walking,
    title={Walking down the memory maze: Beyond context limit through interactive reading},
    author={Chen, Howard and Pasunuru, Ramakanth and Weston, Jason and Celikyilmaz, Asli},
    journal={arXiv preprint arXiv:2310.05029},
    year={2023}
  }
  ```
  </details>  

- **[14] Sparse token transformer with attention back tracking**, ICLR 2023  
  *Heejun Lee and Minki Kang and Youngwan Lee and Sung Ju Hwang*  
  [[Paper](https://openreview.net/pdf?id=VV0hSE8AxCw)] [[Code](https://github.com/gmlwns2000/sttabt)] ![STTABT](https://img.shields.io/badge/STTABT-blue) ![Text_Classification](https://img.shields.io/badge/Text_Classification-green) ![Image_Classification](https://img.shields.io/badge/Image_Classification-green) ![Attention_Approximation_Network](https://img.shields.io/badge/Attention_Approximation_Network-purple) ![Attention_Back--tracking](https://img.shields.io/badge/Attention_Back--tracking-orange) ![Concrete_Masking](https://img.shields.io/badge/Concrete_Masking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  lee2023sparse,
  title={Sparse Token Transformer with Attention Back Tracking},
  author={Heejun Lee and Minki Kang and Youngwan Lee and Sung Ju Hwang},
  booktitle={The Eleventh International Conference on Learning Representations },
  year={2023},
  url={https://openreview.net/forum?id=VV0hSE8AxCw}
  }
  ```
  </details>  

- **[15] Scissorhands: Exploiting the persistence of importance hypothesis for llm kv Caching compression at test time**, NeurIPS 2023  
  *Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava*  
  [[Paper](https://openreview.net/pdf?id=JZfg6wGi6g)] ![Scissorhands](https://img.shields.io/badge/Scissorhands-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  liu2023scissorhands,
  title={Scissorhands: Exploiting the Persistence of Importance Hypothesis for {LLM} {KV} Caching Compression at Test Time},
  author={Zichang Liu and Aditya Desai and Fangshuo Liao and Weitao Wang and Victor Xie and Zhaozhuo Xu and Anastasios Kyrillidis and Anshumali Shrivastava},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=JZfg6wGi6g}
  }
  ```
  </details>  

- **[16] H2o: Heavy-hitter oracle for efficient generative inference of large language models**, NeurIPS 2023  
  *Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen*  
  [[Paper](https://openreview.net/pdf?id=RkRrPp7GKO)] [[Code](https://github.com/FMInference/H2O)] ![H2O](https://img.shields.io/badge/H2O-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  zhang2023ho,
  title={H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models},
  author={Zhenyu Zhang and Ying Sheng and Tianyi Zhou and Tianlong Chen and Lianmin Zheng and Ruisi Cai and Zhao Song and Yuandong Tian and Christopher Re and Clark Barrett and Zhangyang Wang and Beidi Chen},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=RkRrPp7GKO}
  }
  ```
  </details>  

- **[17] Landmark attention: Random-access infinite context length for transformers**, NeurIPS 2023  
  *Amirkeivan Mohtashami and Martin Jaggi*  
  [[Paper](https://openreview.net/pdf?id=7eHn64wOVy)] [[Code](https://github.com/epfml/landmark-attention/)] ![Landmark_attention](https://img.shields.io/badge/Landmark_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Soft_Prompt](https://img.shields.io/badge/Soft_Prompt-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  mohtashami2023randomaccess,
  title={Random-Access Infinite Context Length for Transformers},
  author={Amirkeivan Mohtashami and Martin Jaggi},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=7eHn64wOVy}
  }
  ```
  </details>  

- **[18] Optimizing retrieval-augmented reader models via token elimination**, EMNLP 2023  
  *Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe*  
  [[Paper](https://aclanthology.org/2023.emnlp-main.93.pdf)] [[Code](https://github.com/IntelLabs/token_elimination)] ![Token_Elimination](https://img.shields.io/badge/Token_Elimination-blue) ![QA](https://img.shields.io/badge/QA-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{berchansky2023optimizing,
    title={Optimizing Retrieval-augmented Reader Models via Token Elimination},
    author={Berchansky, Moshe and Izsak, Peter and Caciularu, Avi and Dagan, Ido and Wasserblat, Moshe},
    booktitle={Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing},
    pages={1506--1524},
    year={2023}
  }
  ```
  </details>  

- **[19] Efficient transformers with dynamic token pooling**, ACL 2023  
  *Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria*  
  [[Paper](https://aclanthology.org/2023.acl-long.353.pdf)] [[Code](https://github.com/PiotrNawrot/dynamic-pooling)] ![Dynamic_Token_Pooling](https://img.shields.io/badge/Dynamic_Token_Pooling-blue) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{nawrot2023efficient,
    title={Efficient Transformers with Dynamic Token Pooling},
    author={Nawrot, Piotr and Chorowski, Jan and Lancucki, Adrian and Ponti, Edoardo Maria},
    booktitle={Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    pages={6403--6417},
    year={2023}
  }
  ```
  </details>  

- **[20] Dynamic context pruning for efficient and interpretable autoregressive transformers**, NeurIPS 2023  
  *Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann*  
  [[Paper](https://openreview.net/pdf?id=uvdJgFFzby)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  anagnostidis2023dynamic,
  title={Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers},
  author={Sotiris Anagnostidis and Dario Pavllo and Luca Biggio and Lorenzo Noci and Aurelien Lucchi and Thomas Hofmann},
  booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
  year={2023},
  url={https://openreview.net/forum?id=uvdJgFFzby}
  }
  ```
  </details>  

## 2024

- **[1] Longllmlingua: Accelerating and enhancing llms in long context scenarios via prompt compression**, ACL 2024  
  *Jiang, Huiqiang  and Wu, Qianhui  and Luo, Xufang  and Li, Dongsheng  and Lin, Chin-Yew  and Yang, Yuqing  and Qiu, Lili*  
  [[Paper](https://aclanthology.org/2024.acl-long.91.pdf)] [[Code](https://aka.ms/LLMLingua)] ![LongLLMLingua](https://img.shields.io/badge/LongLLMLingua-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Budget_Controller](https://img.shields.io/badge/Budget_Controller-purple) ![Drop--and--Restore](https://img.shields.io/badge/Drop--and--Restore-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{jiang-etal-2024-longllmlingua,
      title = "{L}ong{LLML}ingua: Accelerating and Enhancing {LLM}s in Long Context Scenarios via Prompt Compression",
      author = "Jiang, Huiqiang  and
        Wu, Qianhui  and
        Luo, Xufang  and
        Li, Dongsheng  and
        Lin, Chin-Yew  and
        Yang, Yuqing  and
        Qiu, Lili",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.91",
      doi = "10.18653/v1/2024.acl-long.91",
      pages = "1658--1677"
  }
  ```
  </details>  

- **[2] In-context autoencoder for context compression in a large language model**, ICLR 2024  
  *Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=uREj4ZuGJE)] [[Code](https://github.com/getao/icae)] ![ICAE](https://img.shields.io/badge/ICAE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2024incontext,
  title={In-context Autoencoder for Context Compression in a Large Language Model},
  author={Tao Ge and Hu Jing and Lei Wang and Xun Wang and Si-Qing Chen and Furu Wei},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=uREj4ZuGJE}
  }
  ```
  </details>  

- **[3] Efficient streaming language models with attention sinks**, ICLR 2024  
  *Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis*  
  [[Paper](https://openreview.net/pdf?id=NG7sS51zVF)] [[Code](https://github.com/mit-han-lab/streaming-llm)] ![StreamingLLM](https://img.shields.io/badge/StreamingLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  xiao2024efficient,
  title={Efficient Streaming Language Models with Attention Sinks},
  author={Guangxuan Xiao and Yuandong Tian and Beidi Chen and Song Han and Mike Lewis},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=NG7sS51zVF}
  }
  ```
  </details>  

- **[4] Model tells you what to discard: Adaptive kv Caching compression for llms**, ICLR 2024  
  *Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao*  
  [[Paper](https://openreview.net/pdf?id=uNrFpDPMyo)] [[Code](https://github.com/machilusZ/FastGen)] ![FastGen](https://img.shields.io/badge/FastGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Policy](https://img.shields.io/badge/Policy-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ge2024model,
  title={Model Tells You What to Discard: Adaptive {KV} Caching Compression for {LLM}s},
  author={Suyu Ge and Yunan Zhang and Liyuan Liu and Minjia Zhang and Jiawei Han and Jianfeng Gao},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=uNrFpDPMyo}
  }
  ```
  </details>  

- **[5] Sparq attention: Bandwidth-efficient llm inference**, ICLR-ME-FoMo 2024  
  *Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr*  
  [[Paper](https://openreview.net/pdf?id=Ue8EHzaFI4)] [[Code](https://github.com/graphcore-research/llm-inference-research/tree/2024-01-paper)] ![SparQ_attention](https://img.shields.io/badge/SparQ_attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Caching](https://img.shields.io/badge/Caching-orange) ![Sparsity](https://img.shields.io/badge/Sparsity-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  ribar2024sparq,
  title={SparQ Attention: Bandwidth-Efficient {LLM} Inference},
  author={Luka Ribar and Ivan Chelombiev and Luke Hudlass-Galley and Charlie Blake and Carlo Luschi and Douglas Orr},
  booktitle={ICLR 2024 Workshop on Mathematical and Empirical Understanding of Foundation Models},
  year={2024},
  url={https://openreview.net/forum?id=Ue8EHzaFI4}
  }
  ```
  </details>  

- **[6] FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing**, arXiv 2024  
  *Li, Zekai and Zheng, Jintu and Liu, Ji and Liu, Han and Zhu, Haowei and Li, Zeping and Yang, Fuwei and Huang, Haiduo and Peng, Jinzhang and Li, Dong and others*  
  [[Paper](https://arxiv.org/pdf/2412.11494)] ![FTP](https://img.shields.io/badge/FTP-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024ftp,
    title={FTP: A Fine-grained Token-wise Pruner for Large Language Models via Token Routing},
    author={Li, Zekai and Zheng, Jintu and Liu, Ji and Liu, Han and Zhu, Haowei and Li, Zeping and Yang, Fuwei and Huang, Haiduo and Peng, Jinzhang and Li, Dong and others},
    journal={arXiv preprint arXiv:2412.11494},
    year={2024}
  }
  ```
  </details>  

- **[7] Discrete prompt compression with reinforcement learning**, IEEE Access 2024  
  *Jung, Hoyoun and Kim, Kyung-Joong*  
  [[Paper](https://arxiv.org/pdf/2308.08758)] [[Code](https://github.com/nenomigami/PromptCompressor)] ![PCRL](https://img.shields.io/badge/PCRL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Reinforcement_Learning](https://img.shields.io/badge/Reinforcement_Learning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{jung2024discrete,
    title={Discrete prompt compression with reinforcement learning},
    author={Jung, Hoyoun and Kim, Kyung-Joong},
    journal={IEEE Access},
    year={2024},
    publisher={IEEE}
  }
  ```
  </details>  

- **[8] Hierarchical context merging: Better long context understanding for pre-trained LLMs**, ICLR 2024  
  *Woomin Song and Seunghyuk Oh and Sangwoo Mo and Jaehyung Kim and Sukmin Yun and Jung-Woo Ha and Jinwoo Shin*  
  [[Paper](https://openreview.net/pdf?id=ulaUJFd96G)] [[Code](https://github.com/alinlab/HOMER)] ![HOMER](https://img.shields.io/badge/HOMER-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![QA](https://img.shields.io/badge/QA-green) ![Passkey_Retrieval](https://img.shields.io/badge/Passkey_Retrieval-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  song2024hierarchical,
  title={Hierarchical Context Merging: Better Long Context Understanding for Pre-trained {LLM}s},
  author={Woomin Song and Seunghyuk Oh and Sangwoo Mo and Jaehyung Kim and Sukmin Yun and Jung-Woo Ha and Jinwoo Shin},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2024},
  url={https://openreview.net/forum?id=ulaUJFd96G}
  }
  ```
  </details>  

- **[9] LazyLLM: Dynamic Token Pruning for Efficient Long Context LLM Inference**, arXiv 2024  
  *Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar*  
  [[Paper](https://arxiv.org/pdf/2407.14057)] [[Code](https://github.com/Adam-Mazur/Lazy-Llama)] ![LazyLLM](https://img.shields.io/badge/LazyLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Restore](https://img.shields.io/badge/Restore-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{fu2024lazyllm,
    title={Lazyllm: Dynamic token pruning for efficient long context llm inference},
    author={Fu, Qichen and Cho, Minsik and Merth, Thomas and Mehta, Sachin and Rastegari, Mohammad and Najibi, Mahyar},
    journal={arXiv preprint arXiv:2407.14057},
    year={2024}
  }
  ```
  </details>  

- **[10] You only Caching once: Decoder-decoder architectures for language models**, NeurIPS 2024  
  *Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei*  
  [[Paper](https://openreview.net/pdf?id=25Ioxw576r)] [[Code](https://github.com/microsoft/unilm/tree/master/YOCO)] ![YOCO](https://img.shields.io/badge/YOCO-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Early_Exit](https://img.shields.io/badge/Early_Exit-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Decoder--Decoder](https://img.shields.io/badge/Decoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  sun2024you,
  title={You Only Caching Once: Decoder-Decoder Architectures for Language Models},
  author={Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=25Ioxw576r}
  }
  ```
  </details>  

- **[11] Reducing Transformer Key-Value Caching Size with Cross-Layer Attention**, NeurIPS 2024  
  *William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan-Kelley*  
  [[Paper](https://openreview.net/pdf?id=M2UzLRoqic)] [[Code](https://github.com/JerryYin777/Cross-Layer-Attention)] ![CLA](https://img.shields.io/badge/CLA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  brandon2024reducing,
  title={Reducing Transformer Key-Value Caching Size with Cross-Layer Attention},
  author={William Brandon and Mayank Mishra and Aniruddha Nrusimha and Rameswar Panda and Jonathan Ragan-Kelley},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=M2UzLRoqic}
  }
  ```
  </details>  

- **[12] Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-Caching compression**, arXiv 2024  
  *Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene*  
  [[Paper](https://arxiv.org/pdf/2407.12077)] [[Code](https://github.com/SmerkyG/GoldFinch-paper?tab=readme-ov-file)] ![GoldFinch](https://img.shields.io/badge/GoldFinch-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Early_Exit](https://img.shields.io/badge/Early_Exit-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{goldstein2024goldfinch,
    title={Goldfinch: High performance rwkv/transformer hybrid with linear pre-fill and extreme kv-Caching compression},
    author={Goldstein, Daniel and Obeid, Fares and Alcaide, Eric and Song, Guangyu and Cheah, Eugene},
    journal={arXiv preprint arXiv:2407.12077},
    year={2024}
  }
  ```
  </details>  

- **[13] Long-context language modeling with parallel context encoding**, ACL 2024  
  *Yen, Howard  and Gao, Tianyu  and Chen, Danqi*  
  [[Paper](https://aclanthology.org/2024.acl-long.142.pdf)] [[Code](https://github.com/princeton-nlp/CEPE)] ![CEPE](https://img.shields.io/badge/CEPE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Encoder--Decoder](https://img.shields.io/badge/Encoder--Decoder-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yen-etal-2024-long,
      title = "Long-Context Language Modeling with Parallel Context Encoding",
      author = "Yen, Howard  and
        Gao, Tianyu  and
        Chen, Danqi",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.142/",
      doi = "10.18653/v1/2024.acl-long.142",
      pages = "2588--2610"
  }
  ```
  </details>  

- **[14] Transformers are multi-state rnns**, EMNLP 2024  
  *Oren, Matanel  and Hassid, Michael  and Yarden, Nir  and Adi, Yossi  and Schwartz, Roy*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1043.pdf)] [[Code](https://github.com/schwartz-lab-NLP/TOVA)] ![TOVA](https://img.shields.io/badge/TOVA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Multi--State_RNN](https://img.shields.io/badge/Multi--State_RNN-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{oren-etal-2024-transformers,
      title = "Transformers are Multi-State {RNN}s",
      author = "Oren, Matanel  and
        Hassid, Michael  and
        Yarden, Nir  and
        Adi, Yossi  and
        Schwartz, Roy",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1043/",
      doi = "10.18653/v1/2024.emnlp-main.1043",
      pages = "18724--18741"
  }
  ```
  </details>  

- **[15] PyramidInfer: Pyramid KV Caching Compression for High-throughput LLM Inference**, ACL 2024  
  *Yang, Dongjie  and Han, Xiaodong  and Gao, Yan  and Hu, Yao  and Zhang, Shilin  and Zhao, Hai*  
  [[Paper](https://aclanthology.org/2024.findings-acl.195.pdf)] [[Code](https://github.com/mutonix/pyramidinfer)] ![PyramidInfer](https://img.shields.io/badge/PyramidInfer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yang-etal-2024-pyramidinfer,
      title = "{P}yramid{I}nfer: Pyramid {KV} Caching Compression for High-throughput {LLM} Inference",
      author = "Yang, Dongjie  and
        Han, Xiaodong  and
        Gao, Yan  and
        Hu, Yao  and
        Zhang, Shilin  and
        Zhao, Hai",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-acl.195/",
      doi = "10.18653/v1/2024.findings-acl.195",
      pages = "3258--3270"
  }
  ```
  </details>  

- **[16] Pyramidkv: Dynamic kv Caching compression based on pyramidal information funneling**, arXiv 2024  
  *Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others*  
  [[Paper](https://arxiv.org/pdf/2406.02069)] [[Code](https://github.com/Zefan-Cai/KVCaching-Factory)] ![PyramidKV](https://img.shields.io/badge/PyramidKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{cai2024pyramidkv,
    title={Pyramidkv: Dynamic kv Caching compression based on pyramidal information funneling},
    author={Cai, Zefan and Zhang, Yichi and Gao, Bofei and Liu, Yuliang and Liu, Tianyu and Lu, Keming and Xiong, Wayne and Dong, Yue and Chang, Baobao and Hu, Junjie and others},
    journal={arXiv preprint arXiv:2406.02069},
    year={2024}
  }
  ```
  </details>  

- **[17] Keyformer: Kv Caching reduction through key tokens selection for efficient generative inference**, MLSys 2024  
  *Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham*  
  [[Paper](https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf)] [[Code](https://github.com/d-matrix-ai/keyformer-llm)] ![Keyformer](https://img.shields.io/badge/Keyformer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{MLSYS2024_48fecef4,
   author = {Adnan, Muhammad and Arunkumar, Akhil and Jain, Gaurav and Nair, Prashant and Soloveychik, Ilya and Kamath, Purushotham},
   booktitle = {Proceedings of Machine Learning and Systems},
   editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
   pages = {114--127},
   title = {Keyformer: KV Caching reduction through key tokens selection for Efficient Generative Inference},
   url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/48fecef47b19fe501d27d338b6d52582-Paper-Conference.pdf},
   volume = {6},
   year = {2024}
  }
  ```
  </details>  

- **[18] Finch: Prompt-guided key-value Caching compression**, TACL 2024  
  *Corallo, Giulio and Papotti, Paolo*  
  [[Paper](https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00716/2480391/tacl_a_00716.pdf)] [[Code](https://github.com/giulio98/context-compression/)] ![FINCH](https://img.shields.io/badge/FINCH-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{10.1162/tacl_a_00716,
      author = {Corallo, Giulio and Papotti, Paolo},
      title = {FINCH: Prompt-guided Key-Value Caching Compression for Large Language Models},
      journal = {Transactions of the Association for Computational Linguistics},
      volume = {12},
      pages = {1517-1532},
      year = {2024},
      month = {11},
      issn = {2307-387X},
      doi = {10.1162/tacl_a_00716},
      url = {https://doi.org/10.1162/tacl\_a\_00716},
      eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00716/2480391/tacl\_a\_00716.pdf},
  }
  
  
  ```
  </details>  

- **[19] A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder**, arXiv 2024  
  *Jo, Hyun-rae and Shin, Dongkun*  
  [[Paper](https://arxiv.org/pdf/2407.20485#page=1.45)] [[Code](https://github.com/Dirac-Notation/A2SF)] ![A2sf](https://img.shields.io/badge/A2sf-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{jo2024a2sf,
    title={A2sf: Accumulative attention scoring with forgetting factor for token pruning in transformer decoder},
    author={Jo, Hyun-rae and Shin, Dongkun},
    journal={arXiv preprint arXiv:2407.20485},
    year={2024}
  }
  ```
  </details>  

- **[20] Razorattention: Efficient kv Caching compression through retrieval heads**, arXiv 2024  
  *Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi*  
  [[Paper](https://arxiv.org/pdf/2407.15891)] ![RazorAttention](https://img.shields.io/badge/RazorAttention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Pooling](https://img.shields.io/badge/Pooling-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Compensation_Token](https://img.shields.io/badge/Compensation_Token-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{tang2024razorattention,
    title={Razorattention: Efficient kv Caching compression through retrieval heads},
    author={Tang, Hanlin and Lin, Yang and Lin, Jing and Han, Qingsen and Hong, Shikuan and Yao, Yiwu and Wang, Gongyi},
    journal={arXiv preprint arXiv:2407.15891},
    year={2024}
  }
  ```
  </details>  

- **[21] SirLLM: Streaming infinite retentive LLM**, ACL 2024  
  *Yao, Yao  and Li, Zuchao  and  Zhao, Hai*  
  [[Paper](https://aclanthology.org/2024.acl-long.143.pdf)] [[Code](https://github.com/Zoeyyao27/SirLLM)] ![SirLLM](https://img.shields.io/badge/SirLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Token_Entropy](https://img.shields.io/badge/Token_Entropy-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Token_Entropy](https://img.shields.io/badge/Token_Entropy-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yao-etal-2024-sirllm,
      title = "{S}ir{LLM}: Streaming Infinite Retentive {LLM}",
      author = "Yao, Yao  and
        Li, Zuchao  and
        Zhao, Hai",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.143/",
      doi = "10.18653/v1/2024.acl-long.143",
      pages = "2611--2624"
  }
  ```
  </details>  

- **[22] A Simple and Effective $ L_2 $ Norm-Based Strategy for KV Caching Compression**, EMNLP 2024  
  *Devoto, Alessio  and  Zhao, Yu  and   Scardapane, Simone  and Minervini, Pasquale*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1027.pdf)] [[Code](https://aclanthology.org/attachments/2024.emnlp-main.1027.software.zip)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![L2_Norm](https://img.shields.io/badge/L2_Norm-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![L2--norm--based](https://img.shields.io/badge/L2--norm--based-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{devoto-etal-2024-simple,
      title = "A Simple and Effective $L\_2$ Norm-Based Strategy for {KV} Caching Compression",
      author = "Devoto, Alessio  and
        Zhao, Yu  and
        Scardapane, Simone  and
        Minervini, Pasquale",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1027/",
      doi = "10.18653/v1/2024.emnlp-main.1027",
      pages = "18476--18499"
  }
  ```
  </details>  

- **[23] Dynamic memory compression: Retrofitting llms for accelerated inference**, arXiv 2024  
  *Nawrot, Piotr and La'ncucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M*  
  [[Paper](https://arxiv.org/pdf/2403.09636)] ![DMC](https://img.shields.io/badge/DMC-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{nawrot2024dynamic,
    title={Dynamic memory compression: Retrofitting llms for accelerated inference},
    author={Nawrot, Piotr and {\L}a{\'n}cucki, Adrian and Chochowski, Marcin and Tarjan, David and Ponti, Edoardo M},
    journal={arXiv preprint arXiv:2403.09636},
    year={2024}
  }
  ```
  </details>  

- **[24] Model tells you where to merge: Adaptive kv Caching merging for llms on long-context tasks**, arXiv 2024  
  *Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia*  
  [[Paper](https://arxiv.org/pdf/2407.08454#page=1.71)] ![KVMerger](https://img.shields.io/badge/KVMerger-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Similarity--based](https://img.shields.io/badge/Similarity--based-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Gaussian_Kernel_weighted_merging](https://img.shields.io/badge/Gaussian_Kernel_weighted_merging-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wang2024model,
    title={Model tells you where to merge: Adaptive kv Caching merging for llms on long-context tasks},
    author={Wang, Zheng and Jin, Boxiao and Yu, Zhongzhi and Zhang, Minjia},
    journal={arXiv preprint arXiv:2407.08454},
    year={2024}
  }
  ```
  </details>  

- **[25] Anchor-based large language models**, ACL 2024  
  *Pang, Jianhui  and Ye, Fanghua  and Wong, Derek  and  He, Xin  and  Chen, Wanshun  and  Wang, Longyue*  
  [[Paper](https://aclanthology.org/2024.findings-acl.295.pdf)] [[Code](https://github.com/pangjh3/AnLLM)] ![AnLLM](https://img.shields.io/badge/AnLLM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Anchor_Token](https://img.shields.io/badge/Anchor_Token-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{pang-etal-2024-anchor,
      title = "Anchor-based Large Language Models",
      author = "Pang, Jianhui  and
        Ye, Fanghua  and
        Wong, Derek  and
        He, Xin  and
        Chen, Wanshun  and
        Wang, Longyue",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-acl.295/",
      doi = "10.18653/v1/2024.findings-acl.295",
      pages = "4958--4976"
  }
  ```
  </details>  

- **[26] Keep the Cost Down: A Review on Methods to Optimize LLM' s KV-Caching Consumption**, COLM 2024  
  *Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao*  
  [[Paper](https://openreview.net/pdf?id=8tKjqqMM5z)] [[Code](https://github.com/zcli-charlie/Awesome-KVCaching)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Survey](https://img.shields.io/badge/Survey-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  luohe2024keep,
  title={Keep the Cost Down: A Review on Methods to Optimize {LLM}{\textquoteright}s {KV}-Caching Consumption},
  author={Shi Luohe and Hongyi Zhang and Yao Yao and Zuchao Li and hai zhao},
  booktitle={First Conference on Language Modeling},
  year={2024},
  url={https://openreview.net/forum?id=8tKjqqMM5z}
  }
  ```
  </details>  

- **[27] SCBench: A KV Caching-Centric Analysis of Long-Context Methods**, arXiv 2024  
  *Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others*  
  [[Paper](https://arxiv.org/pdf/2412.10319)] [[Code](https://aka.ms/SCBench)] ![SCBench](https://img.shields.io/badge/SCBench-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Benchmark](https://img.shields.io/badge/Benchmark-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024scbench,
    title={Scbench: A kv Caching-centric analysis of long-context methods},
    author={Li, Yucheng and Jiang, Huiqiang and Wu, Qianhui and Luo, Xufang and Ahn, Surin and Zhang, Chengruidong and Abdi, Amir H and Li, Dongsheng and Gao, Jianfeng and Yang, Yuqing and others},
    journal={arXiv preprint arXiv:2412.10319},
    year={2024}
  }
  ```
  </details>  

- **[28] On the efficacy of eviction policy for key-value constrained generative language model inference**, arXiv 2024  
  *Ren, Siyu and Zhu, Kenny Q*  
  [[Paper](https://arxiv.org/pdf/2402.06262)] [[Code](https://github.com/DRSY/EasyKV)] ![EasyKV](https://img.shields.io/badge/EasyKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Eviction_Scope](https://img.shields.io/badge/Eviction_Scope-yellow) ![Importance_Score](https://img.shields.io/badge/Importance_Score-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{ren2024efficacy,
    title={On the efficacy of eviction policy for key-value constrained generative language model inference},
    author={Ren, Siyu and Zhu, Kenny Q},
    journal={arXiv preprint arXiv:2402.06262},
    year={2024}
  }
  ```
  </details>  

- **[29] Snapkv: Llm knows what you are looking for before generation**, NeurIPS 2024  
  *Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen*  
  [[Paper](https://openreview.net/pdf?id=poE54GOq2l)] [[Code](https://github.com/FasterDecoding/SnapKV)] ![SnapKV](https://img.shields.io/badge/SnapKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  li2024snapkv,
  title={Snap{KV}: {LLM} Knows What You are Looking for Before Generation},
  author={Yuhong Li and Yingbing Huang and Bowen Yang and Bharat Venkitesh and Acyr Locatelli and Hanchen Ye and Tianle Cai and Patrick Lewis and Deming Chen},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=poE54GOq2l}
  }
  ```
  </details>  

- **[30] SubGen: Token Generation in Sublinear Time and Memory**, arXiv 2024  
  *Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin*  
  [[Paper](https://arxiv.org/pdf/2402.06082)] ![SubGen](https://img.shields.io/badge/SubGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Similarity--based](https://img.shields.io/badge/Similarity--based-purple) ![Clustering](https://img.shields.io/badge/Clustering-orange) ![Purning](https://img.shields.io/badge/Purning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zandieh2024subgen,
    title={SubGen: Token Generation in Sublinear Time and Memory},
    author={Zandieh, Amir and Han, Insu and Mirrokni, Vahab and Karbasi, Amin},
    journal={arXiv preprint arXiv:2402.06082},
    year={2024}
  }
  ```
  </details>  

- **[31] Not All Heads Matter: A Head-Level KV Caching Compression Method with Integrated Retrieval and Reasoning**, arXiv 2024  
  *Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen*  
  [[Paper](https://arxiv.org/pdf/2410.19258)] [[Code](https://github.com/FYYFU/HeadKV)] ![HeadKV](https://img.shields.io/badge/HeadKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Retrieval_and_Reasoning_Importance](https://img.shields.io/badge/Retrieval_and_Reasoning_Importance-yellow) ![_Head--Level](https://img.shields.io/badge/_Head--Level-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{fu2024not,
    title={Not all heads matter: A head-level KV Caching compression method with integrated retrieval and reasoning},
    author={Fu, Yu and Cai, Zefan and Asi, Abedelkadir and Xiong, Wayne and Dong, Yue and Xiao, Wen},
    journal={arXiv preprint arXiv:2410.19258},
    year={2024}
  }
  ```
  </details>  

- **[32] CORM: Caching Optimization with Recent Message for Large Language Model Inference**, arXiv 2024  
  *Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi*  
  [[Paper](https://arxiv.org/pdf/2404.15949v2)] ![CORM](https://img.shields.io/badge/CORM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Query_Vector_Similarity--based](https://img.shields.io/badge/Query_Vector_Similarity--based-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @misc{dai2024corm,
      title={CORM: Caching Optimization with Recent Message for Large Language Model Inference},
      author={Jincheng Dai and Zhuowei Huang and Haiyun Jiang and Chen Chen and Deng Cai and Wei Bi and Shuming Shi},
      year={2024},
      eprint={2404.15949},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
  }
  ```
  </details>  

- **[33] Attention Score is not All You Need for Token Importance Indicator in KV Caching Reduction: Value Also Matters**, EMNLP 2024  
  *Guo, Zhiyu  and Kamigaito, Hidetaka  and  Watanabe, Taro*  
  [[Paper](https://aclanthology.org/2024.emnlp-main.1178.pdf)] [[Code](https://github.com/guozhiyu/vatp)] ![VATP](https://img.shields.io/badge/VATP-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![L1_Norm](https://img.shields.io/badge/L1_Norm-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Value--aware_Token_Pruning](https://img.shields.io/badge/Value--aware_Token_Pruning-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{guo-etal-2024-attention,
      title = "Attention Score is not All You Need for Token Importance Indicator in {KV} Caching Reduction: Value Also Matters",
      author = "Guo, Zhiyu  and
        Kamigaito, Hidetaka  and
        Watanabe, Taro",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.emnlp-main.1178/",
      doi = "10.18653/v1/2024.emnlp-main.1178",
      pages = "21158--21166"
  }
  ```
  </details>  

- **[34] MiniCaching: KV Caching Compression in Depth Dimension for Large Language Models**, NeurIPS 2024  
  *Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang*  
  [[Paper](https://openreview.net/pdf?id=sgVOjDqUMT)] [[Code](https://miniCaching.vmv.re/)] ![MiniCaching](https://img.shields.io/badge/MiniCaching-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![L2_Norm](https://img.shields.io/badge/L2_Norm-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Aaveraging](https://img.shields.io/badge/Aaveraging-orange) ![Retention](https://img.shields.io/badge/Retention-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  liu2024miniCaching,
  title={MiniCaching: {KV} Caching Compression in Depth Dimension for Large Language Models},
  author={Akide Liu and Jing Liu and Zizheng Pan and Yefei He and Gholamreza Haffari and Bohan Zhuang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=sgVOjDqUMT}
  }
  ```
  </details>  

- **[35] Cam: Caching merging for memory-efficient llms inference**, ICML 2024  
  *Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji*  
  [[Paper](https://openreview.net/pdf?id=LCTmppB165)] [[Code](https://github.com/zyxxmu/cam)] ![CaM](https://img.shields.io/badge/CaM-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  zhang2024cam,
  title={CaM: Caching Merging for Memory-efficient {LLM}s Inference},
  author={Yuxin Zhang and Yuxuan Du and Gen Luo and Yunshan Zhong and Zhenyu Zhang and Shiwei Liu and Rongrong Ji},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=LCTmppB165}
  }
  ```
  </details>  

- **[36] D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models**, arXiv 2024  
  *Wan, Zhongwei and Wu, Xinjian and Zhang, Yu and Xin, Yi and Tao, Chaofan and Zhu, Zhihong and Wang, Xin and Luo, Siqi and Xiong, Jing and Zhang, Mi*  
  [[Paper](https://arxiv.org/pdf/2406.13035)] ![D2O](https://img.shields.io/badge/D2O-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wan2024d2o,
    title={D2O: Dynamic Discriminative Operations for Efficient Generative Inference of Large Language Models},
    author={Wan, Zhongwei and Wu, Xinjian and Zhang, Yu and Xin, Yi and Tao, Chaofan and Zhu, Zhihong and Wang, Xin and Luo, Siqi and Xiong, Jing and Zhang, Mi},
    journal={arXiv preprint arXiv:2406.13035},
    year={2024}
  }
  ```
  </details>  

- **[37] Unveiling and harnessing hidden attention sinks: Enhancing large language models without training through attention calibration**, ICML 2024  
  *Yu, Zhongzhi and Wang, Zheng and Fu, Yonggan and Shi, Huihong and Shaikh, Khalid and Lin, Yingyan (Celine)*  
  [[Paper](https://raw.githubusercontent.com/mlresearch/v235/main/assets/yu24l/yu24l.pdf)] [[Code](https://github.com/GATECH-EIC/ACT)] ![ACT](https://img.shields.io/badge/ACT-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention_Sink](https://img.shields.io/badge/Attention_Sink-yellow) ![Analysis](https://img.shields.io/badge/Analysis-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{10.5555/3692070.3694448,
  author = {Yu, Zhongzhi and Wang, Zheng and Fu, Yonggan and Shi, Huihong and Shaikh, Khalid and Lin, Yingyan (Celine)},
  title = {Unveiling and harnessing hidden attention sinks: enhancing large language models without training through attention calibration},
  year = {2025},
  publisher = {JMLR.org},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  articleno = {2378},
  numpages = {19},
  location = {Vienna, Austria},
  series = {ICML'24}
  }
  ```
  </details>  

- **[38] Sglang: Efficient execution of structured language model programs**, NeurIPS 2024  
  *Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng*  
  [[Paper](https://openreview.net/pdf?id=VqkAKQibpq)] [[Code](https://github.com/sgl-project/sglang)] ![SGLang](https://img.shields.io/badge/SGLang-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Reusing](https://img.shields.io/badge/Reusing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  zheng2024sglang,
  title={{SGL}ang: Efficient Execution of Structured Language Model Programs},
  author={Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=VqkAKQibpq}
  }
  ```
  </details>  

- **[39] Prompt Caching: Modular attention reuse for low-latency inference**, MLSys 2024  
  *Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin*  
  [[Paper](https://proceedings.mlsys.org/paper_files/paper/2024/file/a66caa1703fe34705a4368c3014c1966-Paper-Conference.pdf)] [[Code](https://github.com/MachineLearningSystem/24MLSYS-prompt-Caching)] ![Prompt_Caching](https://img.shields.io/badge/Prompt_Caching-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Reusing](https://img.shields.io/badge/Reusing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{MLSYS2024_a66caa17,
   author = {Gim, In and Chen, Guojun and Lee, Seung-seob and Sarda, Nikhil and Khandelwal, Anurag and Zhong, Lin},
   booktitle = {Proceedings of Machine Learning and Systems},
   editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
   pages = {325--338},
   title = {Prompt Caching: Modular Attention Reuse for Low-Latency Inference},
   url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/a66caa1703fe34705a4368c3014c1966-Paper-Conference.pdf},
   volume = {6},
   year = {2024}
  }
  ```
  </details>  

- **[40] Hydragen: High-Throughput LLM Inference with Shared Prefixes**, ES-FoMo-II  2024  
  *Jordan Juravsky and Bradley Brown and Ryan Saul Ehrlich and Daniel Y Fu and Christopher Re and Azalia Mirhoseini*  
  [[Paper](https://openreview.net/pdf?id=Ye3ia34Fpp)] [[Code](https://github.com/ScalingIntelligence/hydragen)] ![Hydragen](https://img.shields.io/badge/Hydragen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Reusing](https://img.shields.io/badge/Reusing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  juravsky2024hydragen,
  title={Hydragen: High-Throughput {LLM} Inference with Shared Prefixes},
  author={Jordan Juravsky and Bradley Brown and Ryan Saul Ehrlich and Daniel Y Fu and Christopher Re and Azalia Mirhoseini},
  booktitle={Workshop on Efficient Systems for Foundation Models II @ ICML2024},
  year={2024},
  url={https://openreview.net/forum?id=Ye3ia34Fpp}
  }
  ```
  </details>  

- **[41] RAGCaching: Efficient Knowledge Caching for Retrieval-Augmented Generation**, arXiv 2024  
  *Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin*  
  [[Paper](https://arxiv.org/pdf/2404.12457)] ![RAGCaching](https://img.shields.io/badge/RAGCaching-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow) ![RAG](https://img.shields.io/badge/RAG-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{jin2024ragCaching,
    title={RAGCaching: Efficient Knowledge Caching for Retrieval-Augmented Generation},
    author={Jin, Chao and Zhang, Zili and Jiang, Xuanlin and Liu, Fangyue and Liu, Xin and Liu, Xuanzhe and Jin, Xin},
    journal={arXiv preprint arXiv:2404.12457},
    year={2024}
  }
  ```
  </details>  

- **[42] CachingBlend: Fast Large Language Model Serving with Cachingd Knowledge Fusion**, arXiv 2024  
  *Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen*  
  [[Paper](https://arxiv.org/pdf/2405.16444)] [[Code](https://github.com/YaoJiayi/CachingBlend)] ![CachingBlend](https://img.shields.io/badge/CachingBlend-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Reusing](https://img.shields.io/badge/Reusing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow) ![RAG](https://img.shields.io/badge/RAG-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{yao2024Cachingblend,
    title={CachingBlend: Fast Large Language Model Serving with Cachingd Knowledge Fusion},
    author={Yao, Jiayi and Li, Hanchen and Liu, Yuhan and Ray, Siddhant and Cheng, Yihua and Zhang, Qizheng and Du, Kuntai and Lu, Shan and Jiang, Junchen},
    journal={arXiv preprint arXiv:2405.16444},
    year={2024}
  }
  ```
  </details>  

- **[43] Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference**, arXiv 2024  
  *Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song*  
  [[Paper](https://arxiv.org/pdf/2406.10774)] [[Code](https://github.com/mit-han-lab/Quest)] ![Quest](https://img.shields.io/badge/Quest-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![PageAttention](https://img.shields.io/badge/PageAttention-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{tang2024quest,
    title={Quest: Query-Aware Sparsity for Efficient Long-Context LLM Inference},
    author={Tang, Jiaming and Zhao, Yilong and Zhu, Kan and Xiao, Guangxuan and Kasikci, Baris and Han, Song},
    journal={arXiv preprint arXiv:2406.10774},
    year={2024}
  }
  ```
  </details>  

- **[44] Ada-kv: Optimizing kv Caching eviction by adaptive budget allocation for efficient llm inference**, arXiv 2024  
  *Feng, Yuan and Lv, Junlin and Cao, Yukun and Xie, Xike and Zhou, S Kevin*  
  [[Paper](https://arxiv.org/pdf/2407.11550)] [[Code](https://github.com/FFY0/AdaKV)] ![Ada--kv](https://img.shields.io/badge/Ada--kv-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{feng2024ada,
    title={Ada-kv: Optimizing kv Caching eviction by adaptive budget allocation for efficient llm inference},
    author={Feng, Yuan and Lv, Junlin and Cao, Yukun and Xie, Xike and Zhou, S Kevin},
    journal={arXiv preprint arXiv:2407.11550},
    year={2024}
  }
  ```
  </details>  

- **[45] Massive activations in large language models**, COLM 2024  
  *Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang*  
  [[Paper](https://openreview.net/pdf?id=F7aAhfitX6)] [[Code](https://github.com/locuslab/massive-activations)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Massive_Activation](https://img.shields.io/badge/Massive_Activation-yellow) ![Analysis](https://img.shields.io/badge/Analysis-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{sun2024massive,
    title={Massive activations in large language models},
    author={Sun, Mingjie and Chen, Xinlei and Kolter, J Zico and Liu, Zhuang},
    journal={arXiv preprint arXiv:2402.17762},
    year={2024}
  }
  ```
  </details>  

- **[46] Unifying KV Caching Compression for Large Language Models with LeanKV**, arXiv 2024  
  *Zhang, Yanqi and Hu, Yuwei and Zhao, Runyuan and Lui, John and Chen, Haibo*  
  [[Paper](https://arxiv.org/pdf/2412.03131)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhang2024unifying,
    title={Unifying kv Caching compression for large language models with leankv},
    author={Zhang, Yanqi and Hu, Yuwei and Zhao, Runyuan and Lui, John and Chen, Haibo},
    journal={arXiv preprint arXiv:2412.03131},
    year={2024}
  }
  ```
  </details>  

- **[47] More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Caching Compression**, arXiv 2024  
  *Zhang, Jiebin and Zhu, Dawei and Song, Yifan and Wu, Wenhao and Kuang, Chuqiao and Li, Xiaoguang and Shang, Lifeng and Liu, Qun and Li, Sujian*  
  [[Paper](https://arxiv.org/pdf/2412.12706)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Quantization](https://img.shields.io/badge/Quantization-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhang2024more,
    title={More Tokens, Lower Precision: Towards the Optimal Token-Precision Trade-off in KV Caching Compression},
    author={Zhang, Jiebin and Zhu, Dawei and Song, Yifan and Wu, Wenhao and Kuang, Chuqiao and Li, Xiaoguang and Shang, Lifeng and Liu, Qun and Li, Sujian},
    journal={arXiv preprint arXiv:2412.12706},
    year={2024}
  }
  ```
  </details>  

- **[48] LayerKV: Optimizing Large Language Model Serving with Layer-wise KV Caching Management**, arXiv 2024  
  *Xiong, Yi and Wu, Hao and Shao, Changxu and Wang, Ziqing and Zhang, Rui and Guo, Yuhong and Zhao, Junping and Zhang, Ke and Pan, Zhenxuan*  
  [[Paper](https://arxiv.org/pdf/2410.00428)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Offloading](https://img.shields.io/badge/Offloading-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{xiong2024layerkv,
    title={Layerkv: Optimizing large language model serving with layer-wise kv Caching management},
    author={Xiong, Yi and Wu, Hao and Shao, Changxu and Wang, Ziqing and Zhang, Rui and Guo, Yuhong and Zhao, Junping and Zhang, Ke and Pan, Zhenxuan},
    journal={arXiv preprint arXiv:2410.00428},
    year={2024}
  }
  ```
  </details>  

- **[49] KV-Compress: Paged KV-Caching Compression with Variable Compression Rates per Attention Head**, arXiv 2024  
  *Isaac Rehg*  
  [[Paper](https://arxiv.org/pdf/2410.00161)] [[Code](https://github.com/IsaacRe/vllm-kvcompress)] ![KV--Compress](https://img.shields.io/badge/KV--Compress-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{rehg2024kv,
    title={KV-Compress: Paged KV-Caching Compression with Variable Compression Rates per Attention Head},
    author={Rehg, Isaac},
    journal={arXiv preprint arXiv:2410.00161},
    year={2024}
  }
  ```
  </details>  

- **[50] ZigZagkv: Dynamic KV Caching Compression for Long-context Modeling based on Layer Uncertainty**, arXiv 2024  
  *Zhong, Meizhi and Liu, Xikai and Zhang, Chen and Lei, Yikun and Gao, Yan and Hu, Yao and Chen, Kehai and Zhang, Min*  
  [[Paper](https://arxiv.org/pdf/2412.09036)] ![ZigZagKV](https://img.shields.io/badge/ZigZagKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Layer_Uncertainty](https://img.shields.io/badge/Layer_Uncertainty-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Layer_Uncertainty](https://img.shields.io/badge/Layer_Uncertainty-yellow) ![Dynamic_Budget_Allocation_](https://img.shields.io/badge/Dynamic_Budget_Allocation_-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhong2024zigzagkv,
    title={ZigZagkv: Dynamic KV Caching Compression for Long-context Modeling based on Layer Uncertainty},
    author={Zhong, Meizhi and Liu, Xikai and Zhang, Chen and Lei, Yikun and Gao, Yan and Hu, Yao and Chen, Kehai and Zhang, Min},
    journal={arXiv preprint arXiv:2412.09036},
    year={2024}
  }
  ```
  </details>  

- **[51] UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference**, arXiv 2024  
  *Xiong, Jing and Shen, Jianghan and Ye, Fanghua and Tao, Chaofan and Wan, Zhongwei and Lu, Jianqiao and Wu, Xun and Zheng, Chuanyang and Guo, Zhijiang and Kong, Lingpeng and others*  
  [[Paper](https://arxiv.org/pdf/2410.03090)] ![UNComp](https://img.shields.io/badge/UNComp-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Matrix_Entropy](https://img.shields.io/badge/Matrix_Entropy-purple) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{xiong2024uncomp,
    title={UNComp: Uncertainty-Aware Long-Context Compressor for Efficient Large Language Model Inference},
    author={Xiong, Jing and Shen, Jianghan and Ye, Fanghua and Tao, Chaofan and Wan, Zhongwei and Lu, Jianqiao and Wu, Xun and Zheng, Chuanyang and Guo, Zhijiang and Kong, Lingpeng and others},
    journal={arXiv preprint arXiv:2410.03090},
    year={2024}
  }
  ```
  </details>  

- **[52] KVSharer: Efficient Inference via Layer-Wise Dissimilar KV Caching Sharing**, arXiv 2024  
  *Yang, Yifei and Cao, Zouying and Chen, Qiguang and Qin, Libo and Yang, Dongjie and Zhao, Hai and Chen, Zhi*  
  [[Paper](https://arxiv.org/pdf/2410.18517)] ![KVSharer](https://img.shields.io/badge/KVSharer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Similarity--based](https://img.shields.io/badge/Similarity--based-purple) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{yang2024kvsharer,
    title={Kvsharer: Efficient inference via layer-wise dissimilar KV Caching sharing},
    author={Yang, Yifei and Cao, Zouying and Chen, Qiguang and Qin, Libo and Yang, Dongjie and Zhao, Hai and Chen, Zhi},
    journal={arXiv preprint arXiv:2410.18517},
    year={2024}
  }
  ```
  </details>  

- **[53] LoCoCo: Pruning In Convolutions for Long Context Compression**, ICML 2024  
  *Cai, Ruisi and Tian, Yuandong and Wang, Zhangyang and Chen, Beidi*  
  [[Paper](https://arxiv.org/pdf/2406.05317)] [[Code](https://github.com/VITA-Group/LoCoCo)] ![LoCoCo](https://img.shields.io/badge/LoCoCo-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Conv](https://img.shields.io/badge/Conv-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  cai2024lococo,
  title={LoCoCo: Pruning In Convolutions for Long Context Compression},
  author={Ruisi Cai and Yuandong Tian and Zhangyang Wang and Beidi Chen},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://openreview.net/forum?id=NUlyqMyhO9}
  }
  ```
  </details>  

- **[54] SimLayerKV: A Simple Framework for Layer-Level KV Caching Reduction**, arXiv 2024  
  *Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min*  
  [[Paper](https://arxiv.org/pdf/2410.13846)] [[Code](https://github.com/sail-sg/SimLayerKV)] ![Simlayerkv](https://img.shields.io/badge/Simlayerkv-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zhang2024simlayerkv,
    title={Simlayerkv: A simple framework for layer-level KV Caching reduction},
    author={Zhang, Xuan and Du, Cunxiao and Du, Chao and Pang, Tianyu and Gao, Wei and Lin, Min},
    journal={arXiv preprint arXiv:2410.13846},
    year={2024}
  }
  ```
  </details>  

- **[55] EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Caching Compression Based on Global-Local Importance**, arXiv 2024  
  *Li, Yingxin and Li, Ye and Meng, Yuan and Ma, Xinzhu and Geng, Zihan and Xia, Shutao and Wang, Zhi*  
  [[Paper](https://arxiv.org/pdf/2412.08521)] ![EMS](https://img.shields.io/badge/EMS-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Global--Local_score](https://img.shields.io/badge/Global--Local_score-yellow) ![Evict--Then--Merge](https://img.shields.io/badge/Evict--Then--Merge-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024ems,
    title={EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Caching Compression Based on Global-Local Importance},
    author={Li, Yingxin and Li, Ye and Meng, Yuan and Ma, Xinzhu and Geng, Zihan and Xia, Shutao and Wang, Zhi},
    journal={arXiv preprint arXiv:2412.08521},
    year={2024}
  }
  ```
  </details>  

- **[56] ClusterKV: Manipulating LLM KV Caching in Semantic Space for Recallable Compression**, arXiv 2024  
  *Liu, Guangda and Li, Chengwei and Zhao, Jieru and Zhang, Chenqi and Guo, Minyi*  
  [[Paper](https://arxiv.org/pdf/2412.03213)] ![ClusterKV](https://img.shields.io/badge/ClusterKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Similarity--based](https://img.shields.io/badge/Similarity--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{liu2024clusterkv,
    title={ClusterKV: Manipulating LLM KV Caching in Semantic Space for Recallable Compression},
    author={Liu, Guangda and Li, Chengwei and Zhao, Jieru and Zhang, Chenqi and Guo, Minyi},
    journal={arXiv preprint arXiv:2412.03213},
    year={2024}
  }
  ```
  </details>  

- **[57] Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads**, arXiv 2024  
  *Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan*  
  [[Paper](https://arxiv.org/pdf/2410.01805)] [[Code](https://github.com/huangyuxiang03/Locret)] ![Locret](https://img.shields.io/badge/Locret-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown) ![Retaining_Head](https://img.shields.io/badge/Retaining_Head-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{huang2024locret,
    title={Locret: Enhancing Eviction in Long-Context LLM Inference with Trained Retaining Heads},
    author={Huang, Yuxiang and Yuan, Binhang and Han, Xu and Xiao, Chaojun and Liu, Zhiyuan},
    journal={arXiv preprint arXiv:2410.01805},
    year={2024}
  }
  ```
  </details>  

- **[58] No Token Left Behind: Reliable KV Caching Compression via Importance-Aware Mixed Precision Quantization**, arXiv 2024  
  *Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo*  
  [[Paper](https://arxiv.org/pdf/2402.18096)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Quantization](https://img.shields.io/badge/Quantization-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{yang2024no,
    title={No token left behind: Reliable kv Caching compression via importance-aware mixed precision quantization},
    author={Yang, June Yong and Kim, Byeongwook and Bae, Jeongin and Kwon, Beomseok and Park, Gunho and Yang, Eunho and Kwon, Se Jung and Lee, Dongsoo},
    journal={arXiv preprint arXiv:2402.18096},
    year={2024}
  }
  ```
  </details>  

- **[59] LoMA: Lossless Compressed Memory Attention**, arXiv 2024  
  *Wang, Yumeng and Xiao, Zhenyang*  
  [[Paper](https://arxiv.org/pdf/2401.09486)] ![LoMA](https://img.shields.io/badge/LoMA-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Learnable_Predictor](https://img.shields.io/badge/Learnable_Predictor-purple) ![Learnable_Mechanism](https://img.shields.io/badge/Learnable_Mechanism-purple) ![Soft_Token](https://img.shields.io/badge/Soft_Token-orange) ![Training--based](https://img.shields.io/badge/Training--based-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wang2024loma,
    title={LoMA: Lossless Compressed Memory Attention},
    author={Wang, Yumeng and Xiao, Zhenyang},
    journal={arXiv preprint arXiv:2401.09486},
    year={2024}
  }
  ```
  </details>  

- **[60] KV Caching Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches**, EMNLP 2024  
  *Yuan, Jiayi  and  Liu, Hongyi  and  Zhong, Shaochen  and  Chuang, Yu-Neng  and Li, Songchen  and  Wang, Guanchu  and  Le, Duy  and  Jin, Hongye  and  Chaudhary, Vipin  and  Xu, Zhaozhuo  and  Liu, Zirui  and  Hu, Xia*  
  [[Paper](https://aclanthology.org/2024.findings-emnlp.266.pdf)] [[Code](https://github.com/henryzhongsc/longctx_bench)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Benchmark](https://img.shields.io/badge/Benchmark-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{yuan-etal-2024-kv,
      title = "{KV} Caching Compression, But What Must We Give in Return? A Comprehensive Benchmark of Long Context Capable Approaches",
      author = "Yuan, Jiayi  and
        Liu, Hongyi  and
        Zhong, Shaochen  and
        Chuang, Yu-Neng  and
        Li, Songchen  and
        Wang, Guanchu  and
        Le, Duy  and
        Jin, Hongye  and
        Chaudhary, Vipin  and
        Xu, Zhaozhuo  and
        Liu, Zirui  and
        Hu, Xia",
      editor = "Al-Onaizan, Yaser  and
        Bansal, Mohit  and
        Chen, Yun-Nung",
      booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
      month = nov,
      year = "2024",
      address = "Miami, Florida, USA",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-emnlp.266/",
      doi = "10.18653/v1/2024.findings-emnlp.266",
      pages = "4623--4648"
  }
  ```
  </details>  

- **[61] NACL: A General and Effective KV Caching Eviction Framework for LLMs at Inference Time**, ACL 2024  
  *Chen, Yilong  and  Wang, Guoxia  and Shang, Junyuan  and   Cui, Shiyao  and Zhang, Zhenyu  and Liu, Tingwen  an Wang, Shuohuan  and  Sun, Yu  and  Yu, Dianhai  and  Wu, Hua*  
  [[Paper](https://aclanthology.org/2024.acl-long.428.pdf)] [[Code](https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL)] ![NACL](https://img.shields.io/badge/NACL-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{chen-etal-2024-nacl,
      title = "{NACL}: A General and Effective {KV} Caching Eviction Framework for {LLM} at Inference Time",
      author = "Chen, Yilong  and
        Wang, Guoxia  and
        Shang, Junyuan  and
        Cui, Shiyao  and
        Zhang, Zhenyu  and
        Liu, Tingwen  and
        Wang, Shuohuan  and
        Sun, Yu  and
        Yu, Dianhai  and
        Wu, Hua",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.acl-long.428/",
      doi = "10.18653/v1/2024.acl-long.428",
      pages = "7913--7926"
  }
  ```
  </details>  

- **[62] TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Caching Selection**, arXiv 2024  
  *Wu, Wei and Pan, Zhuoshi and Wang, Chao and Chen, Liyi and Bai, Yunchu and Fu, Kun and Wang, Zheng and Xiong, Hui*  
  [[Paper](https://arxiv.org/pdf/2411.02886)] ![TokenSelect](https://img.shields.io/badge/TokenSelect-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wu2024tokenselect,
    title={TokenSelect: Efficient Long-Context Inference and Length Extrapolation for LLMs via Dynamic Token-Level KV Caching Selection},
    author={Wu, Wei and Pan, Zhuoshi and Wang, Chao and Chen, Liyi and Bai, Yunchu and Fu, Kun and Wang, Zheng and Xiong, Hui},
    journal={arXiv preprint arXiv:2411.02886},
    year={2024}
  }
  ```
  </details>  

- **[63] ArkVale: Efficient Generative LLM Inference with Recallable Key-Value Eviction**, NeurIPS 2024  
  *Renze Chen and Zhuofeng Wang and Beiquan Cao and Tong Wu and Size Zheng and Xiuhong Li and Xuechao Wei and Shengen Yan and Meng Li and Yun Liang*  
  [[Paper](https://openreview.net/pdf?id=4oAt5L4lYe)] [[Code](https://github.com/pku-liang/ArkVale)] ![ArkVale](https://img.shields.io/badge/ArkVale-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow) ![KV--page](https://img.shields.io/badge/KV--page-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{
  chen2024arkvale,
  title={ArkVale: Efficient Generative {LLM} Inference with Recallable Key-Value Eviction},
  author={Renze Chen and Zhuofeng Wang and Beiquan Cao and Tong Wu and Size Zheng and Xiuhong Li and Xuechao Wei and Shengen Yan and Meng Li and Yun Liang},
  booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
  year={2024},
  url={https://openreview.net/forum?id=4oAt5L4lYe}
  }
  ```
  </details>  

- **[64] Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Caching**, MLSys 2024  
  *Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas*  
  [[Paper](https://proceedings.mlsys.org/paper_files/paper/2024/file/bbb7506579431a85861a05fff048d3e1-Paper-Conference.pdf)] [[Code](https://github.com/VITA-Group/Q-Hitter)] ![Q--Hitter](https://img.shields.io/badge/Q--Hitter-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Quantization](https://img.shields.io/badge/Quantization-orange) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Quantization_Friendlines](https://img.shields.io/badge/Quantization_Friendlines-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{MLSYS2024_bbb75065,
   author = {Zhang, Zhenyu and Liu, Shiwei and Chen, Runjin and Kailkhura, Bhavya and Chen, Beidi and Wang, Atlas},
   booktitle = {Proceedings of Machine Learning and Systems},
   editor = {P. Gibbons and G. Pekhimenko and C. De Sa},
   pages = {381--394},
   title = {Q-Hitter: A Better Token Oracle for Efficient LLM Inference via Sparse-Quantized KV Caching},
   url = {https://proceedings.mlsys.org/paper_files/paper/2024/file/bbb7506579431a85861a05fff048d3e1-Paper-Conference.pdf},
   volume = {6},
   year = {2024}
  }
  ```
  </details>  

- **[65] InfiniGen: Efficient Generative Inference of Large Language Models with Dynamic KV Caching Management**, OSDI  2024  
  *Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim*  
  [[Paper](https://www.usenix.org/system/files/osdi24-lee.pdf)] ![InfiniGen](https://img.shields.io/badge/InfiniGen-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![LLM_Serving](https://img.shields.io/badge/LLM_Serving-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings {298683,
  author = {Wonbeom Lee and Jungi Lee and Junghwan Seo and Jaewoong Sim},
  title = {{InfiniGen}: Efficient Generative Inference of Large Language Models with Dynamic {KV} Caching Management},
  booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
  year = {2024},
  isbn = {978-1-939133-40-3},
  address = {Santa Clara, CA},
  pages = {155--172},
  url = {https://www.usenix.org/conference/osdi24/presentation/lee},
  publisher = {USENIX Association},
  month = jul
  }
  ```
  </details>  

- **[66] XKV: Personalized KV Caching Memory Reduction for Long-Context LLM Inference**, arXiv 2024  
  *Li, Weizhuo and Wang, Zhigang and Gu, Yu and Yu, Ge*  
  [[Paper](https://www.arxiv.org/pdf/2412.05896)] ![XKV](https://img.shields.io/badge/XKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{li2024xkv,
    title={XKV: Personalized KV Caching Memory Reduction for Long-Context LLM Inference},
    author={Li, Weizhuo and Wang, Zhigang and Gu, Yu and Yu, Ge},
    journal={arXiv preprint arXiv:2412.05896},
    year={2024}
  }
  ```
  </details>  

- **[67] In-context KV-Caching Eviction for LLMs via Attention-Gate**, arXiv 2024  
  *Zeng, Zihao and Lin, Bokai and Hou, Tianqi and Zhang, Hao and Deng, Zhijie*  
  [[Paper](https://arxiv.org/pdf/2410.12876)] ![Attention--Gate](https://img.shields.io/badge/Attention--Gate-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![In--context_Eviction](https://img.shields.io/badge/In--context_Eviction-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{zeng2024context,
    title={In-context KV-Caching Eviction for LLMs via Attention-Gate},
    author={Zeng, Zihao and Lin, Bokai and Hou, Tianqi and Zhang, Hao and Deng, Zhijie},
    journal={arXiv preprint arXiv:2410.12876},
    year={2024}
  }
  ```
  </details>  

- **[68] Recycled Attention: Efficient inference for long-context language models**, arXiv 2024  
  *Xu, Fangyuan and Goyal, Tanya and Choi, Eunsol*  
  [[Paper](https://arxiv.org/pdf/2411.05787)] [[Code](https://github.com/carriex/recycled-attention)] ![Recycled_Attention](https://img.shields.io/badge/Recycled_Attention-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{xu2024recycled,
    title={Recycled Attention: Efficient inference for long-context language models},
    author={Xu, Fangyuan and Goyal, Tanya and Choi, Eunsol},
    journal={arXiv preprint arXiv:2411.05787},
    year={2024}
  }
  ```
  </details>  

- **[69] MagicPIG: LSH Sampling for Efficient LLM Generation**, arXiv 2024  
  *Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others*  
  [[Paper](https://arxiv.org/pdf/2410.16179)] [[Code](https://github.com/Infini-AI-Lab/MagicPIG)] ![Magicpig](https://img.shields.io/badge/Magicpig-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Locality_Sensitive_Hashing](https://img.shields.io/badge/Locality_Sensitive_Hashing-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{chen2024magicpig,
    title={Magicpig: Lsh sampling for efficient llm generation},
    author={Chen, Zhuoming and Sadhukhan, Ranajoy and Ye, Zihao and Zhou, Yang and Zhang, Jianyu and Nolte, Niklas and Tian, Yuandong and Douze, Matthijs and Bottou, Leon and Jia, Zhihao and others},
    journal={arXiv preprint arXiv:2410.16179},
    year={2024}
  }
  ```
  </details>  

- **[70] When Attention Sink Emerges in Language Models: An Empirical View**, arXiv 2024  
  *Gu, Xiangming and Pang, Tianyu and Du, Chao and Liu, Qian and Zhang, Fengzhuo and Du, Cunxiao and Wang, Ye and Lin, Min*  
  [[Paper](https://arxiv.org/pdf/2410.10781)] [[Code](https://github.com/sail-sg/Attention-Sink)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Analysis](https://img.shields.io/badge/Analysis-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{gu2024attention,
    title={When Attention Sink Emerges in Language Models: An Empirical View},
    author={Gu, Xiangming and Pang, Tianyu and Du, Chao and Liu, Qian and Zhang, Fengzhuo and Du, Cunxiao and Wang, Ye and Lin, Min},
    journal={arXiv preprint arXiv:2410.10781},
    year={2024}
  }
  ```
  </details>  

- **[71] Get More with LESS: Synthesizing Recurrence with KV Caching Compression for Efficient LLM Inference**, arXiv 2024  
  *Dong, Harry and Yang, Xinyu and Zhang, Zhenyu and Wang, Zhangyang and Chi, Yuejie and Chen, Beidi*  
  [[Paper](https://arxiv.org/pdf/2402.09398)] [[Code](https://github.com/hdong920/LESS)] ![LESS](https://img.shields.io/badge/LESS-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{dong2024get,
    title={Get More with LESS: Synthesizing Recurrence with KV Caching Compression for Efficient LLM Inference},
    author={Dong, Harry and Yang, Xinyu and Zhang, Zhenyu and Wang, Zhangyang and Chi, Yuejie and Chen, Beidi},
    journal={arXiv preprint arXiv:2402.09398},
    year={2024}
  }
  ```
  </details>  

- **[72] SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator**, arXiv 2024  
  *Chen, Guoxuan and Shi, Han and Li, Jiawei and Gao, Yihang and Ren, Xiaozhe and Chen, Yimeng and Jiang, Xin and Li, Zhenguo and Liu, Weiyang and Huang, Chao*  
  [[Paper](https://arxiv.org/pdf/2412.12094)] [[Code](https://sepllm.github.io/)] ![Sepllm](https://img.shields.io/badge/Sepllm-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Merging](https://img.shields.io/badge/Merging-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{chen2024sepllm,
    title={Sepllm: Accelerate large language models by compressing one segment into one separator},
    author={Chen, Guoxuan and Shi, Han and Li, Jiawei and Gao, Yihang and Ren, Xiaozhe and Chen, Yimeng and Jiang, Xin and Li, Zhenguo and Liu, Weiyang and Huang, Chao},
    journal={arXiv preprint arXiv:2412.12094},
    year={2024}
  }
  ```
  </details>  

- **[73] A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference**, arXiv 2024  
  *Wu, You and Wu, Haoyi and Tu, Kewei*  
  [[Paper](https://arxiv.org/pdf/2410.14442)] [[Code](https://github.com/whyNLP/LCKV)] ![LCKV](https://img.shields.io/badge/LCKV-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Sharing](https://img.shields.io/badge/Sharing-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @article{wu2024systematic,
    title={A Systematic Study of Cross-Layer KV Sharing for Efficient LLM Inference},
    author={Wu, You and Wu, Haoyi and Tu, Kewei},
    journal={arXiv preprint arXiv:2410.14442},
    year={2024}
  }
  ```
  </details>  

- **[74] LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts**, arXiv 2024  
  *Gu, Zhuohan and Yao, Jiayi and Du, Kuntai and Jiang, Junchen*  
  [[Paper](https://arxiv.org/pdf/2411.13009)] ![LLMSteer](https://img.shields.io/badge/LLMSteer-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Chunking](https://img.shields.io/badge/Chunking-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Attention_Steering](https://img.shields.io/badge/Attention_Steering-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{gu2024llmsteer,
    title={LLMSteer: Improving Long-Context LLM Inference by Steering Attention on Reused Contexts},
    author={Gu, Zhuohan and Yao, Jiayi and Du, Kuntai and Jiang, Junchen},
    journal={arXiv preprint arXiv:2411.13009},
    year={2024}
  }
  ```
  </details>  

- **[75] Efficient Sparse Attention needs Adaptive Token Release**, ACL 2024  
  *Zhang, Chaoran  and  Zou, Lixin  and  Luo, Dan  and  Luo, Xiangyang  and  Li, Zihao  and Tang, Min  and  Li, Chenliang*  
  [[Paper](https://aclanthology.org/2024.findings-acl.837.pdf)] [[Code](https://github.com/WHUIR/ADORE)] ![ADORE](https://img.shields.io/badge/ADORE-blue) ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![KV_Caching_Compression](https://img.shields.io/badge/KV_Caching_Compression-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown)  
  <details> <summary>BibTex</summary>

  ```text
  @inproceedings{zhang-etal-2024-efficient,
      title = "Efficient Sparse Attention needs Adaptive Token Release",
      author = "Zhang, Chaoran  and
        Zou, Lixin  and
        Luo, Dan  and
        Luo, Xiangyang  and
        Li, Zihao  and
        Tang, Min  and
        Li, Chenliang",
      editor = "Ku, Lun-Wei  and
        Martins, Andre  and
        Srikumar, Vivek",
      booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
      month = aug,
      year = "2024",
      address = "Bangkok, Thailand",
      publisher = "Association for Computational Linguistics",
      url = "https://aclanthology.org/2024.findings-acl.837/",
      doi = "10.18653/v1/2024.findings-acl.837",
      pages = "14081--14094"
  }
  ```
  </details>  

- **[76] Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling**, arXiv 2024  
  *Luo, Xianzhen and Wang, Yixuan and Zhu, Qingfu and Zhang, Zhiming and Zhang, Xuanyu and Yang, Qing and Xu, Dongliang and Che, Wanxiang*  
  [[Paper](https://arxiv.org/pdf/2408.08696)] ![Generation](https://img.shields.io/badge/Generation-green) ![Language_Modeling](https://img.shields.io/badge/Language_Modeling-green) ![Attention--based](https://img.shields.io/badge/Attention--based-purple) ![Pruning](https://img.shields.io/badge/Pruning-orange) ![Retrieving](https://img.shields.io/badge/Retrieving-orange) ![Training--free](https://img.shields.io/badge/Training--free-brown) ![Token_Recycling](https://img.shields.io/badge/Token_Recycling-yellow)  
  <details> <summary>BibTex</summary>

  ```text
  @article{luo2024turning,
    title={Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling},
    author={Luo, Xianzhen and Wang, Yixuan and Zhu, Qingfu and Zhang, Zhiming and Zhang, Xuanyu and Yang, Qing and Xu, Dongliang and Che, Wanxiang},
    journal={arXiv preprint arXiv:2408.08696},
    year={2024}
  }
  ```
  </details>  

